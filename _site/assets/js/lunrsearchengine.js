
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "About Me",
    "body": "  머신러닝 엔지니어를 목표로 공부하고 있습니다. 기록하기를 좋아합니다. 📋 🧑‍🎓 Education:       동국대학교 전자전기공학부(졸업)   2015. 03-2020. 08         Naver Connect Foundation AI Tech 5th   2023. 03-2023. 08   🔗 My Links:       Github       Notion     LinkedIn Buy me a coffee커피 냠냠 ☕ Buy me a coffee "
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                                                                 Semantic Segmentation Overview                              :               Semantic Segmentation What is Semantic Segmentation? Semantic Segmentation은 이미지의 모든 픽셀별로 클래스를 분류하는 task 이다. 말 그대로 pixel wise classification이라서 같은 클래스에 속하더라도 객체를 구분해주지는. . . :                                                                                                                                                                       Seungki                                05 Jun 2023                                                                                                                                                                                                                                                                                                                        Object Detection Overview                              :               Object Detection Classification + Localization 하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인. . . :                                                                                                                                                                       Seungki                                29 Apr 2023                                                                                                                                                                                                                                                                                                    All Posts(41):                                                                                                     Introduction to BentoML              :       BentoML 등장 배경 BentoML is designed for teams working to bring machine learning (ML) models into production in a reliable, scalable, and cost-efficient way. In particular, AI application developers can. . . :                                                                               Seungki                08 Jul 2023                                                                                                                                     Introduction to MLflow              :       MLFlow로 해결할 Pain Point 실험 추적이 어렵다 코드 재현이 어렵다 모델 패키징 및 배포가 어렵다 모델을 관리하기 위한 중앙 저장소가 없다:                                                                               Seungki                07 Jul 2023                                                                                                                                     Introduction to Apache Airflow              :       Apache Airflow의 등장 배경Batch Processing 예약된 시간에 실행되는 프로세스 일회성 실행, 주기적인 실행 전부 가능     월요일날 7:00시에 한번 실행, 매주 월요일 7:00에 실행   :                                                                               Seungki                06 Jul 2023                                                                                                                                     Logging              :       Logging 사용자 로그 데이터, 이벤트 로그 데이터 . .  머신러닝 인퍼런스 요청 로그, 인퍼런스 결과 등을 저장해야 함:                                                                               Seungki                05 Jul 2023                                                                                                                                     FastAPI - 2              :       Event Handler 이벤트가 발생했을 때, 그 처리를 담당하는 함수 FastAPI 에선 Application을 실행, 종료할 때 특정 함수를 실행할 수 있음:                                                                               Seungki                04 Jul 2023                                                                                                                                     FastAPI - 1              :       FastAPI 최근 떠오르는 Python Web Framework API document 작성을 자동으로 해주는 Swagger 간결한 코드 작성 빠른 속도:                                                                               Seungki                04 Jul 2023                                               &laquo; Prev       1        2        3        4        5        6        7      Next &raquo; "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "http://localhost:4000/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "http://localhost:4000/page4/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "http://localhost:4000/page5/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "http://localhost:4000/page6/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "http://localhost:4000/page7/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% assign total_posts = site. posts | size %}       All Posts({{ total_posts }}):         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "http://localhost:4000/BentoML-1/",
    "title": "Introduction to BentoML",
    "body": "2023/07/08 - BentoML 등장 배경:  BentoML is designed for teams working to bring machine learning (ML) models into production in a reliable, scalable, and cost-efficient way. In particular, AI application developers can leverage BentoML to easily integrate state-of-the-art pre-trained models into their applications. By seamlessly bridging the gap between model creation and production deployment, BentoML promotes collaboration between developers and in-house data science teams. 항상 많은 라이브러리들이 새롭게 등장하고, 보통 이 라이브러리들은 해결하려고 하는 핵심 문제가 존재 함. BentoML이 해결하려는 문제:    Model Serving Infra의 어려움          Serving을 위해 다양한 라이브러리, artifact, asset 등. . 사이즈가 큰 파일을 패키징 해야함           Cloud service에 지속적으로 배포하기 위해 많은 작업이 필요함     -&gt; BentoML은 CLI로 이 문제의 복잡도를 낮추려고 함(CLI 명령어로 모두 진행 가능)          Online Serving의 Monitoring 및 Error Handling      Online Serving으로 API 형태로 생성   Error 처리, Logging을 추가로 구현해야 함   BentoML은 Python Logging Module을 사용해 Access Log, Prediction Log를 기본으로 제공   Config를 수정해 Logging도 커스텀할 수 있고, Prometheus 같은 Metric 수집 서버에 전송할 수 있음      Online Serving의 포퍼먼스 튜닝의 어려움      Adaptive Micro Batch 방식을 채택해 동시에 많은 요청이 들어와도 높은 처리량을 보여줌   BentoML 특징: Serving에 특화된 가벼운 라이브러리로 볼 수 있음  쉬운 사용성 Online/Offline Serving 지원 Tensorflow, PyTorch, Keras, XGBoost 등 메이저 프레임워크 지원 Docker, Kubernetes, AWS, Azure 등의 배포 환경 지원 및 가이드 제공 Flask 대비 100배의 처리량 모델 저장소(Yatai) 웹 대시보드 지원 데이터 사이언스와 데브옵스 사이의 간격을 좁혀주고, 높은 성능의 serving 지원 출처 - https://towardsdatascience. com/10-ways-bentoml-can-help-you-serve-and-scale-machine-learning-models-4060f1e59d0d BentoML 사용하기: 1. BentoML 설치:  BentoML은 python 3. 6 이상만 지원 pyenv 등으로 파이썬 버전을 설정해서 진행 가상환경 virtualenv 또는 poetry로 설정pip install bentoml 2. BentoML Flow:  모델 학습 코드 생성 Prediction Service Class 생성 Prediction Service에 모델 저장 Serving Docker Image Build(컨테이너화) Serving 배포Prediction Service Class 생성: 12345678910111213141516171819202122# bento_service. pyimport pandas as pdfrom bentoml import env, artifacts, api, BentoServicefrom bentoml. adapters import DataframeInputfrom bentoml. frameworks. sklearn import SklearnModelArtifact@env(infer_pip_packages=True)@artifacts([SklearnModelArtifact('model')])class IrisClassifier(BentoService):       A minimum prediction service exposing a Scikit-learn model       @api(input=DataframeInput(), batch=True)  def predict(self, df: pd. DataFrame):           An inference API named `predict` with Dataframe input adapter, which codifies    how HTTP requests or CSV files are converted to a pandas Dataframe object as the    inference API function input           return self. artifacts. model. predict(df) BentoService를 활용해 Prediction Service Class 생성 예측할 때 사용하는 API를 위한 Class12@env(infer_pip_packages=True)@artifacts([SklearnModelArtifact('model')]) @env : 파이썬 패키지, install script 등 서비스에 필요한 의존성을 정의 @artifacts : 서비스에서 사용할 artifact 정의 -&gt; Sklearn, XGboost, Pytorch 등 . . 12345678910111213class IrisClassifier(BentoService):       A minimum prediction service exposing a Scikit-learn model       @api(input=DataframeInput(), batch=True)  def predict(self, df: pd. DataFrame):           An inference API named `predict` with Dataframe input adapter, which codifies    how HTTP requests or CSV files are converted to a pandas Dataframe object as the    inference API function input           return self. artifacts. model. predict(df) BentoService를 상속하면 해당 서비스를 Yatai(모델 이미지 레지스터리)에 저장 @api : API 생성     Input과 Output을 원하는 형태(Dataframe, Tensor, JSON 등. . )으로 선택할 수 있음   Doc string으로 Swagger에 들어갈 내용을 추가할 수 있음    @artifacts에 사용한 이름을 토대로 self. artifacts. model로 접근Prediction Service에 저장(Pack): 123456789101112131415161718192021# bento_packer. py# 모델 학습from sklearn import svmfrom sklearn import datasetsclf = svm. SVC(gamma='scale')iris = datasets. load_iris()X, y = iris. data, iris. targetclf. fit(X, y)# bento_service. py에서 정의한 IrisClassifierfrom bento_service import IrisClassifier# IrisClassifier 인스턴스 생성iris_classifier_service = IrisClassifier()# Model Artifact를 Packiris_classifier_service. pack('model', clf)# Model Serving을 위한 서비스를 Disk에 저장saved_path = iris_classifier_service. save() Model Artifact를 주입   BentoML Bundle : Prediction Service를 실행할 때 필요한 모든 코드, 구성이 포함된 폴더, 모델 제공을 위한 바이너리   CLI에서 python bento_packer. py 실행 -&gt; Saved to ~ 경로가 보일 것임 BentoML에 저장된 Prediction Service 확인     bentoml list    BentoML에 저장된 Prediction Service 폴더로 이동 후 파일 확인     tree 명령어로 디렉토리 구조 확인 (tree -L 4)    bentoml. yml에 모델의 메타 정보, 패키지 환경, API input/output, Docs 등을 확인 할 수 있음Serving: Yatai Service 실행:  bentoml yatai-service-start localhost:3000Docker Image Build:  bentoml containerize IrisClassifier:latest -t iris-classifier docker images로 빌드된 이미지 확인-&gt; docker 명령이나 FastAPI를 사용하지 않고 웹 서버를 띄우고, 이미지 빌드! Bentoml Component:  BentoService Service Environment Model Artifact Model Artifact Metadata Model Management &amp; Yatai API Function and Adapters Model Serving Labels Retrieving BentoServices Web UIBentoService:  bentoml. BentoService는 예측 서비스를 만들기 위한 베이스 클래스 @bentoml. artifacts : 여러 머신러닝 모델 포함할 수 있음 @bentoml. api: Input/Output 정의     API 함수 코드에서 ```self. artifacts. {ARTIFACT_NAME}으로 접근 가능    파이썬 코드와 관련된 종속성 저장Service Enviroment:  파이썬 관련 환경, Docker 등을 설정 가능 @bentoml. env(infer_pip_packages=True) : import를 기반으로 필요한 라이브러리 추론 requirements_txt_file을 명시할 수 있음 pip_packages=[]를 사용해 버전을 명시할 수 있음 docker_base_image를 사용해 Base image를 지정 가능 setup_sh를 지정해 Docker Build 과정을 커스텀할 수 있음Model Artifact Metadata: Metadata 접근 방법    CLI      bentoml get model:version      REST API      bentoml serve 후, /metadata로 접근      python          123from bentoml import loadsvc = load('path_to_bento_service')print(svc. artifacts['model']. metadata)          Model Serving: BentoService가 Bento로 저장되면 여러 방법으로 배포할 수 있음!  Online Serving     클라이언트가 REST API Endpoint로 근 실시간으로 예측 요청    Offline Batch Serving     예측을 계산 후, Storage에 저장    Edge Serving     모바일, IoT device에 배포   Web UI:  @bentoml. web_static_content를 사용해 웹프론트엔드에 추가할 수 있음 참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://docs. bentoml. org/en/latest/overview/what-is-bentoml. html"
    }, {
    "id": 12,
    "url": "http://localhost:4000/MLFlow-1/",
    "title": "Introduction to MLflow",
    "body": "2023/07/07 - MLFlow로 해결할 Pain Point:  실험 추적이 어렵다 코드 재현이 어렵다 모델 패키징 및 배포가 어렵다 모델을 관리하기 위한 중앙 저장소가 없다그럼 MLFlow란?:  머신러닝 실험, 배포를 쉽게 관리할 수 있는 오픈 소스 CLI, GUI(웹 인터페이스) 지원Example: 123456789101112131415161718from sklearn import svm, datasetsfrom sklearn. model_selection import GridSearchCVimport mlflowdef main():  mlflow. sklearn. autolog()  iris = datasets. load_iris()  parameters = { kernel : ( linear ,  rbf ),  C : [1, 10]}  svc = svm. SVC()  clf = GridSearchCV(svc, parameters)  with mlflow. start_run() as run:    clf. fit(iris. data, iris. target)if __name__ ==  __main__ :  main()MLFlow의 핵심 기능:  Experiment Management &amp; Tracking     머신러닝 관련 실험들을 관리하고, 각 실험의 내용 기록         여러 사람이 하나의 mlflow 서버 위에서 각자 자기 실험을 만들고 공유 할 수 있음          실험을 정의하고 실험을 실행 할 수 있음, 실행은 머신러닝 훈련코드를 실행한 기록         각 실행에 사용한 소스코드, 하이퍼 파라미터, metric, 부산물(artifact, image. . ) 등을 저장           Model Registry     MLflow로 실행한 머신러닝 모델을 Model registry(모델 저장소)에 등록 가능   모델 저장소에 모델이 저장될 때마다 해당 모델에 버전이 자동으로 올라감   모델 저장소에 등록된 모델은 다른 사람들에게 쉽게 공유 가능    Model Serving     Model Registry에 등록한 모델을 REST API형태의 서버로 serving 가능   Input == 모델의 Input   Output == 모델의 Output   직접 도커 이미지를 만들지 않아도 생성 가능   MLflow Component:  MLflow Tracking     머신러닝 코드 실행, 로깅을 위한 api, ui   MLflow Tracking을 사용해 결과를 local, server에 기록해 여러 실행과 비교 가능   팀에선 다른 사용자의 결과와 비교하며 협업가능    MLflow Project     머신러닝 프로젝트 코드를 패키징하기 위한 표준   Project         간단하게 소스 코드가 저장된 폴더     Git repo     의존성과 어떻게 실행해야 하는지 저장          MLflow Tracking API를 사용하면 MLflow는 프로젝트 버전을 모든 파라미터와 자동으로 로깅    MLflow Model     모델은 모델 파일과 코드로 저장   다양한 플랫폼에 배포할 수 있는 여러 도구 제공   MLflow Tracking API를 사용하면 MLflow는 자동으로 해당 프로젝트에 대한 내용을 사용함    MLflow Registry     MLflow Model의 전체 lifecycle에서 사용할 수 있는 중앙 모델 저장소   MLflow 사용하기: pip install mlflow 1. Experiment 생성:  하나의 Experiment는 진행하고 있는 머신러닝 프로젝트 단위로 구성     개/고양이 분류 실험, 수요량 예측 실험 . .     정해진 Metric으로 모델 평가     RMSE, MSE, MAE, Accuracy. .     하나의 실험 여러개의 runmlflow experiments create --experiment-name my-first-experiment : Experiment 생성 ls -al로 mlruns 폴더 확인 mlflow experiments search : 생성한 experiments 목록 확인 (list는 현재 deprecated) pip install numpy sklearn : 모델에 필요한 라이브러리 설치 (상황 마다 바뀜) mkdir logistic_regression : 폴더 생성 vi logistic_regression/train. py : 머신러닝 코드 생성 train. py: 123456789101112131415161718192021222324import numpy as npfrom sklearn. linear_model import LogisticRegressionimport mlflowimport mlflow. sklearnif __name__ ==  __main__ :  X = np. array([-2, -1, 0, 1, 2, 1]). reshape(-1, 1)  y = np. array([0, 0, 1, 1, 1, 0])  penalty =  elasticnet   l1_ratio = 0. 1  lr = LogisticRegression(penalty=penalty, l1_ratio=l1_ratio, solver= saga )  lr. fit(X, y)  score = lr. score(X, y)  print( Score: %s  % score)   # auto 나오기 전에는 이렇게 사용했음  mlflow. log_param( penalty , penalty)  mlflow. log_param( l1_ratio , l1_ratio)  mlflow. log_metric( score , score)  mlflow. sklearn. log_model(lr,  model )2. MLProject:  Mlflow를 사용한 코드의 프로젝트 메타 정보 저장 프로젝트를 어떤 환경에서 어떻게 실행시킬지 정의 패키지 모듈의 상단에 위치 이름은 MLProject 라는 이름을 꼭 사용해야함vi logistic_regression/MLProject: MLProject 생성 #### MLProject 12345name: tutorialentry_points:  main:    command:  python train. py 3. Run:  하나의 Run은 코드를 1번 실행한 것을 의미 보통 Run은 모델 학습 코드를 실행 한번의 코드 실행 == 하나의 Run 생성 Run을 하면 여러가지 내용이 기록됨     source : 실행한 project의 이름   version : 실행 Hash   start &amp; end time   parameters : 모든 파라미터   metrics : 모델의 평가 지표, metric 시각화   tags   artifacts : 실행과정에서 생기는 다양한 파일들(이미지, 모델 피클 . . )   mlflow run logistic_regression --experiment-name my-first-experiment --env-manager {환경} : Run으로 실행 mlflow ui : UI 실헹 MLflow Autolog:  Automatic logging allows you to log metrics, parameters, and models without the need for explicit log statements.  파라미터를 매번 명시하는게 귀찮음 자동으로 로깅을 해줌 모든 프레임 워크에서 사용가능한 것은 아님!     pytorch. nn. Module 지원 x, pytorch lightning은 지원 . .    MLflow 배포하기: MLflow Architecture:  Python Code(with MLflow package)     모델을 만들고 학습하는 코드   MLflow run으로 실행    Tracking Server     파이썬 코드가 실행되는 동안 parameter, metric, model 등 메타 정보 저장   파일 혹은 DB에 저장         Tracking server는 결국 DB를 바라봄           Artifact Store     파이썬 코드가 실행되는 동안 생기는 model file, image 등의 아티팩트를 저장   파일 혹은 스토리지에 저장         Artifact Store는 결국 storage를 바라봄          mlflow server --backend-store-uri {uri} --default-artifact-root {} : mlflow server 명령어로 Backend Store URI 지정가능 MLflow 실제 Use Case: MLflow Tracking Server는 하나로 통합 운영  Tracking Server를 하나 배포하고, 팀 내 모든 리서처가 이 Tracking Server에 실험 기록     배포할 때는 Docker Image, Kubernetes 등에 진행(회사 인프라에 따라 다름)    로그나 모델이 한 곳에 저장되므로 팀 내 모든 실험을 공유 가능 Artifact Storage는 GCS나 S3 같은 스토리지 사용 DB는 CloudSQL이나 Aurora RDS 같은 DB 사용 두 저장소는 Tracking Server에 의해 관리 참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://mlflow. org/docs/latest/tracking. html"
    }, {
    "id": 13,
    "url": "http://localhost:4000/ApacheAirflow-1/",
    "title": "Introduction to Apache Airflow",
    "body": "2023/07/06 - Apache Airflow의 등장 배경: Batch Processing:  예약된 시간에 실행되는 프로세스 일회성 실행, 주기적인 실행 전부 가능     월요일날 7:00시에 한번 실행, 매주 월요일 7:00에 실행   ML, AI 엔지니어 관점에서의 Batch Processing:  모델을 주기적으로 학습시키는 경우(continuous training) 주기적인 Batch Serving을 하는 경우 그 외 개발에서 필요한 배치성 작업Airflow 등장 전의 Batch Process:  Linux Crontab으로 Batch Process 구축하는 경우 크론 표현식은 Batcj Process의 스케쥴링을 정의한 표현식     이 표현식은 다른 Batch Process 도구에서도 자주 사용됨    출처 - https://docs. aws. amazon. com/elemental-cl3/latest/apireference/channel-scheduling-cron-syntax-summary. html  ***** - Every minute 0**** - Every hour 00*** - Every day at 12:00 AM 00**FRI - At 12:00 AM, only on Friday   001** - At 12:00 AM, on day 1 of the month   https://crontab. cronhub. io/ http://www. cronmaker. com/ https://crontab. guru/Linux Crontab의 한계:  재실행 및 알람     파일을 실행하다 오류가 생긴 경우, 크론탭이 별도의 처리를 하지 않음    실패할 경우, retry 하거나 실패했다는 알람을 보내주는 기능이 필요 과거 실행 이력 및 실행 로그를 보기 어려움 여러 파일을 실행하거나, 복잡한 파이프라인을 만들기 어려움-&gt; 더 정교한 스케쥴링 및 워크플로우 도구가 필요함! 스케쥴링 도구들의 등장:  Luigi argo Apache Airflow Prefect MetaFlowApache Airflow 소개: 현재 스케쥴링, 워크플로우 도구의 표준  업데이트 주기 빠름 스케쥴링 도구로 무거울 수 있지만, 거의 모든 기능을 제공하고, 확장성이 넓어 일반적으로 스케쥴링과 파이프라인 작성 도구로 많이 사용 데이터 엔지니어링 팀에서 많이 사용Airflow의 기능:  파이썬을 사용해 스케쥴링 및 파이프라인 작성 실패 시 알람 실패 시 재실행 시도 동시 실행 워커 수 설정 및 변수 값 분리Airflow 사용해보기:  가상환경 설정12python -m venv . venvsource . venv/bin/activate Airflow 설치12pip install pip --upgradepip install 'apache-airflow==2. 2. 0' Airflow 기본 디렉토리 경로 지정export AIRFLOW_HOME=.  Airflow에서 사용할 DB 초기화airflow db init  DB를 초기화하면 기본 파일이 생성 ls -al로 확인 Airflow에서 사용할 어드민 계정 생성airflow users create --username admin --password 1234 --firstname seungki --lastname kim --role Admin --email {email}  Airflow Webserver 실행airflow webserver --port 8080  WebUI Dashboard 등장     스케쥴러가 실행중이지 않다는 에러가 보임    별도의 터미널을 띄워서 Airflow Scheduler 실행airflow scheduler  Web UI에서 관련 에러가 없어진 것을 확인할 수 있음DAG: Batch Scheduling을 위한 DAG 생성:  Airflow에서는 스케줄링할 작업을 DAG이라고 부름 DAG은 Directed Acyclic Graph의 약자로 Airflow에 한정된 개념이 아닌 소프트웨어 자료구조에서 일반적으로 다루는 개념 DAG은 이름 그대로 순환하지 않는 방향이 존재하는 그래프를 의미Airflow는 Crontab처럼 단순히 하나의 파일을 실행하는 것이 아닌, 여러 작업의 조합도 가능함  DAG 1개 : 1개의 파이프라인 Task : DAG 내에서 실행할 작업하나의 DAG에 여러 Task의 조합으로 구성 출처 - https://www. qubole. com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom example) tutorial_etl_dag라는 DAG은 3가지 Task로 구성  extract transform load -&gt; tutorial_etl_dag라는 DAG을 실행하면 이 3가지 Task을 순차적으로 실행함Task가 꼭 순차적으로 진행하지 않게 할 수도 있음 tutorial DAG  print_data Task 이후, sleep, templated Task 동시(병렬) 실행DAG 작성하기: DAG 작성:  DAG을 담을 디렉토리 생성(이름은 무조건 dags)mkdir dags  dags 폴더 내에 hello_world. py 생성hello_world. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445# hello_world. pyfrom datetime import timedeltafrom airflow import DAGfrom airflow. utils. dates import days_agofrom airflow. operators. bash import BashOperatorfrom airflow. operators. python import PythonOperatordef print_world() -&gt; None:  print( world )# with 구문으로 DAG 정의를 시작합니다. with DAG(  dag_id= hello_world , # DAG의 식별자용 아이디입니다.   description= My First DAG , # DAG에 대해 설명합니다.   start_date=days_ago(2), # DAG 정의 기준 2일 전부터 시작합니다.   schedule_interval= 0 6 * * * , # 매일 06:00에 실행합니다.   tags=[ my_dags ], # 태그 목록을 정의합니다. 추후에 DAG을 검색하는데 용이합니다. ) as dag:  # 테스크를 정의합니다.   # bash 커맨드로 echo hello 를 실행합니다.   t1 = BashOperator(    task_id= print_hello ,    bash_command= echo Hello ,    owner= heumsi , # 이 작업의 오너입니다. 보통 작업을 담당하는 사람 이름을 넣습니다.     retries=3, # 이 테스크가 실패한 경우, 3번 재시도 합니다.     retry_delay=timedelta(minutes=5), # 재시도하는 시간 간격은 5분입니다.   )  # 테스크를 정의합니다.   # python 함수인 print_world를 실행합니다.   t2 = PythonOperator(    task_id= print_world ,    python_callable=print_world,    depends_on_past=True,    owner= heumsi ,    retries=3,    retry_delay=timedelta(minutes=5),  )  # 테스크 순서를 정합니다.   # t1 실행 후 t2를 실행합니다.   t1 &gt;&gt; t2 DAG 정의(이름, 태그)     언제부터 스케쥴링 할지   스케쥴링 간격은 어떻게 할 지    DAG 내 Task 정의     Task 정의는 Airflow의 Operator 클래스 사용   Airflow에는 다양한 Operator 클래스가 존재하며, 뒤에서 살펴봄         첫 번째 Task는 bash 커맨드 실행             Airflow에서 제공하는 BashOperator 사용       bash_command 파라미터에 bash로 실행할 커맨드 전달                두 번째 Tasksms Python 함수 실행             Airflow에서 제공하는 PythonOperator 사용       python_callable 파라미터에 실행할 파이썬 함수 전달                      Dag 내 Task간 순서 정하기     순서는 &gt;&gt;와 같은 형태로 표현   t1 (BashOperator) 실행 후, t2 (PythonOperator)를 실행   실행결과 확인:  파일을 저장하고, UI를 확인해보면 새로 생성한 DAG 보임   DAG 상세 페이지에서 DAG을 ON 상태로 변경   DAG Run의 첫 번째 Task 사각형을 눌러서 Log 버튼을 클릭해서 Log 확인 가능 특정 DAG Run의 기록을 지우고 다시 실행시키고 싶으면 Clear 실행Operator들 소개:  Airflow에서는 다양한 Operator를 제공 PythonOperator     파이썬 함수를 실행   함수 뿐 아니라, Callable한 객체를 파라미터로 넘겨 실행 가능   실행할 파이썬 로직을 함수로 생성한 후, PythonOperator로 실행    BashOperator     Bash 커맨드를 실행   실행해야할 프로세스가 파이썬이 아닌 경우에도 BashOperator로 실행 가능         shell script, scala file . .            DummyOperator     아무것도 실행하지 않음   DAG내에서 Task를 구성할 때, 여러 개의 Task의 Success를 기다려야 하는 복잡한 Task 구성에서 사용    SimpleHttpOperator     특정 호스트로 HTTP 요청을 보내고 Response로 반환   파이썬 함수에서 requests 모듈로 사용한 뒤 PythonOperator로 실행시켜도 무방    그 외     KubernetesOperator   OckerOperator   CustomOperator   등. .     클라우드 기능을 추상화한 Operator도 존재함(AWS, GCP . . )     provider packages   Third Party와 연동해 사용하는 Operator의 경우 Airflow 설치 시에 다음처럼 extra package를 설치해야 함   추가적으로 학습 할 내용:  Variable Connections &amp; Hooks Sensor Marker XComsAirflow Architecture: https://airflow. apache. org/docs/apache-airflow/stable/core-concepts/overview. html DAG Directory:  DAG 파일들을 저장 DAG_FOLDER Scheduler에 의해 . py 파일은 모두 탐색되고 DAG에 파싱Scheduler:  각종 메타 정보의 기록을 담당 DAG들의 스케쥴링 관리 실행 진행 상황과 결과를 DB에 저장 Executer를 통해 실제로 스케쥴링된 DAG을 실행 Airflow의 가장 핵심적인 componentScheduler - Executer:    스케쥴링된 DAG을 실행하는 객체, 크게 2개로 나뉨   Local Executer     DAG Run을 프로세스 단위로 실행하며, 다음 처럼 나뉨         Local             하나의 DAG Run을 프로세스로 띄워서 실행       최대로 생성할 프로세스 수를 정해야 함       Airflow를 간단하게 운영할 때 적합                Sequential             하나의 프로세스에서 모든 DAG Run들을 처리       Airflow 기본 Executer, 별도 설정이 없으면 이것을 사용       테스트용으로 잠시 운영할 때 적합                      Remote Executer     DAG을 외부 프로세스로 실행         Celery             DAG Run을 Celery Work Process로 실행       보통 Redis를 중간에 두고 같이 사용       Local Executer를 사용하다 Airflow 운영 규모가 좀 더 커지면 Celery Executor로 전환                Kubernetes             쿠버네티스 상에서 Airflow를 운영할 때 사용       DAG Run 하나가 하나의 Pod(컨테이너 같은 개념)       운영 규모가 큰 팀에서 사용                     Workers:  DAG을 실제로 실행 Scheduler에 의해 생기고 실행 Executer에 따라 워커의 형태가 다름     Celery, Local Executer의 경우 Worker는 Process   Kubernetes의 경우 Worker는 Pod    Dag Run을 실행하는 과정에서 생기는 로그를 저장Metadata Database:  메타 정보를 저장 Scheduler에 메타 정보가 쌓임 보토 MySQL이나 Postgres를 사용 파싱한 DAG 정보, DAG Run 상태와 실행 내용, Task 정보 등을 저장 User와 Role(RBAC)에 대한 정보 저장 Scheduler와 더불어서 핵심 컴포넌트 트러블 슈팅 시, 디버깅을 위해 직접 DB에 연결해서 데이터를 확인하기도 함 실제 운영 환경에서는 GCP Cloud SQL이나 AWS Aurora DB 등. . 외부 DB 인스턴스를 사용WebServer:  WebUI를 담당 Metadata DB와 통신하며 유저에게 필요한 메타정보를 웹 브라우저에 보여주고 시각화 보통 Airflow 사용자들은 이 웹서버를 이용하여 DAG을 ON/OFF 하면서, 현 상황 파악 REST API도 제공하기 때문에, Web UI를 통해서 통신하지 않아도 괜찮음 웹서버가 당장 작동하지 않아도 Airflow에 큰 장애가 발생하는 것은 아님실제 Airflow 구축 및 활용: Airflow를 구축하는 방법은 보통 3가지 방법을 사용함  Managed Airflow (GCP Composer, AWS MWAA)     클라우드 서비스 형태로 Airflow를 사용하는 방법   장점         설치와 구축을 클릭 몇번으로 클라우드 서비스가 진행     유조는 DAG 파일을 스토리지(업로드) 형태로 관리          단점         비용     자유도가 적음, 제약이 많음           VM + Docker Compose     직접 VM 위에서 Docker Compose로 Airflow를 배포하는 방법   Airflow 구축에 필요한 컴포넌트(Scheduler, Webserver, Database. . )를 Docker 컨테이너 형태로 배포   장점         Managed Service 보다는 조금 복잡하지만, 어려운 난이도는 아님     Docker Compose에 익숙하면 금방 익힐 수 있음     하나의 VM만을 이용하기 때문에 단순          단점         각 Docker 컨테이너 별로 환경이 다르기 때문에, 관리 포인트가 늘어남           Kubernetes + Helm     Helm 차트로 Airflow를 배포하는 방법   Kubernetes는 여러 개의 VM을 동적으로 운영하는 일종의 분산환경, 리소스 사용이 매우 유연함(Scalability가 좋음)   특정 시간에 배치 프로세스를 실행시키는 Airflow와 궁합이 매우 잘 맞음   Airflow DAG 수가 몇 백개로 늘어나도 노드 오토 스케일링으로 모든 프로세스를 잘 처리할 수 있음   쿠버네티스 자체가 난이도가 있음 -&gt; 구축, 운영이 어려움    출처 - https://tech. socarcorp. kr/data/2021/06/01/data-engineering-with-airflow. html MLOps 관점에서의 Airflow: 데이터 엔지니어링에서 많이 쓰이지만, MLOps에서도 활용가능  주기적인 실행이 필요한 경우     Batch Training : 1주일 단위로 모델 학습   Batch Serving(Batch Inference) : 30분 단위로 인퍼런스   인퍼런스 겨로가를 기반으로 일자별, 주차별 모델 퍼포먼스 Report 생성   MySQL에 저장된 메타 데이터를 데이터 웨어하우스로 1시간 단위로 옮기기   S3, GCS 등 Object Storage   Feature Store를 만들기 위해 Batch ETL 실행    참고:  https://github. com/zzsza Naver Connection AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 14,
    "url": "http://localhost:4000/Logging/",
    "title": "Logging",
    "body": "2023/07/05 - Logging:  사용자 로그 데이터, 이벤트 로그 데이터 . .  머신러닝 인퍼런스 요청 로그, 인퍼런스 결과 등을 저장해야 함데이터의 종류: 데이터베이스 데이터(서비스 로그):  Database에 저장되는 데이터 서비스가 운영되기 위해 필요한 데이터     고객 가입일, 물건 구입 내역 등. .    사용자 행동 데이터(유저 행동 로그):  Object storage, Data Warehouse에 주로 저장 유저 로그라고 지칭하면 보통 사용자 행동 데이터를 의미 서비스에 반드시 필요한 내용은 아니지만, 더 좋은 제품을 만들기 위해 또는 데이터 분석시 필요한 데이터 앱이나 웹에서 유저가 어떤 행동을 하는지 나타내는 데이터 UX와 관련해서 인터랙션이 이루어지는 관점에 발생하는 데이터     click, view, swipe . .    인프라 데이터(Metric):  백엔드 웹 서버가 제대로 동작하고 있는지 확인하는 데이터 request 수, response 수 DB 부하 트래픽Metric, Log, Trace:  Metric     값을 측정할 때 사용   cpu, memory 사용량    Log     운영 관점에서 알아야하는 데이터를 남길 때 사용   함수의 호출, 예외 처리 등. .     Trace     개발 관점에서 알아야하는 것   예외 trace   데이터 적재 방식: Database(RDBMS):  데이터가 다시 웹, 앱 서비스에서 사용되는 경우 활용 실제 서비스용 DB구체적으로 들어가면  관계형 데이터베이스(relational) 행과 열로 구성 데이터의 관계를 정의하고, 데이터 모델링 진행 비즈니스와 연관된 중요한 정보     고객 정보, 주문 요청, 내역 . .     영구적으로 저장해야 하는 것은 데이터베이스에 저장 데이터 추출시 SQL 사용 MySQL, PostgreSQL . . https://www. stechies. com/differences-between-dbms-rdbms/ Naver Boostcamp AI Tech 5th - Product Serving Database(NoSQL):  Elasticsearch, Logstash or Fluent, Kibana에서 활용하는 경우구체적으로 들어가면  스키마가 strict한 RDBMS와 다르게 스키마가 없거나 느슨함 Not Only SQL 데이터가 많아지며 RDBMS로 트래픽을 감당하기 어려워서 개발됨 일반적으로 RDBMS에 비해 쓰기와 읽기 성능이 빠름 Key Value store, document, column family, graph . .  json 형태와 비슷하며 xml 등도 활용됨 MongoDBSQL vs NoSQL: https://expeed. com/when-to-use-sql-databases-vs-nosql-databases-making-the-right-decision/ Object Storage:  S3, Cloud Storage에 파일 형태로 저장 csv, parquet, json . .  별도로 DB나 warehouse로 옮기는 작업이 필요함구체적으로 들어가면  어떤 형태의 파일이여도 저장할 수 있는 저장소 특정 시스템에 발생하는 로그를 xxx. log에 저장한 후, object storage에 저장하는 형태 비즈니스에서 사용되지 않는 분석을 위한 데이터 이미지, 음성 등을 저장Data Warehouse:    데이터 분석시 활용하는 데이터 웨어하우스로 바로 저장   여러 공간에 저장된 데이터를 한곳으로 저장 데이터 창고 같은 느낌으로 알면 편함 RDBMS, NoSQL, Object Storage 등에서 저장한 데이터를 한 곳으로 옮겨서 처리 RDBMS와 같은 SQL을 사용하지만 성능이 더 좋은 편 AWS Redshift, GCP BigQuery, Snowflake . . Print vs Logging:  print는 콘솔에만 output을 출력하는 경우로 생각하자 logging은 file, web socket 등 파이썬이 다룰 수 있는 모든 포맷으로 output 출력 가능     언제 어디서 해당 output의 발생을 알 수 있음    심각도에 따른 분류를 할 수 있음     develop 환경에서는 debug로그 까지, production 환경에서는 info 로그만   12345678910111213141516171819202122232425262728#### 1. logging module 써보기import logginglogger = logging. getLogger( example ) # root loggerlogger. info( hello world ) # 아무런 로그도 출력되지 않습니다. #### 1. 1 logging module config 추가하기import logging. configlogger_config = {   version : 1, # required   disable_existing_loggers : True, # 다른 Logger를 overriding 합니다   formatters : {     simple : { format :  %(asctime)s | %(levelname)s - %(message)s },  },   handlers : {     console : {       level :  DEBUG ,       class :  logging. StreamHandler ,       formatter :  simple ,    }  },   loggers : { example : { level :  INFO ,  handlers : [ console ]}},}logging. config. dictConfig(logger_config)logger_with_config = logging. getLogger( example )logger_with_config. info( 이제는 보이죠? ) Config 설정을 해야 output 출력 지정한 로그 포맷 형태로 로그 출력  format :  %(asctime)s | %(levelname)s - %(message)s  https://docs. python. org/3/library/logging. htmlPython Logging Component: Logger:  로그를 생성하는 method 제공(logger. info(), . . ) 로그 level과 logger에 적용된 filter를 기반으로 처리해야 하는 로그인지 판단 handler에게 logrecord 인스턴스 전달 logging. getLogger(name)으로 Logger Object 사용     name이 주어지면 해당 name의 logger 사용   name이 없으면 root logger 사용   마침표로 구분되는 계층 구조         logging. getLogger('foo. bar') -&gt; logging. getLogger('foo')의 자식 logger 반환           logging. setLevel() : Logger에서 사용할 level 지정Handler:  Logger에서 만들어진 log를 적절한 위치로 전송(파일 또는 콘솔 출력. . ) level과 formatter를 각각 설정해서 필터링 할 수 있음 StreamHandler, FileHandler, HTTPHandler . .  https://www. toptal. com/python/in-depth-python-logging  Formatter     최종적으로 log에 출력될 포맷 설정   시간, logger 이름, 심각도, output, 함수 이름, line 정보, 메세지 . .    Logging Flow:  https://docs. python. org/ko/3/howto/logging. html Online Serving Logging (BigQuery): BigQuery에 Online Serving Input과 Output 로그를 적재하는 과정  빅쿼리 테이블을 세팅 빅쿼리에 적재하기 쉽게 json 형태로 로그를 정제 -&gt; pythonjsonlogger 사용 Python logging 모듈을 사용해서 빅쿼리에 실시간으로 로그 적재(file과 console에도 남을 수 있도록 handle 지정)BigQuery Data Structure:  https://jayendrapatil. com/google-cloud-bigquery/  GCP의 project 내부에 BigQuery 리소스가 존재 Dataset 안에 Table, Views . . 참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 15,
    "url": "http://localhost:4000/FastAPI-2/",
    "title": "FastAPI - 2",
    "body": "2023/07/04 - Event Handler:  이벤트가 발생했을 때, 그 처리를 담당하는 함수 FastAPI 에선 Application을 실행, 종료할 때 특정 함수를 실행할 수 있음``` @app. on_event( shutdown )```* startup 할 때 머신러닝 모델 load* shutdown 할 때 로그 저장```pythonfrom fastapi import FastAPIimport uvicornapp = FastAPI()items = {}@app. on_event( startup )def startup_event():  print( Start Up Event )  items[ foo ] = { name :  Fighters }  items[ bar ] = { name :  Tenders }@app. on_event( shutdown )def shutdown_event():  print( Shutdown Event! )  with open( log. txt , mode= a ) as log:    log. write( Application shutdown )@app. get( /items/{item_id} )def read_items(item_id: str):  return items[item_id]API Router:  API router는 더큰 애플리케이션들에서 많이 사용되는 기능 API endpoint를 정의 Python subpackage 모듈   API router는 mini FastAPI로 여러 API를 연결해서 사용   기존에 사용하던 @app. get @app. post를 사용하지 않고, router 파일을 따로 설정하고 app에 import 해서 사용함123456789101112131415161718192021222324252627282930313233343536from fastapi import FastAPI, APIRouterimport uvicornuser_router = APIRouter(prefix= /users )order_router = APIRouter(prefix= /orders )@user_router. get( / , tags=[ users ])def read_users():  return [{ username :  Rick }, { username :  Morty }]@user_router. get( /me , tags=[ users ])def read_user_me():  return { username :  fakecurrentuser }@user_router. get( /{username} , tags=[ users ])def read_user(username: str):  return { username : username}@order_router. get( / , tags=[ orders ])def read_orders():  return [{ order :  Taco }, { order :  Burritto }]@order_router. get( /me , tags=[ orders ])def read_order_me():  return { my_order :  taco }@order_router. get( /{order_id} , tags=[ orders ])def read_order_id(order_id: str):  return { order_id : order_id}app = FastAPI()if __name__ == '__main__':  app. include_router(user_router)  app. include_router(order_router)  uvicorn. run(app, host= 0. 0. 0. 0 , port=8000) user router, order router 2개 생성 app에 연결 - include_router 실제 활용한다면 하나의 파일에 저장하지 않고 각각 저장해서 사용     user. py, order. py   프로젝트 구조 예제: Error Handling:  웹 서버를 안정적으로 운영하기 위해 반드시 필요한 주제 서버에서 Error가 발생한 경우, 어떤 Error가 발생했는지 알아야하고, 해당 클라이언트에 해당 정보를 전달해 대응할 수 있어야 함 서버 개발자는 모니터링 도구를 사용해 Error Log 수집 발생하고 있는 오류를 빠르게 수정할 수 있도록 예외 처리를 잘 만들 필요가 있음12345678910111213141516171819202122from fastapi import FastAPI, HTTPExceptionimport uvicornapp = FastAPI()items = {  1:  Boostcamp ,  2:  AI ,  3:  Tech }@app. get( /v1/{item_id} )async def find_by_id(item_id: int):  return items[item_id]@app. get( /v2/{item_id} )async def find_by_id(item_id: int):  try:    item = items[item_id]  except KeyError:    raise HTTPException(status_code=404, detail=f 아이템을 찾을 수 없습니다 [id: {item_id}] )  return item item_id가 1~3 까진 정상 4이상의 숫자가 들어올 경우 key error가 발생   Internal Server Error, 500 return   클라이언트는 어떤 에러가 난 것인지 정보를 모름 자세한 에러를 보려면 서버에 직접 접근해서 로그를 확인해야 함   에러 핸들링을 위해서는 에러 메세지와 에러의 이유 등을 클라이언트에 전달하도록 코드를 잘 작성해야 함   FastAPI의 HTTPException은 Error response를 더 쉽게 봴 수 있도록 하는 클래스 HTTPException을 이용해서 클라이언트에게 더 자세한 에러 메세지를 보내는 코드 작성Background Task:  FastAPI는 Starlett이라는 비동기 프레임워크를 래핑해서 사용   Background Task 기능은 오래 걸리는 작업들을 background에서 실행 함   CPU 사용이 많은 작업들을 background로 실행하면, 클라이언트는 작업 완료를 기다리지 않고 즉시 response를 받아볼 수 있음     Example) 특정 작업 후 이메일 전송   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# 1. simple long-running tasksimport contextlibimport jsonimport threadingimport timefrom datetime import datetimefrom time import sleepfrom typing import Listimport requestsimport uvicornfrom fastapi import FastAPI, BackgroundTasksfrom pydantic import BaseModel, Fieldclass Server(uvicorn. Server):  def install_signal_handlers(self):    pass  @contextlib. contextmanager  def run_in_thread(self):    thread = threading. Thread(target=self. run)    thread. start()    try:      while not self. started:        time. sleep(1e-3)      yield    finally:      self. should_exit = True      thread. join()def run_tasks_in_fastapi(app: FastAPI, tasks: List):       FastAPI Client를 실행하고, task를 요청합니다  Returns:    List: responses       config = uvicorn. Config(app, host= 127. 0. 0. 1 , port=5000, log_level= error )  server = Server(config=config)  with server. run_in_thread():    responses = []    for task in tasks:      response = requests. post( http://127. 0. 0. 1:5000/task , data=json. dumps(task))      if not response. ok:        continue      responses. append(response. json())  return responsesapp_1 = FastAPI()def cpu_bound_task(wait_time: int):  sleep(wait_time)  return f task done after {wait_time} class TaskInput(BaseModel):  wait_time: int = Field(default=1, le=10, ge=1)@app_1. post( /task )def create_task(task_input: TaskInput):  return cpu_bound_task(task_input. wait_time)tasks = [{ wait_time : i} for i in range(1, 10)]start_time = datetime. now()run_tasks_in_fastapi(app_1, tasks)end_time = datetime. now()print(f Simple Tasks: Took {(end_time - start_time). seconds} )# 2. background tasksapp_2 = FastAPI()@app_2. post( /task ,      status_code=202) # 비동기 작업이 등록됐을 때, HTTP Response 202 (Accepted)를 보통 리턴합니다. https://developer. mozilla. org/en-US/docs/Web/HTTP/Status/202async def create_task_in_background(task_input: TaskInput, background_tasks: BackgroundTasks):  background_tasks. add_task(cpu_bound_task, task_input. wait_time)  return  ok start_time = datetime. now()run_tasks_in_fastapi(app_2, tasks)end_time = datetime. now()print(f Background Tasks: Took {(end_time - start_time). seconds} )# 3. background tasks with in-memory task repofrom uuid import UUID, uuid4app_3 = FastAPI()class TaskInput2(BaseModel):  id_: UUID = Field(default_factory=uuid4)  wait_time: inttask_repo = {}def cpu_bound_task_2(id_: UUID, wait_time: int):  sleep(wait_time)  result = f task done after {wait_time}   task_repo[id_] = result@app_3. post( /task , status_code=202)async def create_task_in_background_2(task_input: TaskInput2, background_tasks: BackgroundTasks):  background_tasks. add_task(cpu_bound_task_2, id_=task_input. id_, wait_time=task_input. wait_time)  return task_input. id_@app_3. get( /task/{task_id} )def get_task_result(task_id: UUID):  try:    return task_repo[task_id]  except KeyError:    return None Background Task를 사용하지 않은 작업들은 작업 시간 만틈 응답을 기다려야 함 작업 결과물을 조회할 때는 task를 어딘가에 저장해두고, GET 요청을 통해 task가 완료됐는지 확인     GET을 통해 리소스가 있는 확인   FastAPI 학습: 프로젝트 구조:  Cookiecutter 프로젝트 구조에 대한 템플릿 공유 https://github. com/cookiecutter/cookiecutter CLI 형태로 프로젝트 생성 과정을 도와줌   개인용 템플릿을 만들어보는 것도 좋은 방법(개인 설정 템플릿화)   처음 진행은 스크래치 부터 작성해서 익숙해지기 익숙해지고 다른 프로젝트 구조 참고 클린 아키텍쳐 관련 책 보면서 고민하기객체 지향:  현재 가지고 있는 코드를 Class로 변경해보기 pydantic Use Case 찾아보기Trial and Error:  코드 작성 -&gt; 수정 -&gt; 코드 작성 반복 작은 규모의 프로젝트부터 하나씩 만들어보기     기능 정의 후 하나씩 구현   명확한 목표    참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://fastapi. tiangolo. com/ko/"
    }, {
    "id": 16,
    "url": "http://localhost:4000/FastAPI-1/",
    "title": "FastAPI - 1",
    "body": "2023/07/04 - FastAPI:  최근 떠오르는 Python Web Framework API document 작성을 자동으로 해주는 Swagger 간결한 코드 작성 빠른 속도 출처 - https://quintagroup. com/services/python/fastapi Path Parameter, Query Parameter:  웹에서 get method를 사용해 데이터를 전송할 수 있음 ID가 402인 사용자 정보를 가져오고 싶은 경우Path Parameter 방식:  /users/402  서버에 402라는 값을 전달하고 변수로 사용Query Parameter 방식:  /users?id=402  Query String API뒤에 입력 데이터를 함께 제공하는 방식으로 사용 Query String은 Key, Value의 쌍으로 이루어지며 &amp;로 연결해 여러 데이터를 넘길 수 있음Path vs Query:  언제 어떤 방식을 사용해야 할까? 상황마다 다름Example1):  어떤 리소스를 식별하고 싶은 경우(그러나 kyle이라는 유저가 없음) Path : /users/Kyle     경로에 존재하는 내용이 없으면 404 에러 발생    Query : /users?name=kyle     데이터가 없는 경우 빈 리스트 -&gt; 추가적인 에러 핸들링   보편적인 경우  리소스 식별 : Path parameter가 적합 정렬, 필터링 : Query parameter가 적합 Path Parameter:  GET method : 정보를 Read하기 위해 사용 유저 정보에 접근하는 API12345678from fastapi import FastAPIimport uvicornapp = FastAPI()@app. get( /users/{user_id} )def get_user(user_id):  return { user_id : user_id} FastAPI는 데코레이터로 GET, POST 표시 @app. get, @app. post GET method의 인자로 있는 {user_id}가 함수의 값으로 인풋 Query Parameter:  URL뒤에 ? 붙이고 Key, Value 형태로 연결 Localhost:8000/items/?skip=0&amp;limit=10uvicorn main:app --reload 1234567891011from fastapi import FastAPIimport uvicornapp = FastAPI()# DB 같은 느낌으로 사용fake_items_db =[{ item_name :  Foo }, { item_name :  Bar }, { item_name :  Baz }]@app. get( /items/ )def read_items(skip: int = 0, limit: int = 10):  return fake_items_db[skip:skip+limit]Optional Path:  특정 파라미터는 선택적으로 사용하고 싶은 경우 Typing 모듈의 Optional 사용 Optional을 이용해 이 파라미터는 Optional 임을 명시Request Body:  클라이언트에서 API에 데이터를 보낼 때 request body 사용     client -&gt; API : request body   API response -&gt; client : response body    Request body에 데이터가 항상 포함되어야 하는 것은 아님 Request body에 데이터를 보내고 싶다면 POST Method 사용     GET Method는 URL, Request header로 데이터 전달    POST Method는 Request body에 데이터를 넣어 보냄 Body의 데이터를 설명하는 content-type Header field 존재, 데이터 타입 명시 해야함     Application/x-www-form-urlencoded: BODY에 Key, Value 사용   Text/plain : 단순 txt 파일   Multipartform-data : 데이터를 바이너리 데이터로 전송   1234567891011121314151617from typing import Optionalfrom fastapi import FastAPIimport uvicornfrom pydantic import BaseModelclass Item(BaseModel):  name: str  description: Optional[str] = None  price: float  tax: Optional[float] = None  app = FastAPI()@app. post( /items/ )def create_item(item: Item):  return item POST 요청으로 item을 생성하는 예제 pydantic으로 request body 데이터 정의 Type hinting에 위에서 생성한 클래스 주입 request body 데이터를 validationResponse Body:  API response -&gt; client : response body 데코레이터의 response_model 인자로 주입 가능12345678910111213141516171819202122from typing import Optionalfrom fastapi import FastAPIimport uvicornfrom pydantic import BaseModelclass ItemIn(BaseModel):  name: str  description: Optional[str] = None  price: float  tax: Optional[float] = None  class ItemOut(BaseModel):  name: str  price: float  tax: Optional[float] = None  app = FastAPI()@app. post( items/ , response_model=ItemOut)def create_item(item: ItemIn):  return item Output 데이터를 해당 정의에 맞게 변형 데이터 validation response에 대한 JSON Schema 추가 자동으로 문서화Form:  Form 입력 형태로 데이터를 받고 싶은 경우 Form을 사용하려면 pip install python-multipart 으로 설치     +간단한 프론트를 위한 pip install Jinja2   123456789from fastapi import FastAPI, Formimport uvicornapp = FastAPI()@app. post( /login )# Form에서 입력된 값을 가져와서 사용한다def login(username: str = Form(. . . ), password: str = Form(. . . )):  return { username : username} Form 클래스를 사용하면 request의 form data에서 값을 가져옴 Localhost:8000/login/ 으로 이동     login으로 접근해서 GET method가 요청됨   123456789101112131415from fastapi import FastAPI, Form, Requestfrom fastapi. templating import Jinja2Templatesimport uvicornapp = FastAPI()template = Jinja2Templates(directory= . / )@app. get( /login/ )def get_login_form(request: Request):  # login_form. html for frontend login page  return template. TemplateResponse( login_form. html , context={ request : request})@app. post( /login/ )def login(username: str = Form(. . . ), password: str = Form(. . . )):  return { username : username} login_form. html 로 login 페이지 구성 제출을 누르면 login 함수 실행(POST 요청) Form(…) -&gt; Python ellipsis, 필수적인 요소를 의미     FastAPI 웹 서버 실행 후 Swagger로 이동하면 required를 볼 수 있음   File:  File 업로드의 경우 Python-multipart 설치 필요1234567891011121314151617181920212223242526272829303132from typing import Listfrom fastapi import FastAPI, File, UploadFilefrom fastapi. responses import HTMLResponseimport uvicornapp = FastAPI()@app. post('/files/')def create_files(files: List(Bytes) = File(. . . )):  return {'file_sizes': [len(file) for file in files]}@app. post('/uploadfiles/')def create_upload_files(files: List(UploadFile) = File(. . . )):  return {'filenames': [file. filename for file in files]}@app. get('/')def main():  content =    &lt;body&gt;&lt;form action= /files/  method= post  enctype= multipart/form-data &gt;&lt;input type= file  name= files  multiple&gt;&lt;input type= submit &gt;&lt;/form&gt;&lt;form action= /uploadfiles/  method= post  enctype= multipart/form-data &gt;&lt;input type= file  name= files  multiple&gt;&lt;input type= submit &gt;&lt;/form&gt;&lt;/body&gt;     return HTMLResponse(content) ”/”로 접근할 때 보여줄 HTML 코드 HTML에서 action으로 넘김 파일을 bytes로 표현 여러 파일은 List에 설정Pydantic:  FastAPI에서 Class를 사용할 때 Data validation / Setting management 라이브러리 Type Hint를 런타임에서 강제해 안전하게 데이터 핸들링 파이썬 기본 타입(str, int . . ) + List, Dict, Tuple 에 대한 validtation 지원 기존 validation 라이브러리 보다 빠름 config를 효과적으로 관리 머신러닝 feature data validation으로도 활용 가능Validation: Machine learning model의 input을 validation 하는 경우 Online serving에서 input 데이터에 대한 validation    여러가지 validation을 하는 로직이 있을 수 있음     사용할 수 있는 방법은 Python class, Dataclass, Pydantic  Pydantic Config:  Validation 처럼 pydantic은 BaseSettings를 상속한 클래스에서 Type Hint로 주입된 설정 데이터를 검증할 수 있음 Field 클래스의 env 인자로, 환경 변수로 부터 해당 필드를 오버라이딩 가능 yaml, ini 파일들을 추가적으로 만들지 않고, . env 파일들을 환경변수로 만들어 두거나, 실행 환경에서 유연하게 오버라이딩 할 수 있음 참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 17,
    "url": "http://localhost:4000/Backend/",
    "title": "Backend Overview",
    "body": "2023/07/03 - 서버 구성 Use Case:  앱/웹 서비스의 서버 머신러닝 서버 하나의 큰 서버가 사용 영역 별로 분할 되어 사용되는 경우 서비스 서버에서 머신러닝 서버로 예측 요청하며 통신하는 경우서버의 형태: 모놀로식 아키텍쳐(Monolothic Architecture):  하나의 큰 서버 모든 것을 하나의 큰 서버에서 처리 전체 서버를 배포해야해서 배포가 느림 마이크로 서비스 아키텍쳐(Micro Service Architecture - MSA):  개별의 서버로 구성하고 서로 통신하도록 하는 경우Rest API:  REST API는 정보를 주고 받을 때 널리 사용되는 형식으로 생각하면 된다 REST라는 형식의 API     각 요청이 어떤 동작이나 정보를 위한 것을 요청 모습 자체로 추론 할 수 있음   기본적인 데아터 처리 CRUD : Create, Read, Update, Delete    Representational State Transfer의 약자     Resource, Method, Representation of Resource로 구성    클라이언트 : 요청을 하는 플랫폼     브라우저 같은 웹   앱   우리가 파이썬을 사용해 요청하는 것도 클라이언트    Resource : Unique한 ID를 가지는 리소스, URI Method : 서버에 요청을 보내기 위한 방식 : GET, POST, PUT, PATCH, DELETEURI, URL:  URL : Uniform Resource Locator, 인터넷 상 자원의 위치 URI : Uniform Resource Identifier, 인터넷 상의 자원을 식별하기 위한 문자열의 구성 URI는 URL을 포함, URI &gt; URLHTTP Method: GET vs POST:  GET : 정보 요청을 위해 사용(Read)     어떤 정보를 가져와서 조회하기 위해 사용되는 방식   URL에 변수(데이터)를 포함시켜서 요청함   데이터를 헤더에 포함하여 전송함   URL에 데이터가 노출되어 보안에 취약   캐싱할 수 있음(다른 방법과 비교해서 빠를 수 있음)      POST : 정보를 입력하기 위해 사용(Create)      데이터를 서버로 제출해 추가 또는 수정하기 위해 사용하는 방식   URL에 변수(데이터)를 노출하지 않고 요청   데이터를 Body에 포함   URL에 데이터가 노출되지 않아 기본 보안은 되어 있음   캐싱 불가능(그 안에 아키텍쳐로 캐싱은 가능함)    출처 - https://stackoverflow. com/questions/43934585/which-http-method-get-or-post-i-should-use-for-creating-php-restfull-login-api PUT, PATCH, DELETE:  PUT : 정보를 업데이트하기 위해 사용(Update) PATCH : 정보를 업데이트하기 위해 사용(Update) DELETE : 정보를 삭제하기 위해 사용(Delete)Header, Body:  Http 통신은 Request를 하고, Response를 받을 때 정보를 Packet(패킷)에 저장   Packet 구조: Header/Body   Header : 보내는 주소, 받는 주소, 시간 Body : 실제 전달하려는 내용 Status Code:    클라이언트의 요청에 따라 서버가 어떻게 반응하는지 알려주는 코드   1xx : 요청을 받았고, 프로세스를 진행함 2xx : 요청을 성공적으로 받았고 실행함 3xx : 요청 완료를 위한 추가 작업 필요 4xx : 요청 문법이 잘못되었거나 요청을 처리할 수 없음 5xx : 서버가 요청에 대해 실패함동기와 비동기(Sync, Async): 동기(Sync):  서버에서 요청을 보냈을 때, 응답이 돌아와야 다음 동작을 수행 할 수 있음. A 작업이 모두 완료될 때까지 B 작업은 대기비동기(Async):  요청을 보낼 때 응답 상태와 상관없이 다음 동작을 수행함. A 작업과 B 작업이 동시에 실행됨상황에 따라 동기적, 비동기적으로 구현할건지 정하면 됨 IP:  네트워크에 연결된 특정 PC주소를 나타내는 체계 Internet Protocol 4그룹의 숫자로 구성된 IP 주소 체계를 IPv4라고 함 각 그룹마다 0~255로 나타낼 수 있음용도가 정해진 경우:  localhost, 127. 0. 0. 1 : 현재 사용중인 local PC 0. 0. 0. 0, 255. 255. 255. 255 : broadcast address, 로컬 네트워크에 접속된 모든 장치와 소통하는 주소 개인 PC보급으로 누구나 PC를 사용해 IPv4로 할당할 수 있는 한계점을 진입해서 IPv6 등장Port(포트):  IP주소 뒤에서 나오는 숫자 PC에 접속할 수 있는 통로(채널) 사용중인 포트는 중복 불가 Example) 주피터 노트북은 8888 포트는 0~65535까지 존재 0~1024는 통신을 위한 규약에 정해짐 Example)     22 : SSH   80 : HTTP   443 : HTTPS    참고:  https://github. com/zzsza Naver Connection AI Tech 5th - Product Serving(변성윤) https://geekflare. com/backend-solutions-for-web-and-mobile-apps/"
    }, {
    "id": 18,
    "url": "http://localhost:4000/CICD/",
    "title": "CI/CD Basic - 1",
    "body": "2023/07/02 - 개발 환경: Local:  각자의 컴퓨터에서 개발 각 환경을 통일 시키기 위해 docker 또는 pyenv, venv 사용Development:  Local에서 개발한 기능을 테스트 하는 환경 테스트 서버Staging:  Production 환경에 배포하기 전에 운영하거나 보안, 성능을 측정하는 환경 Staging 서버Production:  실제 서비스를 운영하는 환경 운영 서버개발 환경을 나누는 것은 실제 운영중인 서비스에 장애가 생기는 것을 방지하기 위함. 만약에 dev, staging, production 환경이 동일하다면 소스 코드를 저장하는 즉시 반영이 된다고 생각하면 됨.  CI/CD란?: Continuous Integration(지속적 통합):  새로 작성한 코드 변경 사항이 Build, Test를 진행한 후 Test Case를 통과하는지 확인 지속적으로 코드 품질 관리Continuous Delivery, Deployment(지속적 배포):  작성한 코드가 신뢰 가능한 상태가 되면(CI를 통과하면) 자동으로 배포될 수 있도록 하는 과정 CI 이후 CD development, staging, main 브랜치에 merge 되는 경우 코드가 자동으로 서버에 배포간단하게 요약하자면    CI : 빌드, 테스트의 자동화     CD : 배포 자동화  CI/CD Solutions:  Jenkins, Travis CI, AWS CodeDeploy, Github Action etc. . 출처 - https://www. simform. com/blog/scalable-ci-cd-pipeline-examples/  Github Action:  Github에서 출시한 기능으로, 소프트웨어 workflow 자동화를 도와주는 도구 Test code, Deployment, Shell script Github tag, release 자동 설정 새로운 브랜치 생성시 특정 작업 실행   다양한 Workflow template이 존재 https://github. com/sdras/awesome-actions   Private repo는 유료     유료와 무료 사이의 제한이나 제약 조건, 과금에 대한 내용 찾아봐서 사용    Github Action 사용 방식:  코드 작업 작업 후, Github Action으로 무엇을 할 것인지 생각 사용할 Workflow 정의 Workflow 정의 후 정상 작동하는지 확인 Github Action Workflow: Workflow:  여러 Job으로 구성, Event로 Trigger되는 자동화된 프로세스 Workflow 파일은 YAML로 작성, 레포지토리의 . /github/workflows에 저장Event:  Workflow를 trigger하는 특정 행동, 규칙 Example     특정 브랜치로 Push   특정 브랜치로 Pull Request   특정 시간대에 반복   Jobs:  Runner에서 실행되는 Steps들의 조합 여러 Jobs이 있는 경우 병렬로 실행, 순차적 실행도 가능     다른 Job에 의존 관계를 가질 수 있음   Example) Job A Success -&gt; Run Job B   Steps:  Job에서 실행되는 개별 작업 Action을 실행하거나 쉘 커맨드 실행 하나의 Job에서 데이터를 공유할 수 있음Actions:  Workflow에서 제일 작은 단위 Job을 생성하기 위해 여러 step을 묶은 개념 재사용 가능한 componentRunner:  Github Action도 일종의 서버에서 실행되는 개념 Runner -&gt; Workflow가 실행될 서버     Github-hosted runner   Self-hosted runner    참고:  https://github. com/zzsza Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 19,
    "url": "http://localhost:4000/Cloud/",
    "title": "Cloud Computing",
    "body": "2023/06/30 - 클라우드를 사용하는 이유: What is Cloud?: Google Cloud에서 정의하는 Cloud Computing은 다음과 같다  클라우드 컴퓨팅은 컴퓨팅 리소스를 인터넷을 통해 서비스로 사용할 수 있는 주문형 서비스이다. 기업에서 직접 리소스를 조달하거나 구성, 관리할 필요가 없으며 사용한 만큼만 비용을 지불하면 된다. Why use Cloud?:    기존의 전통적인 서버실(Internet Data Center)를 운영하려면, 물리적 공간과 확장성(scalability)까지 고려를 해야 함          Example1) 트래픽이 몰리는 경우 컴퓨터 10대를 더 추가 설치하기 어려움           Example2) 트래픽이 적어지면 컴퓨터 10대를 없애야 하나?        클라우드 서비스가 점점 발전함에 따라, 개발자가 직접 설정해야 했던 작업을 클라우드에서 쉽게하는 방향으로 발전(cloud managed service) Apache Spark를 쉽게 운영할 수 있도록 AWS EMR, GCP Dataproc 활용(직접 하둡, 등을 설치 할 필요 없이 이미 설치되어 있음) 여러가지 환경을 미리 설치해두고 사용하는 것이 편함(tensorflow, CUDA) Cloud의 다양한 서비스: 다음은 Google Cloud에서 정의하는 PaaS, IaaS, SaaS이다.  SaaS(Software as a Service)     SaaS(Software as a service)는 전체 애플리케이션 스택을 제공하여 고객이 액세스하고 사용할 수 있는 전체 클라우드 기반 애플리케이션을 제공   SaaS 제품은 서비스 제공업체에서 모든 업데이트, 버그 수정, 전반적인 유지관리 등을 전적으로 관리하며 즉시 사용할 수 있다   대부분의 SaaS 애플리케이션은 웹브라우저를 통해 직접 액세스할 수 있으므로 고객이 기기에 아무것도 다운로드하거나 설치할 필요가 없다    PaaS(Platform as a Service)     PaaS(Platform as a Service)는 클라우드를 통해 애플리케이션을 개발하는 데 필요한 모든 하드웨어 및 소프트웨어 리소스를 제공하고 관리한다   개발자와 IT 운영팀은 인프라 또는 플랫폼을 자체적으로 빌드하고 유지관리할 필요 없이 PaaS를 사용하여 애플리케이션을 개발, 실행, 관리할 수 있다   고객은 여전히 코드를 작성하고 데이터와 애플리케이션을 관리해야 하지만, 클라우드 서비스 제공업체에서 앱을 빌드하고 배포하는 환경을 관리하고 유지관리한다    IaaS(Infrastructure as a Service)     IaaS(Infrastructure as a Service)는 클라우드를 통해 컴퓨팅, 스토리지, 네트워킹, 가상화와 같은 주문형 인프라 리소스를 조직에 제공한다   고객이 자체 데이터 센터 인프라를 관리, 유지관리 또는 업데이트할 필요는 없지만 운영체제, 미들웨어, 가상 머신, 앱 또는 데이터를 책임진다    출처 : https://www. stackscale. com/blog/cloud-service-models/  출처 : https://cloud. google. com/learn/paas-vs-iaas-vs-saas?hl=ko Cloud 서비스 기업:  AWS, Google Cloud, Azure, Naver Cloud Platform Cloud 제품:  Computing Service(Server)     연산을 수행하는 서비스   가상 컴퓨터, 서버, VM(virtual machine), Instance(인스턴스)   가장 많이 사용하는 제품   회사별로 월 무료 사용량이 존재      Serverless Computing          computing service와 유사하지만, 서버 관리를 클라우드쪽에 진행           코드를 클라우드에 제출하면, 그 코드를 가지고 서버를 실행해주는 형태           요청 부하에 따른 자동확장 가능(auto scaling)           Micro Service로 많이 활용          Stateless Container          Docker를 사용한 컨테이너 기반으로 서버를 실행하는 구조           Docker image를 업로드하면 해당 이미지 기반으로 서버를 실행해주는 형태          Object Storage          다양한 오브젝트를 저장할 수 있는 저장소           다양한 형태의 데이터를 저장 가능, API를 사용해 데이터에 접근 가능           머신러닝 모델의 pkl, csv 파일, 실험 로그 등을 저장할 수 있음          Database(RDB)          웹, 앱서비스와 데이터베이스가 연결되어 있는 경우가 많으며, 대표적으로 MySQL, PosgreSQL 등을 사용할 수 있음           보통 사용자 로그 데이터는 데이터베이스에 저장하지만, 저장된 데이터를 어떻게 사용하냐에 따라 Database에 저장할지, Object Storage에 저장할지 결정          Data Warehouse          데이터베이스, 스토리지에 있는 데이터 등을 모두 모아서 웨어하우스에 저장           데이터 분석에 특화된 데이터베이스           퍼포먼스 빠름       참고:  https://github. com/zzsza https://cloud. google. com/learn/paas-vs-iaas-vs-saas?hl=ko Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 20,
    "url": "http://localhost:4000/semantic_segmentation/",
    "title": "Semantic Segmentation Overview",
    "body": "2023/06/05 - Semantic Segmentation: What is Semantic Segmentation?: Semantic Segmentation은 이미지의 모든 픽셀별로 클래스를 분류하는 task 이다. 말 그대로 pixel wise classification이라서 같은 클래스에 속하더라도 객체를 구분해주지는 않는다.  출처 - CS231n Ideas used for Semantic Segmentation: Sliding Window: 이미지를 수 많은 작은 단위(crop)으로 나눠서 각자의 영역에 image classification을 진행하는 방법. 이 경우에 computation cost가 굉장히 많이 들고, crop 되는 영역이 겹칠 경우에는 인접한 feature에 대한 공통의 계산을 해야하기 때문에 비효율적이다.  출처 - CS231n Fully Convolutional Network(FCN): FC layer가 없는 convolution layer만 구성된 네트워크를 사용하는 방법이다. 이미지가 flatten되는 과정에서 공간정보를 잃게되는데, conv layer만 사용할 경우 공간정보를 유지할 수 있다.  출처 - Fully Convolutional Networks for Semantic Segmentation Downsampling 된 이미지 상태에서는 segmentation을 진행 할 수는 없기 때문에 이미지를 다시 키우는 upsampling을 진행한다. Upsampling 방법에는 여러가지가 있지만 대표적으로 deconvolution(transpose convolution) 연산을 이용한다. (수학에서 말하는 deconvolution과는 다른 연산. CNN에서 이야기하는 convolution 연산도 정확하게는 cross corelation 연산이기 때문에 용어 사용에 주의를 할 필요가 있다. ) 출처 - CS231n Upsampling: Unpooling : Nearest Neighbor, Bed of NailsFixed algorithm을 이용하는 방식. Receptive field를 복사하거나 빈자리에 0을 채워 넣는 방식이다.  Unpooling - 출처 - CS231n Deconvolution(Transpose Convolution)Unpooling의 경우 고정된 함수로 진행하는 연산이기 때문에 학습과정을 거치지 않는다. 반면에 deconvolution의 경우 upsampling하는 과정을 학습한다. Deconvolution은 아주 간단하게 생각한다면 convolution 연산을 반대로 한다고 생각하면 된다. 과정을 간단하게 설명하자면, input feature map에서 좌상단 부터 차례대로 scalar 값을 하나씩 뽑고 filter와 곱해서 그 출력을 output region 복사한다. Overlapping 되는 영역을 sum을 해준다. (아래 그림을 참고) Transposed Convolution - 출처 - Dive into Deep Learning Spatial size를 키워주기 위해서 학습가능한 filter를 사용한다고 보면 된다.  Transposed Convolution - 출처 - https://github. com/vdumoulin/conv_arithmetic ​ Deconvolution 하는 과정에서 작은 이미지를 과도하게 늘리는 과정에서 feature에 대한 정보가 유실되는 경우가 많다. 이를 해결하기 위해서 FCN에서는 skip architecture를 사용한다. Skip Architecture in FCN(Fully Convolutional Networks for Semantic Segmentation): FCN에서 이용한 skip architecture를 간단하게 설명하자면, convolution(encoding path)에서 나온 feature map을 deconvolution(decoding path)에서 나온 결과와 더해주는 것 이다. FCN 논문에서는 low layer information과 high layer information을 combine한다는 표현을 쓴다.  Skip Architecture - 출처 - Fully Convolutional Networks for Semantic Segmentation  FCN - 32, 16, 8 - 출처 - Fully Convolutional Networks for Semantic Segmentation Summary: 이런 FCN 개념을 이용한 semantic segmentation은 결국은 convolution network를 통한 downsampling/upsampling을 진행해서 pixel에 대한 cross-entropy를 계산하여 네트워크를 end-to-end로 학습 가능하다. Semantic Segmentation은 기본적인 segmentation task이며, 더 많은 요소까지 고려한 다양한 segmentation task들이 존재한다. Further Studying:  Instance Segmentation Panoptic Segmentation Depth estimation 참고:  CS231n https://github. com/vdumoulin/conv_arithmetic Fully Convolutional Networks for Semantic Segmentation Dive into Deep Learning"
    }, {
    "id": 21,
    "url": "http://localhost:4000/Debugging/",
    "title": "Debugging",
    "body": "2023/05/01 - Debugging: 디버깅이란?: 오류나 버그를 찾고 수정하는 과정. 이런 디버깅은은 보통 바라는 상황과 실제 상황이 차이가 나는 경우 행해야 하는 경우가 많다.  버그가 발생하는 이유:  휴먼 에러(실수)     개발 과정 중 문법, 로직 오류    실행 환경     OS, 가상 환경, 컨테이너, 하드웨어, 네트워크 상태 등    의존성     라이브러리에서 사용하는 다른 라이브러리의 버그로 인한 이슈    복잡성     소프트웨어가 복잡해질수록 버그 가능성이 높아질 수 있음    잘못된 커뮤니케이션     요구 사항에 대한 misunderstanding    Debugging Process: 문제 발생, 문제 인식, 해결책 찾기, 버그 기록, 버그 재현 등의 과정으로 진행할 수 있다.  출처 - https://www. javatpoint. com/debugging  꼭 기록하는 습관을 가지자 답을 찾더라도 항상 교차 검증하자 항상 목적이 뭔지 상기하면서 해결책을 찾자 Server Management:  대부분 AI, ML 모델들은 서버에서 동작한다 서버의 관리를 배울 필요성이 있다 서버 관리의 목적:  서버를 안정적으로 운영해서 장애를 발생하지 않기 위함 서버에서 작업을 원활하게 진행하기 위함 알아야할 지식:  쉘 커맨드 파일 시스템 네트워크 패키지 관리 성능 모니터링 컨테이너, 오커스트레이팅 Linux 파일 시스템:  리눅스는 파일, 폴더 구조를 일관된 방식으로 제공한다. 시스템 구성, 로그 등을 별도로 저장하며 소프트웨어를 설치하는 공간도 따로 제공한다. 출처 - https://linuxconcept. com/linux-file-system-hierarchy/  파일 공간이 부족할 때 사용하지 않는 파일들을 지워준다. 그러나 한방에 전부 날리는 일은 피하도록 하자. (sudo 명령어로 실수해서 시스템 날려먹는 일 하지말자!) 파일 시스템의 Case Study: 경로나 호스트 머신의 디스크 문제를 인식하기 전에 서버가 어떤 환경에서 실행 중인지 확인하자.  On Premise 환경, IDC와 같은 물리적으로 접근 가능한 서버 환경인지 클라우드 환경인지 Docker/Kubernetes 같은 컨테이너 환경의 여부 네트워크:    IP, DNS, port, 방화벽 등에 대한 개념을 공부하자     ping : 서버가 연결되어 있는지, 얼마자 빠른 속도로 데이터가 전송되는 테스트     nslookup : 특정 도메인을 찾을 수 있는지, DNS 서버에 연결 가능한지     netstat : 포트 개방 확인(TCP connection), 특정 포트만 확인하고 할 때 grep 활용,   ex. netstat -tnlp | grep 3000  참고:  https://www. geeksforgeeks. org/software-engineering-debugging/ Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤)"
    }, {
    "id": 22,
    "url": "http://localhost:4000/objectdetection/",
    "title": "Object Detection Overview",
    "body": "2023/04/29 - Object Detection: Classification + Localization: 하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인 아키텍쳐의 경우 CNN 네트워크에 class score를 정해주는 FC layer와 box coordinate을 정해주는 FC layer로 구성된다. 이 경우 계산하는 loss는 두 개이고, 학습하는 data는 이미지의 클래스 레이블과 bounding box가 정해진 ground truth를 가진 형태이다. 학습을 할 때 두 개의 loss에 대한 가중합을 학습한다. 이 때 각 loss에 대한 가중치를 hyper parameter 형태로 조절해줘야하고, 이런 multi-loss에 대한 hyper parameter를 결정하는 작업은 까다롭다.  two losses for classification and localization - 출처 : CS231n What is Object Detection?: 앞서 말한 classification + localization과는 다르게 객체가 여러개가 존재해서, 각 객체에 대한 bounding box와 class를 정해줘야 하는 task이다. Object detection은 결국 하나의 이미지내에서 multi object에 대한 classification and localization을 한다고 보면 된다(학습을 위해 사용하는 방법은 다름).  출처 - YOLOv3: An Incremental Improvement Ideas Used for Object Detection: Sliding Window: Object detection을 위해 초기에 시도 되었던 방법이다. Window를 이미지 내에서 sliding(이동) 시키면서 모든 window의 경우에 대해 classification을 진행하는 방식이다. Semantic segmentation의 sliding window와 거의 동일하다고 볼 수 있다. 이런 brute force 방식의 접근은 결국 너무 높은 computational cost를 요구하기 때문에, 특히 높은 용량의 데이터를 다루는 computer vision에서 지양해야한다.  sliding window Region Proposal(RoI - Regions of Interest): 객체가 있을 법한 후보군들의 region 찾아서 그 region들에 대해 CNN 네트워크의 입력으로 주는 방식이다. 아주 간단하게 설명하자면, region proposal을 뽑아서 CNN 네트워크의 입력으로 줘서 object detection을 수행한다고 보면 된다. 이 경우 모든 경우의 수 말고 몇몇(~2k)개의 후보군만 확인하면 되는 방식이라고 brute force보다 효율적이다. Region proposal 방식은 R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)의 논문에 처음 소개 되었다. R-CNN: R-CNN은 selective search라는 전통적인 알고리즘 기법(학습 x)을 통해 RoI를 만들어낸다. 뽑힌 RoI의 사이즈는 다양하기 때문에, classification을 위한 CNN 네트워크의 입력으로 주기 위해서는 image의 사이즈를 FC layer에 들어갈 수 있게 전부 동일하게 고정된 사이즈로 변경해줘야 한다. 이 때 변경된 이미지는 warped image region이라고 한다. Warped images 각각은 CNN 네트워크에 통과 시키고 classification을 위해서 SVM(2014년임을 고려하자)을 사용한다. 또한, RoI를 보정하기 위한 regression 과정도 거친다. 보정의 경우 RoI와 bounding box가 일치하지 않는 경우를 보정하기 위한 offset 값을 4개 예측 해준다고 보면 된다.  R-CNN architecture - 출처 : CS231n ​ R-CNN 방식은 그럼에도 불구하고 selective search를 통한 CNN 연산을 2000번 넘게 연산으로 인해 computational cost가 높고, selective search도 cpu를 통한 연산이기 때문에 상대적으로 속도가 느리다. 이를 해결하기 위해 Fast R-CNN이 등장 했다. Fast R-CNN: Fast R-CNN의 경우 앞서 말한 RCNN과 동일하게 selective search라는 region proposal method를 사용하지만, RCNN과 다른점은 selective search로 구한 RoI 각각을 CNN 네트워크를 통과시키는 것이 아니라 input image 하나에 대해 CNN에 통과시키고, RoI들은 축소된 형태로 여러가지 사이즈로 feature map에 나타난다. 이렇게 한다면, RCNN과 다르게 입력 이미지에 대해서만 CNN을 연산하고, 각각의 RoI에 대해서 classification과 regression을 진행하기 때문에 더 효율적이다. 조금 더 세부하게 과정을 살피자면, feature map의 RoI들은 사이즈가 제각각 이기 때문에 RoI pooling이라는 과정을 통해서 FC layer의 입력에 넣을 수 있도록 고정된 크기의 vector로 변환해야한다. 이 RoI feature vector는 softmax 연산을 통해 classification을 하고, bounding box regression으로 bounding box를 위한 보정값을 예측한다.  Fast R-CNN architecture - 출처 : Fast R-CNN, CS231n Fast R-CNN에서도 한계점은 있다. 학습 시키지 않는 전통적인 알고리즘 기반의 selective search가 bottleneck의 원인 되었고, 이를 해결하기 위해서 Faster R-CNN이라는 방법이 등장했다. Faster R-CNN: 앞의 Fast R-CNN의 한계는 결국 region proposal 단계의 bottleneck 때문이다. Faster R-CNN은 이를 해결하기 위해 네트워크가 region proposal을 학습 할 수 있는 형태로 바꾼다. 간단히 말하자면 RPN(Region Proposal Network)를 RoI pooling과 함께 GPU단에서 해결 할 수 있도록 설계되었다.  Faster R-CNN architecture - 출처 : CS231n 지금까지 설명한 R-CNN 계열의 네트워크들은 region based method를 사용하는 2 stage detector 방식이다. 이 다음으로 1 stage detector와 2 stage detector의 차이 그리고 대표적인 1 stage detector 방식인 YOLO를 살펴보자.  R-CNN comparison - 출처 : Recent Advances in Deep Learning for Object Detection 1-stage detector vs 2-stage detector: 1 stage vs 2 stage - 출처 : A Survey of Deep Learning-Based Object Detection 2 stage detector:  localization과 classification을 순차적으로 해결 속도가 느림 RCNN family가 대표적인 2 stage detector1 stage detector:  localization과 classification을 동시에 해결 feature extraction과 object detection이 전체 이미지에 대해 이루어지는 간단한 디자인 속도가 빠름(real time detection이 가능) 낮은 background error YOLO가 대표적인 1 stage detectorMetrics For Object Detection: Precision and Recall: precision and recall - 출처 : https://www. datacamp. com/tutorial/precision-recall-curve-tutorial  accuracy - 출처 : https://www. datacamp. com/tutorial/precision-recall-curve-tutorial    Precision(정확도) - 올바르게 탐지한 물체의 수(TP) / 모델이 탐지한 물체의 수(TP+FP)     Recall(재현율) - 올바르게 탐지한 물체의 수(TP) / 실제 정답 물체의 수(TP+FN)  정확도와 재현율에 따른 Trade Off: 모든 영역에 대하여 전무 물체가 존재한다고 판단을 하는 경우, 재현율은 높아지지만 정확도가 떨어진다. 확실할 때만(confident 한 경우만) 물체가 존재한다고 판단을 하는 경우, 정확도는 높아지지만, 재현율이 떨어진다. Mean Average Precision (mAP): 출처 - End-to-end training of object class detectors for mean average precision  출처 - End-to-end training of object class detectors for mean average precision  Precision과 recall은 보통 반비례 관계를 가진다.  mAP를 계산하기 위해서는 우선 각 클래스의 average precision(AP)를 계산한다. AP는 각 precision-recall 그래프의 넓이로 계산 할 수 있다. 그 다음 모든 AP들의 평균을 계산하면 mAP를 구할 수 있다. Intersection over Union (IoU): True Positive(TP)와 False Positive(FP)를 결정하는 기준으로 IoU를 사용한다. IoU를 간략히 설명하자면 두 바운딩 박스가 겹치는 비율로 생각 할 수 있다.  출처 - pyimagesearch. com/2016/11/07/intersection-over-union-iou-for-object-detection/  출처 - pyimagesearch. com/2016/11/07/intersection-over-union-iou-for-object-detection/  mAP@0. 5는 ground truth와 prediction의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미 NMS 계산의 경우, 같은 클래스끼리 IoU가 50% 이상일 때 낮은 confidence의 bounding box를 제거한다Non Maximum Suppression (NMS): 의미만 해석하자면 제일 큰 것을 제외하고 나머지는 suppress 하자는 의미이다. Object detection에서 하나의 instance에는 하나의 bounding box가 적용되어야 한다. 그렇게 때문에 하나의 물체에 대해 여러개의 bounding box가 겹쳐져 있을 경우 하나로 합치는 방법이 필요하다. NMS는 보통 IoU가 특정 threshold를 넘어가는 중복되는 box를 제거하는 방식이다.  출처 - https://wikidocs. net/163295 YOLO (You Look Only Once): YOLO timeline - 출처 : https://www. researchgate. net/figure/Timeline-of-You-Only-Look-Once-YOLO-variants_fig1_369379818  YOLO v1 : 하나의 이미지의 bbox와 classification을 동시에 예측하는 1 stage detector의 등장 YOLO v3 : multi scale feature map의 사용 YOLO v5 : small, medium, large 크기 별로 모델 구성Unified Detection in YOLO: detection in YOLO - 출처 : You Only Look Once YOLO에서 object detection을 regression problem으로 전환해서 풀고 있다. 또한 간단한 구조의 end to end 네트워크를 사용하고 있다. YOLO에서 unified detection의 과정을 설명하자면, 입력 이미지를 SxS 그리드 영역으로 나누고 각 그리드 영역마다 B개의 bounding box와 confidence score를 계산하고 C개의 클래스에 대해서 해당 클래스일 확률을 계산한다. (parameter used in YOLO, S=7, B=2, C=20) Confidence score는 그리드에서 (object가 존재할 확률) x (ground truth와 IoU)로 계산한다. 수식으로 표현하면 다음과 같다. \(confidence = Pr(object) \times IoU^{truth}_{pred}\)C개의 클래스에 대한 해당 클래스일 확률은 다음과 같이 계산한다. \(class probability = Pr(Class_{i}|Object)\)간단하게 요약을 하자면, input 이미지가 들어오면 이미지를 7x7(49개)의 그리드로 나누고, 각 그리드 마다 2개의 bounding box와 confidence score를 계산하고, 각 그리드(49개의 그리드)마다 어떤 클래스인지 해당 클래스일 확률을 계산한다. Network Design of YOLO: YOLO's architecture - 출처 : You Only Look Once    googlenet의 변형 사용     24 conv layers for feature extraction     2 fully connected layers for prediction  output tensor of YOLO - 출처 : https://wikidocs. net/187967  output은 7x7x30으로 나온다 bbox 1, 2에 대한 (x좌표, y좌표, 너비, 높이, confidence score) + 20개 클래스에 대한 확률Inference: 각 bounding box의 confidence score를 20개의 클래스 확률과 곱해서 각 bounding box가 각 클래스에 대해 어떤 클래스에 해당하는지 확률을 계산한다.  YOLO test time - 출처 : You Only Look Once 49개의 그리드당 2개의 bounding box를 계산하기 때문에 총 98개의 box에 대한 class specific confidence score를 계산한다. Advantages and Disadvantages of YOLO: 장점:  속도가 빨라서 real time detection에 활용 가능하다 물체의 일반화된 특징을 학습하기 때문에 새로운 도메인의 이미지에 대한 좋은 성능을 보인다단점:  그리드보다 작은 크기의 물체 검출이 불가능하다 신경망을 통과할 때 마지막 feature만 사용하기 때문에 정확도가 하락한다Further Studying:  Object Detection의 milestone을 확인해보기 YOLO vs SSD 비교해보기 최근 SOTA Object Detection 모델들의 트렌드 살펴보기 참고:  You Only Look Once: Unified, Real-Time Object Detection End-to-end training of object class detectors for mean average precision Rich feature hierarchies for accurate object detection and semantic segmentation boostcourse : 재활용 쓰레기를 활용한 딥러닝 - Detection CS231n"
    }, {
    "id": 23,
    "url": "http://localhost:4000/Docker-Basic-1/",
    "title": "Docker Basic - 1",
    "body": "2023/04/28 - Virtualization: 가상화란 무엇인가: 가상화는 서버, 스토리지, 네트워크 및 기타 물리적 시스템에 대한 가상 표현을 생성하는데 사용할 수 있는 기술이다. 가상 소프트웨어는 물리적 하드웨어 기능을 모방하여 하나의 물리적 머신에서 여러 가상 시스템을 동시에 실행할 수 있다. 가상화는 간단하게 말하자면 Local이나 Production 서버에서의 환경을 위한 일종의 템플릿이라고 생각할 수 있다.  개발과 운영 서버의 환경 불일치를 해소할 수 있고, 어느 조건에서나 동일한 환경으로 프로그램을 실행 할 수 있게 된다.  Virtual Machine vs Docker: VM(Virtual Machine): 도커의 등장 전에는 주로 VM(Virtual Machine)을 사용했다. VM은 호스트 머신이라고 하는 실제 물리적인 컴퓨터 위에 OS를 포함한 가상화 소프트웨어를 두는 방식이라고 이해하면 된다. 그러나 이러한 방식은 OS 위에 OS를 하나 더 실행시킨다는 점에서 굉장히 많은 리소스를 사용하게 된다(무겁다).  Container: 컨테이너는 어떤 환경에서나 실행하기 위해 필요한 모든 요소를 포함하는 소프트웨어 패키지같은 형태이다. 컨테이너의 정의를 찾아보면,  소프트웨어 서비스를 실행하는 데 필요한 특정 버전의 프로그래밍 언어 런타임 및 라이브러리와 같은 종속 항목과 애플리케이션 코드를 함께 포함하는 경량 패키지이다. 으로 정의된다.  출처 - https://www. weave. works/blog/a-practical-guide-to-choosing-between-docker-containers-and-vms VM의 경우 host OS 위에 다시 guest OS가 존재하는 형태인 반면에, 컨테이너는 OS 하나 위에서 OS에 상관없이 컨테이너를 띄우는 것을 볼 수 있다.  Docker: 도커 소개: 도커는 이런 컨테이너에 기반한 개발과 운영을 매우 빠르게 확장 할 수 있는 오픈소스 프로젝트이다. 도커에 대해 간단히 설명하자면, 도커의 이미지를 만들어두면 재부팅 할 경우 도커의 이미지 상태로 다시 실행이 된다고 보면 된다.  출처 - https://medium. com/swlh/understand-dockerfile-dd11746ed183  Docker Image : 컨테이너를 실행할 때 사용할 수 있는 Template (Read only) Docker Container : Docker Image를 활용해 실행된 인스턴스 (Write allowed) 도커로 할 수 있는 일: 다른 사람이 만든 소프트웨어를 가져와서 바로 사용 할 수 있음  MySQL을 도커로 실행 Jupyter Notebook을 도커로 실행이 때 다른 사람이 만든 소프트웨어를 Docker Image라고 이해하면 되고, OS를 포함한 실행 환경이 저장되어 있다.  Linux, Windows 등 어디서나 동일하게 실행할 수 있다.  도커로 MySQL 실행 해보기: 도커 실행:    docker명령어로 도커 동작 확인     docker pull mysql:8로 mysql 8 버전의 이미지를 다운     docker images로 다운 받은 이미지 확인     docker run --name mysql-tutorial -e MYSQL_ROOT_PASSWORD=0000 -d -p 3306:3306 mysql:8          다운 받은 MySQL 이미지 기반으로 docker container를 만들고 실행           --name mysql-tutorial : 컨테이너의 이름을 mysql-tutorial 로 정하겠다는 것. 설정 하지 않으면 랜덤으로 생성 됨           -e MYSQL_ROOT_PASSWORD=0000 : 환경변수 설정을 하는 부분. 사용하는 이미지에 따라 설정이 다르지만, 현재 하고 있는 MySQL의 경우 환경변수를 통해 root 계정의 비밀번호를 설정하고 있음.           -d : 데몬(백그라운드) 모드. 컨테이너를 백그라운드 상태로 실행. 이 설정을 하지 않을 경우, 현재 실행하는 셸 위에서 컨테이너가 실행되고 컨테이너의 로그를 바로 볼 수 있지만, 컨테이너를 나갈 경우 실행이 종료 됨.           -p 3306:3306 : 포트 지정. -p {localhost port}:{container port} 형태로, 현재의 경우 로컬 포트 3306으로 접근 시 컨테이너 포트 3306으로 연결되도록 설정. MySQL은 기본적으로 3306 포트로 통신함.          docker ps 로 실행한 컨테이너와 정보를 확인 할 수 있음     docker exec -it mysql-tutorial /bin/bash MySQL이 실행되고 있는지 확인하기 위해 컨테이너로 진입 할 수 있다. Compute engine에서 SSH와 접속하는 것과 유사하다.      docker exec -it {container name or ID} /bin/bash      mysql -u root -p MySQL 프로세스로 들어가면 MySQL 쉘 화면이 보인다.     docker stop {container name or ID} 실행 중인 컨테이너를 멈출 수 있다.     docker ps -a로 작동을 멈춘 컨테이너를 확인 할 수 있다. docker ps의 경우 실행중인 컨테이너 목록만 보여줌.     docker rm {container name or ID} 으로 멈춘 컨테이너 삭제 가능      docker rm {container name or ID} -f 로 실행중인 컨테이너도 삭제 가능    Volume mount:    Docker run 할 때 파일이 자동으로 공유가 되는 것이 아님. 호스트와 컨테이너를 연결(sync) 해주는 것이 volume mount.     docker container는 특별한 설정이 없으면 컨테이너를 삭제할 때 파일이 사라짐     Host와 container는 처음부터 파일 공유가 되지 않음     파일을 유지하고 싶을 경우 host와 container의 저장소를 공유해야 함     Volume mount를 진행하면 host와 container의 폴더가 공유됨     -v 옵션을 사용하며 port 처럼 사용함. -v host_folder:container_folder     ex. docker run -it -p 8888:8888 -v /some/host/folder/for/work : /home/workspace/jupyter/note  DockerHub: 필요한 이미지가 있을 경우, 공개된 모든 이미지를 다운받을 수 있다.  Docker Image 만들기: pytorch example 코드를 실행하는 docker image 생성 해보기 Dockerfile 생성:  vi Dockerfile 로 만들든 gui로 만들든 Dockerfile을 생성해서 필요한 내용을 작성한다. 1From pytorch/pytorch:1. 13. 1-cuda11. 6-cudnn8-runtime From {image name}:{tag} 형식은 이미지 빌드에 사용할 베이스 이미지를 지정하는 것이다. 보통 공개된 이미지 기반으로 새로운 설정을 추가하는 방법으로 사용. 1copy . /app   copy {로컬 디렉토리(파일)} {컨테니어 내 디렉토리(파일)} 컨테이너는 자체적인 파일 시스템을 가짐. copy명령어는 dockerfile이 존재하는 경로 기준 로컬 디렉토리를 컨테이너 내부의 디렉토리로 복사한다.     해당 코드의 경우 프로젝트 최상위에 존재하는 모든 파일을 컨테이너 내부 /app 디렉토리로 복사한다.     파일을 컨테이너에서 사용하고 싶으면 반드시 copy를 써서 복사해야 함.  1WORKDIR /app WORKDIR {컨테이너 내 디렉토리} dockerfile의 RUN , CMD, ENTRYPOINT 등의 명령어를 실행할 컨테이너 경로를 지정한다. 아래 라인에 등장하는 RUN, CMD는 컨테이너 내부의 /app에서 실행한다. 12ENV PYTHONPATH=/appENV PYTHONBUFFERED=1 ENV {환경변수 이름=값} 컨테이너 내의 환경변수를 지정한다. 파이썬 애플리케이션의 경우 보통 위의 두 값을 지정한다. 1234RUN pip install pip==23. 0. 1 &amp;&amp; \	pip install poetry==1. 2. 1 &amp;&amp; \	poetry export -o requirements. txt &amp;&amp; \	pip install -r requirements. txt RUN은 컨테이너 내에서 리눅스 명령어를 실행한다. 한번에 실행할 명령어가 여러 대인 경우 &amp;&amp; \로 이어준다. 이전 라인에서 COPY 와 WORKDIR이 실행 되었기 때문에 requirements. txt가 존재하고, 이를 pip install -r 명령어로 실행할 수 있다. 1CMD [ python ,  main. py ] CMD [ 실행할 명령어 ,  인자 , . . ] docker run으로 이미지를 기반으로 컨테이너를 만들 때, 실행할 명령어의 이미지는 실행되는 즉시 python main. py를 실행한다. CMD는 띄어쓰기를 사용하지 않는다. Docker Image Build: 1docker build -t {빌드할 이미지 이름:태그 이름} {Dockerfile이 위치한 경로}1docker build -t 02-docker:latest .  이미지 생성 아래 이미지에서 . 는 현태 폴더에 Dockerfile이 있음을 의미 -t {빌드할 이미지 이름:태그 이름} 옵션으로 이미지 이름과 태그 지정 태그 미지정시 “latest”로 채워짐  빌드를 마치면 docker images명령어로 방금 빌드한 이미지를 확인 할 수 있다. 1docker images | grep 02-docker Docker Image 실행: 1docker run {image name:tag} 빌드한 이미지를 실행 할 수 있음 태그가 latest인 경우 생략 가능1docker run 02-docker:latest Dockerfile 기타:    EXPOSE : 컨테이너 외부에 노출할 포트 지정     ENTRYPOINT : 이미지를 컨테이너로 뛰울 때 항상 실행하는 커맨드  Docker Image Push:    우리가 만든 이미지를 업로드 할 수 있다. 이를 위해 대표적인 registry인 Dockerhub에 도커 이미지를 push 할 수 있다. (github과 비슷함)     docker login 명령어로 내 dockerhub 계정을 cli에 연동 할 수 있음.     docker tag {기존 이미지:태그} {새 이미지 이름:태그} dockerhub에 올릴 이미지 이름은 내 계정ID/이미지 이름 형태여야 함.     docker push {이미지이름:태그} dockerhub에 이미지를 psuh 한다. Dockerhub에서 push된 이미지 확인 가능.     내가 push한 이미지는 pull로 언제든지 다시 받을 수 있음.   참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://github. com/zzsza https://aws. amazon. com/ko/what-is/virtualization/ https://cloud. google. com/learn/what-are-containers?hl=ko"
    }, {
    "id": 24,
    "url": "http://localhost:4000/Basic-Linux-2/",
    "title": "Linux Basic - 2",
    "body": "2023/04/27 - 표준 스트림(standard stream): 표준 스트림은 프로그래밍 언어의 인터페이스를 비롯한 유닉스 계열 운영체제에서 컴퓨터 프로그램과 단말기 사이에 미리 연결된 입출력 통로를 가르킨다. 유닉스에서 통작하는 프로그램은 실행 시 3개의 스트림이 자동으로 열린다. 입력을 위한 스트림(Standard Input, STDIN, 0), 출력을 위한 스트림(Standard Output, STDOUT, 1), 마지막으로 오류 메시지의 출력을 위한 스트림(Standard Error , STDERR, 2)가 존재한다. 보통 입출력은 물리적으로 연결된 콘솔을 통해 일어나는데, 표준 스트림을 이것을 추상화한 것이라고 보면 된다.  출처 : https://en. wikipedia. org/wiki/Standard_streams ​  Standard Input, STDIN, 0 : 입력(비밀번호, 커맨드 등) Standard Output, STDOUT, 1 : 출력(터미널에 나오는 값) Standard Error , STDERR, 2 : 디버깅 정보나 에러 출력 Redirection and Pipe: Redirection:  프로그램의 출력을 다른 파일이나 스트림으로 전달하는 것.      &lt;    스트림의 흐름을 바꿔주는 것이라고 이해하면 편하다. Pipe:    프로그램의 출력을 다른 프로그램의 입력으로 사용하고 싶을 때 사용한다. 예를 들어 A라는 명령어의 output을 B의 input으로 사용하고 싶을 경우 처럼, 다양한 커맨드를 조합하는 방식으로 사용 할 수 있다.     ls | grep  vi  : 현재 폴더에 있는 파일 명 중 “vi” 가 들어간 단어를 찾기  서버에서 자주 사용하는 쉘 커맨드: ps:    Process Status의 약자. 현재 실행되고 있는 프로세스를 출력한다.     -e : 모든 프로세스     -f : full format으로 자세히 보여줌   curl:    Client URL의 약자. CL 기반의 data transfer 커맨드이다. Request를 테스트 할 수 있는 명령어이다. 웹 서버를 작성한 후 요청이 제대로 실행되는지 확인할 수 있다.     curl -X localhost:5000/ {data}     curl 외에도 httpie 또는 Postman 등이 있다.   df:    Disk Free의 약자. 현재 사용 중인 디스크의 용량을 확일 할 수 있다.     -h : 읽기 쉬운 형태로 출력  scp:    Secure Copy의 약자. SSH를 이용해 네트워크로 연결된 호스트 간 파일을 주고 받는 명령어이다.     -r : 재귀적으로 복사     -P : SSH 포트 지정     -i : SSH 설정을 활용해 실행  remote to local:  scp user@ip:remote_directory local_pathremote to remote:  scp user@ip:remote_directory user2@ip2:target_remote_directory nohup:    터미널 종료 후에도 계속 작업이 유지하도록 실행한다(백그라운드 실행)     nohup python3 app. py &amp;     nohup으로 실행될 파일은 permission이 755여야 함.      chmod 755 {실행파일}   nohup으로 실행된 파일 종료:    ps ef | grep app. py : app. py의 pid(Process ID) 찾고 kill -9 {pid}로 프로세스를 kill 하면 된다.     Log는 nohup. out에 저장 된다.   chmod:    Change Mod의 약자. 파일의 권한을 변경하는 경우 사용한다. 유닉스에서 파일이나 디엑토리의 시스템 모드를 변경한다.     ls -al로 확인  Permission: r : Read, 4 w : write, 2 x : execute, 1 - : Denied 참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://www. youtube. com/watch?v=EL6AQl-e3AQ 위키피디아-표준 스트림"
    }, {
    "id": 25,
    "url": "http://localhost:4000/Shell-Script-1/",
    "title": "Shell Script Basic - 1",
    "body": "2023/04/26 - Shell Script:    쉘 스크립트는 쉘에서 사용 할 수 있는 명어들의 조합을 모아서 만든 파일이라고 보면 편하다. 기본적으로 쉘을 이용해서 명령어들을 순차적으로 읽으면서 실행시켜준다.     . sh 파일을 생성해서 그 안에 쉘 커맨드를 추가 할 수 있다. If, while, case 문이 존재하며 작성후 bash {name. sh} 로 실행이 가능하다.  쉘 스크립트의 사용:    #!/bin/bash : 이 스크립트를 Bash 쉘로 해석 하겠다는 선언문 같은 것     $(date +%s) : date를 %s (unix timestamp)로 변형     START=$(date +%s) : START라는 변수에 저장     쉘 스크립트를 통해 편리하게 자동화를 구축할 수 있다. 많이 연습해두자.     쉘 스크립트를 통해 구현 할 수 있는 기능 예시 : https://www. geeksforgeeks. org/introduction-linux-shell-shell-scripting/     위 링크에서 쉘 스크립팅에 대한 더 자세한 내용을 볼 수 있다.   참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://www. youtube. com/watch?v=cXnVygkAg4I https://www. geeksforgeeks. org/introduction-linux-shell-shell-scripting/"
    }, {
    "id": 26,
    "url": "http://localhost:4000/Basic-Linux-1/",
    "title": "Linux Basic - 1",
    "body": "2023/04/26 - 리눅스 개요: What is Linux?: 리눅스는 오픈소스 운영체제(OS, Operating System)이다. 리눅스 세계 최대의 오픈소스 프로젝트이며, 누구든지 자유롭게 운영체제 프로그램의 소스를 변경하여 재배포 시킬 수 있는 프리웨어다.  리눅스를 배워야하는 이유: CLI를 통한 개발은 필수적이다. Shell script의 작성부터 서버 개발까지, 리눅스는 어떻게 보면 개발자를 위한 필수 교양이기 때문에 학습하는 것을 강력하게 권장한다.  다양한 리눅스 배포판: 출처 : https://en. wikipedia. org/wiki/List_of_Linux_distributions  Debian Ubuntu Redhat Centos이 외에도 정말 다양한 리눅스 배포판이 존재한다. 목적에 맞게 골라서 사용하면 된다.  Introduction to Shell: 쉘의 종류: 쉘은 커널(kernel)과 사용자간의 다리역할을 해서, 사용자로부터 명령을 받아서 해석하고 프로그램을 실행해준다. (여기서 커널은 운영체제의 메모리에 항상 올라가 있는 부분으로, 하드웨어와 소프트웨어 사이의 인터페이스를 제공해주는 역할을 해준다고 보면 된다) 조금 더 간단하게 말하면, 사용자가 문자를 입력해 컴퓨터에 명령할 수 있도록 하는 프로그램이다.  sh : 최초의 쉘 bash : 리눅스 표준 쉘 zsh : Mac OS 기본 쉘이것 외에도 다양한 쉘이 존재한다.  쉘을 사용하는 상황:  서버에 접속해서 사용하는 경우 crontab 등 리눅스의 내장 기능을 활용하는 경우 Docker를 사용하는 경우 서버를 관리할 경우 Test code의 실행이나 배포 파이프라인(github action)의 실행이것 외에도 정말 많은 이유로 쉘을 사용하는 상황이 온다. 1kimseungki@DESKTOP-1P30XXX:~ username : 사용자 이름 (kimseungki) hostname : 컴퓨터 네트워크에 접속된 장치에 할당된 이름. IP 대신 기억하기 쉬운 이름으로 저장하는 경우가 많음 (DESKTOP-1P30XXX) Shell Command: 기본적인 shell 명령어: man:  쉘 커맨드의 매뉴얼 문서를 보고 싶은 경우 사용. man python  종료는 q 입력. mkdir:  Make Directory의 약자. 폴더를 생성한다. mkdir linux-test (linux-test라는 폴더를 현재의 경로에 생성) ls:  List Segments의 약자. 현재 접근한 폴더의 구성요소 확인. 옵션 -a : . 으로 시작하는 파일, 폴더를 포함해 전체 파일 출력 -l : 퍼미션, 소유자, 만든 날짜, 용량까지 출력 -h : 용량을 사람이 읽기 쉽도록 표현 ls -a 또는 ls -alh 처럼 사용 pwd:  Print Working Directory의 약자. 현재 폴더의 경로를 절대 경로로 보여줌. pwd cd:  Change Directory의 약자. 명시한 폴더의 경로로 이동한다. cd linux-test echo:  파이썬의 print 처럼 터미널에 텍스트를 출력해준다. echo  hi  : 터미널에 hi 출력 echo `쉘커맨드` 입력시 쉘 커맨드의 결과를 출력  ex. echo `pwd` cp:  Copy의 약자. 파일 또는 폴더를 복사한다. cp vi-test. sh vi-test2. sh 옵션 -r : 디렉토리를 복사할 때 디렉토리 안에 파일이 있으면 재귀적으로 모두 복사 -f : 복사할 때 강제로 실행 vi:  vim 편집기로 파일을 생성한다. INSERT 모드에서만 수정 할 수 있음. vi vi-test. sh 를 사용하면 vim 편집기로 vi-test. sh라는 파일을 생성함. i를 눌러서 INSERT 모드로 변경해서 수정을 할 수 있음.  나가기 위해서는 ESC + wq 또는 ESC + ! 후에 wq 입력으로 저장하고 나갈 수 있다. 그냥 q의 경우 저장하지 않고 나간다. vim editor mode vi 편집기 mode   Command mode      vi 실행시 기본 모드   방향키를 통해 커서 이동 가능   dd : 현재 위치한 한 줄 삭제   i : INSERT 모드로 변경   x : 커서가 위치한 곳의 글자 1개 삭제   p : 현재 커서가 있는 줄 바로 아래에 붙여넣기   k : 커서 위로 / j : 커서 아래로 / l : 커서 오른쪽으로 / h : 커서 왼쪽으로      Last Line mode      ESC 누른 후 콜론(:)을 누르면 나오는 모드   w : 현재 파일명으로 저장   q : vi 종료(저장되지 않음)   q! : vi 강제 종료   wq : 저장 후 종료   set nu : 라인 번호 출력    bash:  bash로 쉘 스크립트 실행bash vi-test. sh 로 vi-test. sh 라는 쉘 스크립트 파일을 실행 vi-test. sh sudo:  관리자 권한으로 실행하고 싶은 경우 앞에 sudo를 붙임. 한 마디로 최고 권한을 가진 슈퍼 유저로 프로그램을 실행하겠다는 뜻이다. sudo의 사용에는 신중을 가하고 사용하는 것을 권장한다. mv:  Move의 약자. 파일 또는 폴더를 이동하기 위해 사용한다. 이름을 바꾸기 위해 사용 할 수 도 있다. mv vi-test. sh vi-test3. sh 를 사용하면 vi-test. sh가 vi-test3. sh로 이름이 변경된다.  mv {원본파일} {이동위치} 로 파일을 이동 시킬 수 있다.  cat:  Concatenate의 약자. 특정 파일 내용을 출력하기 위해 사용한다. 여러 파일을 인자로 주면 합쳐서(concat) 출력 해준다. Concat을 한 상태에서 파일에 저장(overwrite)또는 추가(append) 할 수 있다. cat vi-test3. sh  concat 해서 출력cat vi-test2. sh vi-test3. sh  파일에 overwritecat vi-test2. sh vi-test3. sh &gt; new_test. sh  파일에 appendcat vi-test2. sh vi-test3. sh &gt;&gt; new_test. sh history:  최근데 입력한 쉘 커맨드의 역사를 출력. History 결과에서 느낌표를 붙이고 숫자 입력시 그 커맨드를 다시 활용 할 수 있음. find:  파일 및 디렉토리를 검색할 때 사용 할 수 있다. find . -name  File  : 현재 폴더에서 File이란 이름을 가지는 파일 및 디렉토리 검색 find . -type file -name  *. txt  : 현재 경로에서 하위 디렉토리까지 . txt라는 확장자를 가진 모든 파일 검색 alias:  기본 명령어를 별칭으로 설정 할 수 있음. alias ll2='ls -l' : ll2 입력시 ls -l이 동작 됨 alias gp='git push' : gp 입력시 git push가 동작 됨 tree:  폴더의 하위 구조를 계층적으로 표현해줌. 프로젝트의 구조를 설명 할 때 유용하다. tree -L {level} 의 형태로 사용 tree -L 1 : 1 level 까지 보여주기 head, tail:  파일의 앞 또는 뒤 n행을 출력함. head -n 1 vi-test3. sh : vi-test3. sh의 앞 1 행 출력 cat 로 파일 전체를 보기에 너무 긴 경우 활용 할 수 있음.  sort:  행 단위로 정렬 해줌. -r : 정렬을 내림차순으로 정렬(기본:오름차순) -n : numeric sort cat fruits. txt | sort : fruits. txt를 cat로 출력할때 오름차순 정렬을 해서 출력한다.  uniq:  중복된 행이 연속으로 있는 경우 중복 제거한다. sort와 함께 사용하면 효과적이다. -c : 중복된 행의 개수 출력 cat fruits. txt | sort | uniq grep:  파일에 주어진 패턴 목록과 매칭되는 라인을 검색 해줌. grep은 뒤에서 나올 pipe와 같이 사용하곤 한다. grep {option} {filename} 옵션 -i : 대소문자 구분 없이 찾기 -w : 정확히 그 단어만 찾기 -v : 특정 패턴 제외한 결과 출력 -E : 정규 표현식 사용  cut: 파일에서 특정 필드를 추출한다. -f : 잘라낼 필드 지정 -d : 필드를 구분하는 구분자 참고  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://www. youtube. com/watch?v=EL6AQl-e3AQ FabioLolix/LinuxTimeline: Linux Distributions Timeline"
    }, {
    "id": 27,
    "url": "http://localhost:4000/python-version-control/",
    "title": "Version Control",
    "body": "2023/04/25 - Python Versioning: Python is Semantic Versioning:    파이썬 3. 11. x 버전은 아래 마이너 버전(3. 10. x, 3. 9. x . . )에 호환      3. 8. x에서 생성한 코드가 있다면 그대로 3. 11. x에 실행해도 문제가 없음   하지만 패키지가 3. 11. x를 지원하지 않는 문제가 있을 수도 있음      파이썬 3. x 버전은 아래 메이저 버전 2. x에 호환 안됨      2. x의 코드는 3. x의 버전에서 실행 되지 않음    프로젝트의 파이썬 버전 표시: 파이썬 버전은 보통 프로젝트의 github readme에 작성    python 3. 8 이상에서 실행 가능      python&gt;=3. 8, python 3. 8+, python 3. 8^      python 3. 8 만 실행 가능      python == 3. 8      python 3. 10 이상, 3. 11 미만에서 실행 가능      python=”&gt;=3. 10, &lt;3. 11”    파이썬 설치 방법: 파이썬의 설치 방법에는 여러 종류가 있고, 각각의 장단점이 있다.  파이썬 공식 홈페이지에서 파일을 다운받아 설치     홈페이지의 바이너리 파일을 다운받아서 설치한다. 이렇게 설치는 케이스가 제일 적다.       Conda를 이용하여 설치          만약 python 3. 11을 설치한다면 conda install python=3. 11. 0 으로 설치한다. Python의 버전 관리를 conda에 맡김.           장점 : conda 사용 중이라면, 별다른 도구 없이 바로 설치 가능           단점 : conda가 무겁기 때문에 production 환경에선 잘 사용되지 않음           사용하는 경우 : conda 중심의 셋팅이 이미 되어 있는 경우          Docker로 파이썬 3. 11. 0 이미지 설치          docker pull python:3. 11. 0으로 파이썬 버전 관리를 컨테이너 이미지로 진행한다. 파이썬을 사용하고 싶으면 docker run -it python python으로 Docker 컨테이너 실행과 접속을 한다.           장점 : 로컬환경에 바이너리를 설치하지 않기 때문에 파이썬 설치 및 삭제가 쉬움           단점 : 파이썬을 이용하기 위해서는 컨데이너에 매번 접속해야 함           사용하는 경우 : 로컬 환경과 파이썬 환경을 완전히 격리하고 싶은 경우          패키지 매니저로 설치          패키지 관리자(brew, apt, winget)로 파이썬 설치          Mac OS의 경우             brew install python@3. 11                Linux의 경우(Ubuntu)             apt install python3. 11                Window의 경우             winget install Python3. 11                         장점 : 설치가 간단           단점 : 패치 버전까지 포함하는 파이썬 특정 버전을 설치할 수 없음           사용하는 경우 : CLI로 빠르고 간단하게 설치하고 싶은 경우          pyenv로 설치하기          pyenv는 파이썬의 여러 버전을 cli로 쉽게 설치할 수 있는 도구. pyenv install 3. 11. 0을 사용해서 설치.           장점 : 파이썬의 여러 버전을 설치하고 다룰 수 있음           단점 : pyenv를 먼저 설치해야 함           사용하는 경우 : 여러 버전의 파이썬을 바꿔줘야 하는 경우       파이썬 설치 시 여러 방법을 사용하면 충돌이 날 가능성이 존재한다. 파이썬 설치 전 지금 사용하는 python이 어디서 설치된 것인지 확인하는 과정이 필요함. which python{version} 으로 확인 가능.  Pyenv: MacOS, Linux: Mac 1brew install pyenvLinux 1sudo apt-get install -y make build-essential libsqlite3-dev wget curl llvm libncurses5-dev libssl-dev zlib1g-dev libbz2-dev libreadline-dev libncursesw5-dev xz-utils tk-dev1curl https://pyenv. run | bashMac이나 linux는 본인이 사용하는 shell 설정 파일(~/. bashrt, ~/. zshrc) 끝에 환경 변수들을 추가해야 함 123export PATH= ~/. pyenv/bin:$PATH eval  $(pyenv init -) eval  $(pyenv virtualenv-init -) shell을 확인하는 방법 1echo $SHELL환경 변수 추가 후 source “shell 설정 파일”로 shell 설정 업데이트 1source ~/. bashrc Windows: Powershell에서 다음 명령어 입력 1Invoke-WebRequest -UseBasicParsing -Uri  https://raw. githubusercontent. com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win. ps1  -OutFile  install-pyenv-win. ps1 ; . /install-pyenv-win. ps1Powershell 재실행, 필요한 경우 환경 변수 추가 pyenv shell 3. 11. 0 으로 현재 shell에 파이썬 버전 활성화 pyenv global 3. 11. 0으로 shell의 기본 파이썬 버전 설정 파이썬 프로젝트의 버전 관리: 가상환경: 하나의 로컬 환경에서 두개 이상의 프로젝트를 진행하면, 각각 사용하는 파이썬이나 패키지의 버전이 달라서 문제가 발생 할 수 있음. 이런 문제를 해결 하고자 가상 환경을 생성해 프로젝트 별로 각자의 환경을 갖게 함. 가상환경을 만드는 방법: venv, conda, pyenv-virtualenv, pipenv 등 다양한 방법이 있음. 그 중 venv가 파이썬 가상 환경 구축에 많이 사용 됨. venv: 1python -m venv  가상 환경 폴더를 만들 경로  보통 프로젝트 최상위 경로에서 . venv로 만드는 것이 관습 venv는 파이썬 내장 모듈 가상 환경 접속 : source {가상환경폴더}/bin/activate 접속하면 shell 왼쪽에 . venv 같은 가상환경 접속이 표시됨windows venv activation1path\to\venv\Scripts\activate. bat또는 1path\to\venv\Scripts\Activate. ps1 패키지 매니저: 패키지 매니저는 패키지를 설치하고 버전을 관리해준다. 파이썬의 패키지 매니저에는 pip, poetry, conda 등이 존재한다. (conda는 anaconda 자체의 패키지 매니저로 보는 것이 맞다) pip: pip는 항상 최신 버전의 pip를 사용하는 것이 좋음 패키지 설치 pip install {package name}[==version] ex. pip install pandas==2. 0. 0 패키지 목록 확인 pip list pip list --not-required --format=freeze (의존성 패키지 제외) 설치한 패키지 목록을 저장 freeze &gt; requirements. txt 저장한 패키지 목록을 다른 환경에서 설치 pip install -r requirements. txt pip의 단점   개발 환경과 배포 환경의 패키지가 분리되지 않음      black이라는 파이썬 코드 formatter 패키지가 있는데, black은 개발 환경에서만 사용될 뿐, 실제 배포 환경에서는 사용하지 않음   pip를 사용할 경우 requirements. txt에 black이 포함되고 실제 배포할 때 설치되어 용량을 더 사용하게 됨      pip list로 패키지간 의존성을 알 수 없음      pip install로 설치한 패키지는 black 하나 뿐인데 pip list에는 이 외의 패키지들도 등장한다. 이 패키지들은 black이 의존하는 패키지들인데, 이런 의존성에 대한 정보가 없음      pip uninstall 시 의존성이 있던 패키지들은 삭제되지 않음      black을 삭제해도 black과 함께 설치된 패키지들은 삭제되지 않음   요약하자면 pip로는 정교한 패키지 관리가 불가능하기 때문에 협업을 하는 경우 곤란한 상황이 올 수도 있다.  Poetry: pip의 문제를 해결하기 위해 poetry라는 대체재가 등장했다. Poetry 설치는 공식문서 참조 poetry 사용   프로젝트의 경로에서 poetry init 으로 파이썬 프로젝트 초기화. 기본적인 설정을 하고나면 프로젝트 경로에 pyproject. toml 이라는 파일이 생성된다. pyproject. toml는 파이썬 프로젝트에 대한 메타 정보를 담고 있는 파일임.     패키지의 설치는 poetry add 명령어로 설치한다. -D 옵션을 붙일 경우 개발 환경에서만 사용할 패키지를 설치 할 수 있음.  ex. add black -D  pyproject. toml 에서 dev 환경과 build 환경을 나눠서 관리 할 수 있기 때문에 정교한 패키지 관리가 가능함. 참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://peps. python. org/pep-0440/"
    }, {
    "id": 28,
    "url": "http://localhost:4000/Versioning/",
    "title": "Versioning",
    "body": "2023/04/25 - Version and Versioning: 버전(Version):  소프트웨어 제품의 특정 릴리스에 대한 고유한 식별자 소프트웨어의 출시나 업데이트가 이루어질 때마다 새로운 버전을 부여함 버저닝(Versioning):  소프트웨어의 버전 작성은 특정 상태에 대한 유일한 버전 번호를 결정하는 일이라고 보면 된다 다양한 버전을 관리하고 식별하기 위해 사용되는 방법 버저닝 방법(Versioning strategy):    CalVer(Calendar Versioning)      날짜 기반 시스템을 활용한 버저닝   버전 번호는 연도와 월로 구성   날짜 기반으로 출시 시기 예측이 수월   ex. Ubuntu 20. 04       출처 - https://blog. datalust. co/switching-to-calendar-versioning/      SemVer(Semantic Versioning)      마침표로 구분된 주 번호, 부 번호, 패치 번호로 구성   이전 버전과 호환되지 않은 변경이 있는 경우 주 번호 증가   이전 버전과 호환되며 새로운 기능이 추가되면 부 번호 증가   이전 버전의 버그 수정이 진행되면 패치 번호가 증가   ex. Python 3. 11. 0    출처 - https://forums. ubports. com/topic/1822/semantic-versioning-for-ut   HashVer(Hash Versioning)     SHA-1, SHA-256 해시 알고리즘을 사용해 버전에 대한 고유 식별자를 생성   코드가 변결될 때마다 해시가 변경되므로 모든 버전이 고유한 식별자를 가지도록 보장   ex. Git command 7e6d3fd   버저닝이라는 것은 결국 코드의 특정 상태를 표현하는 것이 핵심이다. 협업을 할 때에도 특정 상태에 대한 통일된 명칭을 만들어야 커뮤니케이션이 원활하다는 것을 알 것이다.  참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://semver. org/ https://calver. org/"
    }, {
    "id": 29,
    "url": "http://localhost:4000/Software-engineering/",
    "title": "Software Engineering",
    "body": "2023/04/24 - 소프트웨어 엔지니어링이란: 소프트웨어의 개발, 운용, 유지보수 등의 life cycle 전반을 체계적이고 정량적으로 다루는 개념이다. 한 마디로 소프트웨어의 품질과 유지 보수성을 보장하는 학문이라고 봐도 무방할 것 같다. Software Development Life Cycle(SDLC): 소프트웨어의 개발 수명 주기는 고품질 소프트웨어를 설계하고 구축하기 위해 필수적인 비용 및 시간 효율적인 프로세스이다. 계획(Planning) &amp; 분석(Analysis):  비용 분석, 스케쥴링, 리소스 추정 및 할당과 같은 작업 client, 전문가, 관리자 등 여러 이해 관계자들의 요구 사항을 수집하여 공통 목표를 정의     보통 협의된 요구사항들에 대해서 문서화 해서 수령하고, 이 요구사항에 대한 문서를 SRS(Software Requirement Specification)라고 부릅니다    공통 목표를 달성하기 위한 세부 계획을 수립설계(Design architecture):  요구 사항을 만족하는 최적의 솔루션 찾기 모듈을 통합하거나, 기술을 선택하고, 개발 도구를 찾고 기능이 동작할 수 있는 아키텍쳐를 설계하는 단계 데이터의 flow나 동작에 대한 고민구현(Implementation):  실제적인 제품의 개발이 시작되는 단계 개발 가이드라인을 준수하면서 코드의 개발부터 시작해서 통합해서 빌드하는 단계까지 모두 포함검증(Testing):  기능이 동작하는지, 오류가 있는지 테스트하는 단계 client의 요구 사항을 충족하는지 확인하는 작업이 포함 구현 단계와 동시에 진행되는 경우가 많음배포(Deployment):  사용자가 사용하는 소프트웨어를 프로덕션 이라고 하고 개발팀에서 지속적으로 개발하고 테스트하는 소프트웨어의 복사본을 테스트 환경 또는 빌드 환경 패키징, 환경 구성 및 설치 하는 작업도 배포에 포함유지관리(Maintenance):  소프트웨어의 변경 사항 관리, 버그 픽스, 성능 및 사용자 환경 모니터링이 들어가는 단계 출처 - https://bigwater. consulting/2019/04/08/software-development-life-cycle-sdlc/ SDLC는 위 과정을 계속 반복하는 프로세스. SDLC 모델들은 여러가지가 존재하고, 다른 수명 주기 방법론과 intersect 하는 부분도 많기 때문에 관심이 있다면 terminology에 대한 명확한 설명을 더 찾아보는 것도 좋을 것 같다. 소프트웨어의 설계: Modularity, Cohesion, Coupling:    모듈성(modularity)      소프트웨어에서 임의의 두 부분이 직접적인 상호관계가 많아지면 모듈성이 떨어짐   시스템의 구성 요소가 분리되고 재결합 될 수 있는 정도      응집도(cohesion)      모듈 내부의 기능적인 응집 정도   하나의 모듈은 하나의 기능을 수행하는 것이 이상적   하나의 클래스에 모든 기능을 구현하는 것이 아닌 목적에 맞게 나누고 교류하는 인터페이스가 중요      결합도(coupling)      모듈과 모듈같의 상호 결합 정도, 모듈 간의 상호의존성을 나타내는 정도   응집도와 결합도에서 보통 응집도는 높을수록 좋고 결합도는 낮을수록 이상적이다. 응집도와 결합도의 다양한 유형에 대해 추가적으로 알아봐도 좋을 것 같다.  출처 - https://www. geeksforgeeks. org/software-engineering-coupling-and-cohesion Testing: 소프트웨어 개발에서의 테스트는 넓게 보면 프로그램이 예상대로 작동하고 문제가 없는지 확인하는 과정이라고 생각하면 좋을 것 같다. 조금 더 자세히 말하자면 사용자가 안정적으로 소프트웨어를 사용할 수 있도록, 기능이 추가될 때 기존 시스템에서의 오류 확인, 아키텍쳐 확인, 서버에 대한 확인, 데이터베이스의 연결에 대한 확인 등 여러가지 단계가 포함된다.  출처 : https://www. geeksforgeeks. org/levels-of-software-testing 딥러닝에서의 testing life cycle에 대해서 더 알아봐야겠다. 문서화(Documentation): 소프트웨어를 위한 Readme, API 문서, 아키텍쳐 문서 등이 여기에 포함. 파이토치의 documentation을 예시로 들자면  Pytorch에 대한 소개와 설명 OS별 설치 방법 시작 방법 추가 학습 자료 오픈소스에 기여하는 방법소프트웨어 엔지니어링 역량의 필요성: 머신러닝 모델을 설계하고 만드는 것은 전체 과정의 극히 일부. 결국 product를 serving하기 위해서는 소프트웨어 엔지니어링은 필수적이다. 전체 시스템을 알기 위해서는 소프트웨어 엔지니어링 관점으로 생각을 확장해야 함.  출처 : Hidden Technical Debt in Machine Learning Systems  참고:  Naver Connection Boostcamp AI Tech 5th - Product Serving(변성윤) https://zzsza. github. io/ https://aws. amazon. com/ko/what-is/sdlc/ https://www. geeksforgeeks. org/software-engineering-coupling-and-cohesion/"
    }, {
    "id": 30,
    "url": "http://localhost:4000/pandas_7/",
    "title": "Pandas - 7(Groupby)",
    "body": "2023/02/17 - Groupby 1: Groupby: groupby의 특징:  SQL groupby 명령어와 같음 split -&gt; apply -&gt; combine의 과정을 거쳐서 연산 출처-https://jakevdp. github. io/blog/2017/03/22/group-by-from-scratch/ groupby의 사용: 123456789# data from: ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',     'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],     'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],     'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],     'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}df = pd. DataFrame(ipl_data)df          Team   Rank   Year   Points         0   Riders   1   2014   876       1   Riders   2   2015   789       2   Devils   2   2014   863       3   Devils   3   2015   673       4   Kings   3   2014   741       5   kings   4   2015   812       6   Kings   1   2016   756       7   Kings   1   2017   788       8   Riders   2   2016   694       9   Royals   4   2014   701       10   Royals   1   2015   804       11   Riders   2   2017   690    12345df. groupby( Team )[ Points ]. sum() #  Team 은 묶음의 기준이 되는 column#  Points 는 적용 받는 column# sum()은 적용하는 연산# 결과는 Team을 기준으로 Points를 sum 한 것1234567TeamDevils  1536Kings   2285Riders  3049Royals  1505kings   812Name: Points, dtype: int64 한 개 이상의 column 묶을 수 있음12h_index = df. groupby([ Team ,  Year ])[ Points ]. sum()h_index1234567891011121314Team  YearDevils 2014  863    2015  673Kings  2014  741    2016  756    2017  788Riders 2014  876    2015  789    2016  694    2017  690Royals 2014  701    2015  804kings  2015  812Name: Points, dtype: int64Hierarchical index:  groupby 명령의 결과물도 결국은 dataframe 두 개의 column으로 groupby를 할 경우 index가 두 개 생성1h_index. index12345678910111213MultiIndex([('Devils', 2014),      ('Devils', 2015),      ( 'Kings', 2014),      ( 'Kings', 2016),      ( 'Kings', 2017),      ('Riders', 2014),      ('Riders', 2015),      ('Riders', 2016),      ('Riders', 2017),      ('Royals', 2014),      ('Royals', 2015),      ( 'kings', 2015)],      names=['Team', 'Year'])1h_index[ Devils : Kings ]1234567Team  YearDevils 2014  863    2015  673Kings  2014  741    2016  756    2017  788Name: Points, dtype: int64unstack() group으로 묶여진 데이터를 matrix 형태로 전환해줌1h_index1234567891011121314Team  YearDevils 2014  863    2015  673Kings  2014  741    2016  756    2017  788Riders 2014  876    2015  789    2016  694    2017  690Royals 2014  701    2015  804kings  2015  812Name: Points, dtype: int641h_index. unstack()      Year   2014   2015   2016   2017         Team                       Devils   863. 0   673. 0   NaN   NaN       Kings   741. 0   NaN   756. 0   788. 0       Riders   876. 0   789. 0   694. 0   690. 0       Royals   701. 0   804. 0   NaN   NaN       kings   NaN   812. 0   NaN   NaN    swaplevel() index level을 변경 할 수 있음1h_index. swaplevel()1234567891011121314Year Team 2014 Devils  8632015 Devils  6732014 Kings   7412016 Kings   7562017 Kings   7882014 Riders  8762015 Riders  7892016 Riders  6942017 Riders  6902014 Royals  7012015 Royals  804   kings   812Name: Points, dtype: int641h_index. swaplevel(). sort_index(level=0)1234567891011121314Year Team 2014 Devils  863   Kings   741   Riders  876   Royals  7012015 Devils  673   Riders  789   Royals  804   kings   8122016 Kings   756   Riders  6942017 Kings   788   Riders  690Name: Points, dtype: int64operations index level을 기준으로 기본 연산 수행 가능1h_index. sum(level=0)123456789C:\Users\KIMSEUNGKI\AppData\Local\Temp\ipykernel_3760\1316823663. py:1: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df. sum(level=1) should use df. groupby(level=1). sum().  h_index. sum(level=0)TeamDevils  1536Kings   2285Riders  3049Royals  1505kings   812Name: Points, dtype: int641h_index. sum(level=1)123456Year2014  31812015  30782016  14502017  1478Name: Points, dtype: int64참고 부스트코스 AI 기초다지기 pandas II - 최성철 https://jakevdp. github. io/blog/2017/03/22/group-by-from-scratch/"
    }, {
    "id": 31,
    "url": "http://localhost:4000/pandas_6/",
    "title": "Pandas - 6(Lambda, Map, Apply)",
    "body": "2023/02/16 - Lambda, Map, Apply: Lambda:  한 줄로 함수를 표현하는 익명 함수 기법 Lisp 언어에서 시작된 기법12f = lambda x,y: x + yf(1,4)15 일반적인 함수로 나타내면123def f(x , y):  return x + yf(1,4)1512f = lambda x: x ** 2f(3)191(lambda x: x +1)(5)16Map: list(map()):  함수와 sequence형 데이터를 인자로 받음 각 element 마다 입력받은 함수를 적용해서 list로 반환 pandas에서 일반적으로 함수를 lambda형태로 표현123ex = [1,2,3,4,5]f = lambda x: x ** 2 list(map(f, ex)) # 인자를 리스트 ex로 받아서 함수 f에 적용1[1, 4, 9, 16, 25] 두 개 이상의 argument가 있으면 두 개의 seqeunce형을 써야함12f = lambda x, y: x + ylist(map(f, ex, ex)) # 함수 f의 x,y에 인자 ex가 들어감1[2, 4, 6, 8, 10] 익명 함수 그대로 사용 가능1list(map(lambda x: x+5, ex))1[6, 7, 8, 9, 10]map for series:  series 타입 데이터에도 map 사용 가능 function 대신 dict, sequence형 자료등으로 대체 가능12s1 = Series(np. arange(10))s1. head(5)1234560  01  12  23  34  4dtype: int321s1. map(lambda x: x**2). head(5)1234560   01   12   43   94  16dtype: int64dict 사용:  dict type을 이용해서 데이터 교체 할 수 있음 보통 dict를 많이 이용1s112345678910110  01  12  23  34  45  56  67  78  89  9dtype: int3212z = {1: 'A', 2: 'B', 3: 'C'} # dict 자료형을 이용해서 데이터를 변경가능s1. map(z) # 비어있는 데이터는 NaN으로12345678910110  NaN1   A2   B3   C4  NaN5  NaN6  NaN7  NaN8  NaN9  NaNdtype: objectSeries를 이용해서 Series 데이터 변환:  같은 series 끼리 이용해서 데이터 변환 가능1s112345678910110  01  12  23  34  45  56  67  78  89  9dtype: int321s2 = Series(np. arange(10,15))1234560  101  112  123  134  14dtype: int321s1. map(s2)12345678910110  10. 01  11. 02  12. 03  13. 04  14. 05   NaN6   NaN7   NaN8   NaN9   NaNdtype: float64map을 사용하는 예시:  특정 데이터를 숫자로 나타내기 위해서 number code를 부여하는 경우12df = pd. read_csv( https://raw. githubusercontent. com/rstudio/Intro/master/data/wages. csv )df. head()           earn   height   sex   race   ed   age         0   79571. 299011   73. 89   male   white   16   49       1   96396. 988643   66. 23   female   white   16   62       2   48710. 666947   63. 77   female   white   16   33       3   80478. 096153   63. 22   female   other   16   95       4   82089. 345498   63. 08   female   white   17   43    1df. sex. unique() # series 데이터의 유일한 값을 리스트로 반환1array(['male', 'female'], dtype=object)12df[ sex_code ] = df. sex. map({ male :0,  female :1}) # sex_code라는 column 새로 추가해서 성별코드 부여df. head(5)           earn   height   sex   race   ed   age   sex_code         0   79571. 299011   73. 89   male   white   16   49   0       1   96396. 988643   66. 23   female   white   16   62   1       2   48710. 666947   63. 77   female   white   16   33   1       3   80478. 096153   63. 22   female   other   16   95   1       4   82089. 345498   63. 08   female   white   17   43   1    replace():  데이터 변환할때 map 대신 사용 가능 map 함수의 기능중 데이터 변환 기능만 담당 데이터 변환시 자주 사용123df. sex. replace(  { male :0,  female :1}). head()1234560  01  12  13  14  1Name: sex, dtype: int641df. sex. head(5)1234560   male1  female2  female3  female4  femaleName: sex, dtype: object dataframe의 원본을 변환하기 위해 inplace=True로 설정12345df. sex. replace(  [ male ,  female ],   [0,1], inplace=True)df. head(8)          earn   height   sex   race   ed   age   sex_code         0   79571. 299011   73. 89   0   white   16   49   0       1   96396. 988643   66. 23   1   white   16   62   1       2   48710. 666947   63. 77   1   white   16   33   1       3   80478. 096153   63. 22   1   other   16   95   1       4   82089. 345498   63. 08   1   white   17   43   1       5   15313. 352901   64. 53   1   white   15   30   1       6   47104. 171821   61. 54   1   white   12   53   1       7   50960. 054282   73. 29   0   white   17   50   0    Apply: apply for dataframe:  map의 경우 series 데이터에 있는 element 단위로 적용할 때 사용 apply는 map과 달리, series 전체(column)에 해당 함수를 적용 입력값이 series 데이터로 입력받아서 handling 가능apply():  각 column 별로 결과값을 반환하는 것을 볼 수 있음12df = pd. read_csv( https://raw. githubusercontent. com/rstudio/Intro/master/data/wages. csv )df. head()          earn   height   sex   race   ed   age         0   79571. 299011   73. 89   male   white   16   49       1   96396. 988643   66. 23   female   white   16   62       2   48710. 666947   63. 77   female   white   16   33       3   80478. 096153   63. 22   female   other   16   95       4   82089. 345498   63. 08   female   white   17   43    12df_info = df[[ earn ,  height , age ]] # 적용 할 columns들 df_info. head()          earn   height   age         0   79571. 299011   73. 89   49       1   96396. 988643   66. 23   62       2   48710. 666947   63. 77   33       3   80478. 096153   63. 22   95       4   82089. 345498   63. 08   43    12f = lambda x : x. max() - x. min() # column 내의 최대값-최솟값 리턴df_info. apply(f)1234earn   318047. 708444height    19. 870000age      73. 000000dtype: float64 내장 연산 함수를 사용할 때도 똑같은 효과 mean, std. . 등 사용 가능1df_info. apply(sum)1234earn   4. 474344e+07height  9. 183125e+04age    6. 250800e+04dtype: float641df_info. sum()1234earn   4. 474344e+07height  9. 183125e+04age    6. 250800e+04dtype: float64Series값 반환:  scalar 값 이외에도 series값의 반환도 가능함1234def f(x):  return Series([x. min(), x. max(), x. mean()],           index=[ min ,  max ,  mean ])df_info. apply(f)           earn   height   age         min   -98. 580489   57. 34000   22. 000000       max   317949. 127955   77. 21000   95. 000000       mean   32446. 292622   66. 59264   45. 328499    applymap():  series 단위가 아닌 element 단위로 함수 적용 series 단위에 apply를 적용시킬 때와 같은 효과12f = lambda x : -xdf_info. applymap(f). head(5) # df_info라는 dataframe의 모든 element에 대해서 f 적용           earn   height   age         0   -79571. 299011   -73. 89   -49       1   -96396. 988643   -66. 23   -62       2   -48710. 666947   -63. 77   -33       3   -80478. 096153   -63. 22   -95       4   -82089. 345498   -63. 08   -43    12f = lambda x : -xdf_info[ earn ]. apply(f). head(5) # series  earn 에 대해서 모든 element에 f 적용1234560  -79571. 2990111  -96396. 9886432  -48710. 6669473  -80478. 0961534  -82089. 345498Name: earn, dtype: float64Built-in Functions: Pandas의 여러가지 내장함수: 12df = pd. read_csv( https://raw. githubusercontent. com/rstudio/Intro/master/data/wages. csv )df. head()          earn   height   sex   race   ed   age         0   79571. 299011   73. 89   male   white   16   49       1   96396. 988643   66. 23   female   white   16   62       2   48710. 666947   63. 77   female   white   16   33       3   80478. 096153   63. 22   female   other   16   95       4   82089. 345498   63. 08   female   white   17   43    describe():  numeric type 데이터의 요약 정보를 보여줌1df. describe()           earn   height   ed   age         count   1379. 000000   1379. 000000   1379. 000000   1379. 000000       mean   32446. 292622   66. 592640   13. 354605   45. 328499       std   31257. 070006   3. 818108   2. 438741   15. 789715       min   -98. 580489   57. 340000   3. 000000   22. 000000       25%   10538. 790721   63. 720000   12. 000000   33. 000000       50%   26877. 870178   66. 050000   13. 000000   42. 000000       75%   44506. 215336   69. 315000   15. 000000   55. 000000       max   317949. 127955   77. 210000   18. 000000   95. 000000    unique():  series 데이터의 유일한 값을 list로 반환1df. race. unique() # 유일한 인종의 값 list1array(['white', 'other', 'hispanic', 'black'], dtype=object)1np. array(dict(enumerate(df[ race ]. unique()))) # dict type으로 index1array({0: 'white', 1: 'other', 2: 'hispanic', 3: 'black'}, dtype=object)1234value = list(map(int, np. array(list(enumerate(df[ race ]. unique())))[:,0]. tolist()))key = np. array(list(enumerate(df[ race ]. unique())), dtype=str)[:, 1]. tolist()# label index 값과 label 값 각각 추출value, key1([0, 1, 2, 3], ['white', 'other', 'hispanic', 'black'])1df[ race ]. replace(to_replace=key, value=value, inplace=True) # label 값을 index값으로 변환1234value = list(map(int, np. array(list(enumerate(df[ sex ]. unique())))[:,0]. tolist()))key = np. array(list(enumerate(df[ sex ]. unique())), dtype=str)[:, 1]. tolist()# 성별에 동일하게 적용value, key 1([0, 1], ['male', 'female']) “sex”와 “race”의 index labelling을 unique와 replace를 통해서 함12df[ sex ]. replace(to_replace=key, value=value, inplace=True)df. head()           earn   height   sex   race   ed   age         0   79571. 299011   73. 89   0   0   16   49       1   96396. 988643   66. 23   1   0   16   62       2   48710. 666947   63. 77   1   0   16   33       3   80478. 096153   63. 22   1   1   16   95       4   82089. 345498   63. 08   1   0   17   43    sum():  기본적인 column 또는 row 값의 연산 지원 sub, mean, min, max, count, median, var . . 등 전부 비슷1df. sum(axis=0) # column 별로 sum1234567earn   4. 474344e+07height  9. 183125e+04sex    8. 590000e+02race   5. 610000e+02ed    1. 841600e+04age    6. 250800e+04dtype: float641df. sum(axis=1) # row 별로 sum1234567891011120    79710. 1890111    96542. 2186432    48824. 4369473    80654. 3161534    82213. 425498      . . .   1374  30290. 0603631375  25019. 8295141376  13824. 3113121377  95563. 6644101378   9686. 681857Length: 1379, dtype: float64isnull():  column 도는 row 값의 NaN(null) 값의 index 반환1df. isnull()           earn   height   sex   race   ed   age         0   False   False   False   False   False   False       1   False   False   False   False   False   False       2   False   False   False   False   False   False       3   False   False   False   False   False   False       4   False   False   False   False   False   False       …   …   …   …   …   …   …       1374   False   False   False   False   False   False       1375   False   False   False   False   False   False       1376   False   False   False   False   False   False       1377   False   False   False   False   False   False       1378   False   False   False   False   False   False    sort_values():  column 값을 기준으로 데이터를 sorting ascending=True는 오름차순1df. sort_values([ age ,  earn ], ascending=True). head(10)           earn   height   sex   race   ed   age         1038   -56. 321979   67. 81   0   2   10   22       800   -27. 876819   72. 29   0   0   12   22       963   -25. 655260   68. 90   0   0   12   22       1105   988. 565070   64. 71   1   0   12   22       801   1000. 221504   64. 09   1   0   12   22       862   1002. 023843   66. 59   1   0   12   22       933   1007. 994941   68. 26   1   0   12   22       988   1578. 542814   64. 53   0   0   12   22       522   1955. 168187   69. 87   1   3   12   22       765   2581. 870402   64. 79   1   0   12   22    참고:  부스트코스 AI 기초다지기 pandas l - 최성철"
    }, {
    "id": 32,
    "url": "http://localhost:4000/pandas_5/",
    "title": "Pandas - 5(DataFrame Operation)",
    "body": "2023/02/15 - DataFrame Operation: Series Operation:  비어 있는 값은 NaN 겹치는 index가 없으면 NaN으로 반환 index 기준으로 연산 수행12s1 = Series(range(1,6), index=list( abcde ))s1123456a  1b  2c  3d  4e  5dtype: int6412s2 = Series(range(5,10), index=list( bcdef ))s2123456b  5c  6d  7e  8f  9dtype: int641s1. add(s2)1234567a   NaNb   7. 0c   9. 0d  11. 0e  13. 0f   NaNdtype: float641s1+s21234567a   NaNb   7. 0c   9. 0d  11. 0e  13. 0f   NaNdtype: float64DataFrame Operation:  dataframe은 column과 index를 모두 고려 fill_value를 통해 NaN값을 변환해서 연산 수행 add, sub, div, mul 연산 전부에 해당12df1 = DataFrame(np. arange(9). reshape(3,3), columns=list( abc ))df1           a   b   c         0   0   1   2       1   3   4   5       2   6   7   8    12df2 = DataFrame(np. arange(16). reshape(4,4), columns=list( abcd ))df2           a   b   c   d         0   0   1   2   3       1   4   5   6   7       2   8   9   10   11       3   12   13   14   15    1df1+df2           a   b   c   d         0   0. 0   2. 0   4. 0   NaN       1   7. 0   9. 0   11. 0   NaN       2   14. 0   16. 0   18. 0   NaN       3   NaN   NaN   NaN   NaN    1df1. add(df2,fill_value=0) # NaN값 0으로 변환하고 연산           a   b   c   d         0   0. 0   2. 0   4. 0   3. 0       1   7. 0   9. 0   11. 0   7. 0       2   14. 0   16. 0   18. 0   11. 0       3   12. 0   13. 0   14. 0   15. 0    Series + DataFrame Operation:  column 기준으로 broadcasting이 발생(axis 지정 안 할 경우)12df = DataFrame(np. arange(16). reshape(4,4), columns=list( abcd ))df           a   b   c   d         0   0   1   2   3       1   4   5   6   7       2   8   9   10   11       3   12   13   14   15    12s = Series(np. arange(10,14),index=list( abcd ))s12345a  10b  11c  12d  13dtype: int32 출처-https://stackoverflow. com/questions/29954263/what-does-the-term-broadcasting-mean-in-pandas-documentation 1df+s # column을 기준으로 broadcasting 일어남           a   b   c   d         0   10   12   14   16       1   14   16   18   20       2   18   20   22   24       3   22   24   26   28     axis를 기준으로 row broadcasting 실행 가능12df = DataFrame(np. arange(16). reshape(4,4), columns=list( abcd ))s2 = Series(np. arange(10,14))df+s2           a   b   c   d   0   1   2   3         0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN       3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN    1df. add(s2,axis=0)           a   b   c   d         0   10   11   12   13       1   15   16   17   18       2   20   21   22   23       3   25   26   27   28    참고:  부스트코스 AI 기초다지기 pandas l - 최성철 https://stackoverflow. com/questions/29954263/what-does-the-term-broadcasting-mean-in-pandas-documentation"
    }, {
    "id": 33,
    "url": "http://localhost:4000/pandas_4/",
    "title": "Pandas - 4(Selection, Drop)",
    "body": "2023/02/14 - Selection &amp; Drop: Selection: Selection with column names: head() 제일 상단 부터 안에 적힌 수 만큼 데이터를 가져옴12df = pd. read_excel( . /pandas_data/excel-comp-data. xlsx )df. head()          account   name   street   city   state   postal-code   Jan   Feb   Mar         0   211829   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       1   320563   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       2   648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       3   109996   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       4   121213   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000    한개의 column 선택시, series 객체1df[ account ]. head(2) # 상단 부터 2개의 데이터 가져옴1230  2118291  320563Name: account, dtype: int64 1개 이상의 column 선택시, dataframe 객체1df[[ account ,  street ,  state ]]. head(3) # 대괄호로 묶은 리스트 형태로 넣어야함          account   street   state         0   211829   34456 Sean Highway   Texas       1   320563   1311 Alvis Tunnel   NorthCarolina       2   648336   62184 Schamberger Underpass Apt. 231   Iowa    Selection with index number: dataframe[:n] row 기준으로 데이터 가져옴 column 이름 없이 사용하는 index number는 row기준 표시1df[:3]          account   name   street   city   state   postal-code   Jan   Feb   Mar         0   211829   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       1   320563   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       2   648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000    column 이름과 함께 row index 사용시, 해당 column만1df[ account ][:3]12340  2118291  3205632  648336Name: account, dtype: int64Series Selection: series[:n] 특정 series의 데이터를 가져와서 특정 인덱스 까지의 데이터 뽑기12account_series = df[ account ] #  account 의 series 데이터 가져옴account_series[:3] #  account 의 series에서 상단에서 3번째 data까지 가져옴1234 0  211829 1  320563 2  648336 Name: account, dtype: int64series[[a,b,c,. . ]] 명시한 인덱스들에 해당하는 데이터 뽑기1account_series[[1,5,2]] # 인덱스 번호에 해당하는 데이터 가져오기12341  3205635  1329712  648336Name: account, dtype: int64series[condition] boolean index를 이용해서 조건에 맞는 데이터만 뽑기1account_series[account_series&lt;250000]1234567891011120   2118293   1099964   1212135   1329716   1450687   2052178   2097449   21230310  21409811  23190712  242368Name: account, dtype: int64 Index Change: 인덱스 변경하기:  원래 index는 0~ 부터 시작하는 숫자 데이터를 분류하다 보면 특정 케이스를 식별(identify) 할 수 있는 데이터가 존재 함 (ex. 주민등록번호,전화번호) 그 series의 데이터를 index로 사용 할 수 있음12df. index = df[ account ] #  account  series를 인덱스로 사용df. head()          account   name   street   city   state   postal-code   Jan   Feb   Mar         account                                           211829   211829   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       320563   320563   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       648336   648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       109996   109996   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       121213   121213   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000   12del df[ account ] #  account  series가 그대로 남아있기 때문에 지워준다df. head()          name   street   city   state   postal-code   Jan   Feb   Mar         account                                       211829   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       320563   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       109996   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       121213   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000    인덱스 재설정하기:  그냥 0~ 부터의 숫자를 인덱스로 사용하기 merge가 없을 경우 편함12df. index = list(range(0,15))df. head()          name   street   city   state   postal-code   Jan   Feb   Mar         0   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       1   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       2   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       3   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       4   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000    Basic, loc, iloc Selection: basic:  dataframe[[column list]] [:n] column과 index number를 사용1df[[ name , street ]][:2]          name   street         account               211829   Kerluke, Koepp and Hilpert   34456 Sean Highway       320563   Walter-Trantow   1311 Alvis Tunnel    loc[[index name list],[column list]]:  index name과 column 사용1df. loc[[211829,320563],[ name , street ]]          name   street         account               211829   Kerluke, Koepp and Hilpert   34456 Sean Highway       320563   Walter-Trantow   1311 Alvis Tunnel    iloc[]:  index number와 column number 사용1df. iloc[:3,:2]          name   street         account               211829   Kerluke, Koepp and Hilpert   34456 Sean Highway       320563   Walter-Trantow   1311 Alvis Tunnel       648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231    column name과 index number 사용1df[[ name ,  street ]]. iloc[:4]          name   street         account               211829   Kerluke, Koepp and Hilpert   34456 Sean Highway       320563   Walter-Trantow   1311 Alvis Tunnel       648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231       109996   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144    Drop: 1df. head(8)          name   street   city   state   postal-code   Jan   Feb   Mar         0   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       1   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       2   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       3   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       4   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000       5   Williamson, Schumm and Hettinger   89403 Casimer Spring   Jeremieburgh   Arkansas   62785   150000   120000   35000       6   Casper LLC   340 Consuela Bridge Apt. 400   Lake Gabriellaton   Mississipi   18008   62000   120000   70000       7   Kovacek-Johnston   91971 Cronin Vista Suite 601   Deronville   RhodeIsland   53461   145000   95000   35000    drop(i):  index number로 drop1df. head(8). drop(1)          name   street   city   state   postal-code   Jan   Feb   Mar         0   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       2   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       3   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Hyattburgh   Maine   46021   45000   120000   10000       4   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000       5   Williamson, Schumm and Hettinger   89403 Casimer Spring   Jeremieburgh   Arkansas   62785   150000   120000   35000       6   Casper LLC   340 Consuela Bridge Apt. 400   Lake Gabriellaton   Mississipi   18008   62000   120000   70000       7   Kovacek-Johnston   91971 Cronin Vista Suite 601   Deronville   RhodeIsland   53461   145000   95000   35000    drop([a,b,c. . ]):  한개 이상의 index number로 drop1df. head(8). drop([0,1,2,3])          name   street   city   state   postal-code   Jan   Feb   Mar         4   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000       5   Williamson, Schumm and Hettinger   89403 Casimer Spring   Jeremieburgh   Arkansas   62785   150000   120000   35000       6   Casper LLC   340 Consuela Bridge Apt. 400   Lake Gabriellaton   Mississipi   18008   62000   120000   70000       7   Kovacek-Johnston   91971 Cronin Vista Suite 601   Deronville   RhodeIsland   53461   145000   95000   35000    drop(“column”, axis=i):  axis 지정으로 축을 기준으로 drop1df. head(5). drop( city , axis=1) # column 중  city  drop          name   street   state   postal-code   Jan   Feb   Mar         0   Kerluke, Koepp and Hilpert   34456 Sean Highway   Texas   28752   10000   62000   35000       1   Walter-Trantow   1311 Alvis Tunnel   NorthCarolina   38365   95000   45000   35000       2   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   Iowa   76517   91000   120000   35000       3   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Maine   46021   45000   120000   10000       4   Bauch-Goldner   7274 Marissa Common   California   49681   162000   120000   35000    1df. head(5). drop(3, axis=0) # 3번 row drop, 사실상 그냥 drop(i)와 같음          name   street   city   state   postal-code   Jan   Feb   Mar         0   Kerluke, Koepp and Hilpert   34456 Sean Highway   New Jaycob   Texas   28752   10000   62000   35000       1   Walter-Trantow   1311 Alvis Tunnel   Port Khadijah   NorthCarolina   38365   95000   45000   35000       2   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   New Lilianland   Iowa   76517   91000   120000   35000       4   Bauch-Goldner   7274 Marissa Common   Shanahanchester   California   49681   162000   120000   35000    inplace = True로 원본에서 삭제:  pandas는 원본의 내용을 쉽게 삭제 안함 원본을 변경하기 위해서는 inplace = True를 추가해야함12df. drop( city ,axis=1,inplace=True)df. head()          account   name   street   state   postal-code   Jan   Feb   Mar         0   211829   Kerluke, Koepp and Hilpert   34456 Sean Highway   Texas   28752   10000   62000   35000       1   320563   Walter-Trantow   1311 Alvis Tunnel   NorthCarolina   38365   95000   45000   35000       2   648336   Bashirian, Kunde and Price   62184 Schamberger Underpass Apt. 231   Iowa   76517   91000   120000   35000       3   109996   D’Amore, Gleichner and Bode   155 Fadel Crescent Apt. 144   Maine   46021   45000   120000   10000       4   121213   Bauch-Goldner   7274 Marissa Common   California   49681   162000   120000   35000    참고:  부스트코스 AI 기초다지기 pandas l - 최성철"
    }, {
    "id": 34,
    "url": "http://localhost:4000/pandas_3/",
    "title": "Pandas - 3(DataFrame)",
    "body": "2023/02/13 - Pandas DataFrame: DataFrame의 특징: 출처-https://www. geeksforgeeks. org/python-pandas-dataframe/  series들이 모인 전체 data table numpy array와 유사 row와 column index 존재 각 column은 다른 type일 수 있음 기본적으로 2차원DataFrame 사용: DataFrame(): 1234567# Example from - https://chrisalbon. com/python/pandas_map_values_to_values. htmlraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],    'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'],    'age': [42, 52, 36, 24, 73],    'city': ['San Francisco', 'Baltimore', 'Miami', 'Douglas', 'Boston']}df = pd. DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'city'])df          first_name   last_name   age   city         0   Jason   Miller   42   San Francisco       1   Molly   Jacobson   52   Baltimore       2   Tina   Ali   36   Miami       3   Jake   Milner   24   Douglas       4   Amy   Cooze   73   Boston    특정 column들만 가져오기1DataFrame(raw_data, columns = [ age ,  city ])          age   city         0   42   San Francisco       1   52   Baltimore       2   36   Miami       3   24   Douglas       4   73   Boston    새로운 column 추가하기123DataFrame(raw_data,      columns = [ first_name , last_name , age ,  city ,  debt ]     )          first_name   last_name   age   city   debt         0   Jason   Miller   42   San Francisco   NaN       1   Molly   Jacobson   52   Baltimore   NaN       2   Tina   Ali   36   Miami   NaN       3   Jake   Milner   24   Douglas   NaN       4   Amy   Cooze   73   Boston   NaN    column 선택 - series 추출 (1)12df = DataFrame(raw_data, columns = [ first_name , last_name , age ,  city ,  debt ])df. first_name1234560  Jason1  Molly2   Tina3   Jake4   AmyName: first_name, dtype: object column 선택 - series 추출 (2)1df[ first_name ]1234560  Jason1  Molly2   Tina3   Jake4   AmyName: first_name, dtype: objectDataFrame data handling: 데이터 인덱스로 접근하기: loc[] index location 인덱스의 이름으로 접근하는 방식임1df          first_name   last_name   age   city   debt         0   Jason   Miller   42   San Francisco   NaN       1   Molly   Jacobson   52   Baltimore   NaN       2   Tina   Ali   36   Miami   NaN       3   Jake   Milner   24   Douglas   NaN       4   Amy   Cooze   73   Boston   NaN   1df. loc[1]123456first_name    Mollylast_name   Jacobsonage         52city     Baltimoredebt        NaNName: 1, dtype: objectiloc[] index position 인덱스의 number로 접근1df[ age ]. iloc[1:]123451  522  363  244  73Name: age, dtype: int64loc[] vs iloc[]123# Example from - https://stackoverflow. com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanations = pd. Series(np. nan, index=[49,48,47,46,45, 1, 2, 3, 4, 5])s123456789101149  NaN48  NaN47  NaN46  NaN45  NaN1  NaN2  NaN3  NaN4  NaN5  NaNdtype: float64loc[]1s. loc[:3] # 인덱스 이름이 3인 지점 까지12345678949  NaN48  NaN47  NaN46  NaN45  NaN1  NaN2  NaN3  NaNdtype: float64iloc[]1s. iloc[:3] # 3번째 인덱스 까지(인덱스 2 까지)123449  NaN48  NaN47  NaNdtype: float64Column에 새로운 데이터 할당: 1df. age &gt; 401234560   True1   True2  False3  False4   TrueName: age, dtype: bool 조건을 통해 반환된 값을 새로운 column에 할당하기12df. debt = df. age &gt; 40df          first_name   last_name   age   city   debt         0   Jason   Miller   42   San Francisco   True       1   Molly   Jacobson   52   Baltimore   True       2   Tina   Ali   36   Miami   False       3   Jake   Milner   24   Douglas   False       4   Amy   Cooze   73   Boston   True   12values = Series(data=[ M , F , F ],index=[0,1,3])values12340  M1  F3  Fdtype: object12df[ sex ] = valuesdf          first_name   last_name   age   city   debt   sex         0   Jason   Miller   42   San Francisco   True   M       1   Molly   Jacobson   52   Baltimore   True   F       2   Tina   Ali   36   Miami   False   NaN       3   Jake   Milner   24   Douglas   False   F       4   Amy   Cooze   73   Boston   True   NaN   T 메소드:  transpose1df. T          0   1   2   3   4         first_name   Jason   Molly   Tina   Jake   Amy       last_name   Miller   Jacobson   Ali   Milner   Cooze       age   42   52   36   24   73       city   San Francisco   Baltimore   Miami   Douglas   Boston       debt   True   True   False   False   True   values:  array 형태로 값1df. values12345array([['Jason', 'Miller', 42, 'San Francisco', True, 'M'],    ['Molly', 'Jacobson', 52, 'Baltimore', True, 'F'],    ['Tina', 'Ali', 36, 'Miami', False, nan],    ['Jake', 'Milner', 24, 'Douglas', False, 'F'],    ['Amy', 'Cooze', 73, 'Boston', True, nan]], dtype=object)to_csv():  csv로 변환1df. to_csv()1',first_name,last_name,age,city,debt,sex\n0,Jason,Miller,42,San Francisco,True,M\n1,Molly,Jacobson,52,Baltimore,True,F\n2,Tina,Ali,36,Miami,False,\n3,Jake,Milner,24,Douglas,False,F\n4,Amy,Cooze,73,Boston,True,\n'del:  column을 삭제함12del df[ debt ]df          first_name   last_name   age   city         0   Jason   Miller   42   San Francisco       1   Molly   Jacobson   52   Baltimore       2   Tina   Ali   36   Miami       3   Jake   Milner   24   Douglas       4   Amy   Cooze   73   Boston   dict 타입에서 dataframe 생성: 1234pop = {'Nevada': {2001: 2. 4, 2002: 2. 9}, 'Ohio': {2000: 1. 5, 2001: 1. 7, 2002: 3. 6}}DataFrame(pop)          Nevada   Ohio         2001   2. 4   1. 7       2002   2. 9   3. 6       2000   NaN   1. 5    참고:  부스트코스 AI 기초다지기 pandas l - 최성철 https://www. geeksforgeeks. org/python-pandas-dataframe"
    }, {
    "id": 35,
    "url": "http://localhost:4000/pandas_2/",
    "title": "Pandas - 2(Series)",
    "body": "2023/02/12 - Pandas Series: Series의 특징: 출처 - https://www. geeksforgeeks. org/python-pandas-series/  numpy. ndarray의 subclass 인덱스는 순서대로 존재하지 않아도 됨, 중복 가능Series, DataFrame 객체: 12from pandas import Series, DataFrameimport numpy as npSeries 사용: Series(): 123list_data = [1,2,3,4,5]example_obj = Series(data = list_data) # data에는 list 타입 or dict 타입 가능example_obj # series 객체, index, value, data type이 나옴1234560  11  22  33  44  5dtype: int64 index 이름 정할 수 있음1234list_data = [1,2,3,4,5]list_name = [ a , b , c , d , e ]example_obj = Series(data = list_data, index=list_name) # index의 이름도 지정 가능, 잘 쓰지는 않음example_obj123456a  1b  2c  3d  4e  5dtype: int64 index 이름을 따로 넣지 않고 바로 dict 타입으로 이름을 넣을 수 있음123dict_data = { a :1,  b :2,  c :3,  d :4,  e :5}example_obj = Series(dict_data, dtype=np. float32, name= example_data ) # data type, series 이름 설정example_obj123456a  1. 0b  2. 0c  3. 0d  4. 0e  5. 0Name: example_data, dtype: float32Series data index 접근하기:  series객체명[인덱스명] 형태로 접근 가능1example_obj[ a ]11. 0 data index에 값 할당하기12example_obj[ a ] = 3. 2example_obj123456a  3. 2b  2. 0c  3. 0d  4. 0e  5. 0Name: example_data, dtype: float32 series의 값만 가져오기1example_obj. values1array([1. , 2. , 3. , 4. , 5. ], dtype=float32) series의 인덱스만 가져오기1example_obj. index1Index(['a', 'b', 'c', 'd', 'e'], dtype='object')to_dict():  series 객체를 dict type으로1example_obj. to_dict() # series 객체를 dict 형태로1{'a': 1. 0, 'b': 2. 0, 'c': 3. 0, 'd': 4. 0, 'e': 5. 0}index 값을 기준으로 series 생성하기: 1234dict_data_1 = { a :1,  b :2,  c :3,  d :4,  e :5}indexes = [ a , b , c , d , e , f , g , h ]series_obj_1 = Series(dict_data_1, index=indexes)series_obj_1123456789a  1. 0b  2. 0c  3. 0d  4. 0e  5. 0f  NaNg  NaNh  NaNdtype: float64참고:  부스트코스 AI 기초다지기 pandas l - 최성철 https://www. geeksforgeeks. org/python-pandas-series/"
    }, {
    "id": 36,
    "url": "http://localhost:4000/pandas_1/",
    "title": "Pandas - 1(Introduction)",
    "body": "2023/02/10 - Pandas 소개: Pandas란?:  구조화된 데이터의 처리를 지원하는 python 라이브러리 고성능 array 계산 라이브러리인 numpy와 통합하여, 강력한 스프레드시트 처리 기능을 제공 인덱싱, 연산용 함수, 전처리 함수 등을 제공 부동 소수점이 아닌 데이터 뿐만 아니라 부동 소수점 데이터에서도 결측 데이터(NaN)를 쉽게 처리 데이터 세트의 유연한 재구성 및 피벗 축의 계층적 라벨링(눈금당 여러 개의 라벨 가질 수 있음) 시계열 특정 기능 : 날짜 범위 생성, 주파수 변환, moving window 통계, 날짜 이동 및 지연Pandas 시작하기: Pandas 라이브러리 호출: 1import pandas as pdPandas Data Loading: read_csv():  csv 타입 데이터 로드 sep(구분자)은 정규표현식을 이용해서 지정123data_url = 'https://archive. ics. uci. edu/ml/machine-learning-databases/housing/housing. data'# data urldf_data = pd. read_csv(data_url, sep='\s+', header=None) # csv 타입 데이터 로드, seperate은 빈공간으로 지정, column 없음head(): 1df_data. head() # 처음 다섯줄 출력columns:  column의 이름 지정1234df_data. columns = [  'CRIM','ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO' ,'B', 'LSTAT', 'MEDV'] # Column Header 이름 지정df_data. head()          CRIM   ZN   INDUS   CHAS   NOX   RM   AGE   DIS   RAD   TAX   PTRATIO   B   LSTAT   MEDV         0   0. 00632   18. 0   2. 31   0   0. 538   6. 575   65. 2   4. 0900   1   296. 0   15. 3   396. 90   4. 98   24. 0       1   0. 02731   0. 0   7. 07   0   0. 469   6. 421   78. 9   4. 9671   2   242. 0   17. 8   396. 90   9. 14   21. 6       2   0. 02729   0. 0   7. 07   0   0. 469   7. 185   61. 1   4. 9671   2   242. 0   17. 8   392. 83   4. 03   34. 7       3   0. 03237   0. 0   2. 18   0   0. 458   6. 998   45. 8   6. 0622   3   222. 0   18. 7   394. 63   2. 94   33. 4       4   0. 06905   0. 0   2. 18   0   0. 458   7. 147   54. 2   6. 0622   3   222. 0   18. 7   396. 90   5. 33   36. 2    values:    dataframe의 numpy 형태의 값을 불러옴     pandas는 결국 numpy 기반으로 만들어졌기 때문에 값의 형태는 numpy 형태임  1df_data. values12345678910111213array([[6. 3200e-03, 1. 8000e+01, 2. 3100e+00, . . . , 3. 9690e+02, 4. 9800e+00,    2. 4000e+01],    [2. 7310e-02, 0. 0000e+00, 7. 0700e+00, . . . , 3. 9690e+02, 9. 1400e+00,    2. 1600e+01],    [2. 7290e-02, 0. 0000e+00, 7. 0700e+00, . . . , 3. 9283e+02, 4. 0300e+00,    3. 4700e+01],    . . . ,    [6. 0760e-02, 0. 0000e+00, 1. 1930e+01, . . . , 3. 9690e+02, 5. 6400e+00,    2. 3900e+01],    [1. 0959e-01, 0. 0000e+00, 1. 1930e+01, . . . , 3. 9345e+02, 6. 4800e+00,    2. 2000e+01],    [4. 7410e-02, 0. 0000e+00, 1. 1930e+01, . . . , 3. 9690e+02, 7. 8800e+00,    1. 1900e+01]]) type은 당연히 ndarray로 나옴1type(df_data. values) # numpy의 ndarray 타입1numpy. ndarrayPandas의 구성: 출처: https://geo-python-site. readthedocs. io/en/latest/notebooks/L5/exploring-data-using-pandas. html  series는 하나의 벡터로 볼 수 있음 data table 전체는 dataframe참고:  부스트코스 AI 기초다지기 pandas l - 최성철 https://geo-python-site. readthedocs. io/en/latest/notebooks/L5/exploring-data-using-pandas. html"
    }, {
    "id": 37,
    "url": "http://localhost:4000/numpy_performance/",
    "title": "Numpy - 7(Numpy Performance)",
    "body": "2023/02/08 - Numpy performance check: timeit:  Jupyter 환경에서 코드의 퍼포먼스 체크하는 함수123456789def sclar_vector_product(scalar, vector):  result = []  for value in vector:    result. append(scalar * value)  return resultiternation_max = 100000000vector = list(range(iternation_max))scalar = 21234%timeit sclar_vector_product(scalar, vector) # for loop을 이용한 성능%timeit [scalar * value for value in range(iternation_max)]# list comprehension을 이용한 성능%timeit np. arange(iternation_max) * scalar # numpy를 이용한 성능속도 비교:  일반적으로 속도는  for loop &lt; list comprehension &lt; numpy  100,000,000번(1억 번)의 loop가 돌 때, 약 4배 이상의 성능 차이 numpy는 C로 구현되어 있어, 성능을 확보하는 대신 파이썬의 dynamic typing을 포기함 대용량 계산에서는 가장 흔히 사용됨 concatenate 처럼 계산이 아닌 할당에서는 연산 속도의 이점 없음참고:  부스트 코스 AI numpy - 최성철"
    }, {
    "id": 38,
    "url": "http://localhost:4000/numpy_io/",
    "title": "Numpy - 6(Data I/O)",
    "body": "2023/02/08 - Data I/O: loadtxt &amp; savetxt:  text type의 데이터를 읽고, 저장하는 기능loadtxt:  파일 호출12a = np. loadtxt( . /populations. txt )a[:10]12345678910array([[ 1900. , 30000. ,  4000. , 48300. ],    [ 1901. , 47200. ,  6100. , 48200. ],    [ 1902. , 70200. ,  9800. , 41500. ],    [ 1903. , 77400. , 35200. , 38200. ],    [ 1904. , 36300. , 59400. , 40600. ],    [ 1905. , 20600. , 41700. , 39800. ],    [ 1906. , 18100. , 19000. , 38600. ],    [ 1907. , 21400. , 13000. , 42300. ],    [ 1908. , 22000. ,  8300. , 44500. ],    [ 1909. , 25400. ,  9100. , 42100. ]]) int type 변환123a_int = a. astype(int)a_int[:3]a_int_3 = a_int[:3]123array([[ 1900, 30000, 4000, 48300],    [ 1901, 47200, 6100, 48200],    [ 1902, 70200, 9800, 41500]])savetxt: 1np. savetxt('int_data. csv',a_int_3, delimiter= , )numpy object - npy:  numpy object(pickle) 형태로 데이터를 저장하고 불러옴 binary 파일 형태로 저장1np. save( npy_test , arr=a_int)12npy_array = np. load(file= npy_test. npy )npy_array[:3]참고:  부스트 코스 AI numpy - 최성철"
    }, {
    "id": 39,
    "url": "http://localhost:4000/numpy_array_operation/",
    "title": "Numpy - 5(Array Operations 3)",
    "body": "2023/02/07 - 배열 연산:  numpy에서 배열 연산은 벡터화(vectorized) 연산을 사용 일반적으로 numpy의 범용 함수(universal functions)를 통해서 구현 배열 요소에 대한 반복적인 계산을 효율적으로 수행Axis의 의미:  element의 합을 구해주는 sum()을 이용해서 설명하면Matrix: 12test_matrix = np. arange(1,13). reshape(3,4)test_matrix123array([[ 1, 2, 3, 4],    [ 5, 6, 7, 8],    [ 9, 10, 11, 12]]) axis=11test_matrix. sum(axis=1) # 같은 row 방향으로 sum1array([10, 26, 42]) axis=01test_matrix. sum(axis=0) # 같은 col 방향으로 sum1array([15, 18, 21, 24])3rd order tensor: 123test_matrix = np. arange(1,13). reshape(3,4)third_order_tensor = np. stack([test_matrix,test_matrix,test_matrix],axis=0)third_order_tensor1234567891011array([[[ 1, 2, 3, 4],    [ 5, 6, 7, 8],    [ 9, 10, 11, 12]],    [[ 1, 2, 3, 4],    [ 5, 6, 7, 8],    [ 9, 10, 11, 12]],    [[ 1, 2, 3, 4],    [ 5, 6, 7, 8],    [ 9, 10, 11, 12]]]) axis=21third_order_tensor. sum(axis=2) # 같은 row방향으로 sum123array([[10, 26, 42],    [10, 26, 42],    [10, 26, 42]]) axis=11third_order_tensor. sum(axis=1) # 같은 col방향으로 sum123array([[15, 18, 21, 24],    [15, 18, 21, 24],    [15, 18, 21, 24]]) axis=01third_order_tensor. sum(axis=0) # 깊이 방향으로 sum123array([[ 3, 6, 9, 12],    [15, 18, 21, 24],    [27, 30, 33, 36]])Mean &amp; Std: Mean:  평균 구하기12test_array = np. arange(1,13). reshape(3,4)test_array123array([[ 1, 2, 3, 4],    [ 5, 6, 7, 8],    [ 9, 10, 11, 12]])1test_array. mean()16. 51test_array. mean(axis=0)1array([5. , 6. , 7. , 8. ])Std:  표준편차 구하기1test_array. std()13. 4520525295346631test_array. std(axis=1)1array([1. 11803399, 1. 11803399, 1. 11803399])Mathematical Functions:    numpy는 다양한 수학 연산자를 제공함   absolute(), abs() : 절대값 함수   square(), sqrt() : 제곱, 제곱근 함수   cumsum() : 누적합 계산 diff() : 차분 계산 prod() : 곱 계산 dot() : 점곱 계산 matmul() : 행렬곱 계산 tensordot() : 텐서곱 계산 cross() : 벡터곱 계산 inner() : 내적 계산 outer() : 외적 계산 var() : 분산 계산 min() : 최소값 max() : 최대값 argmin() : 최소값 인덱스 argmax() : 최대값 인덱스 median() : 중앙값이 외에도 지수함수, 로그함수, 삼각함수 등 여러가지 함수 제공 Basic Array Operations: 1test_a = np. array([[1,2,3],[4,5,6]], float)+ operation: 1test_a + test_a12array([[ 2. , 4. , 6. ],    [ 8. , 10. , 12. ]])- operation: 1test_a - test_a12array([[0. , 0. , 0. ],    [0. , 0. , 0. ]])* operation:  element-wise operation, dot-product와 다름1test_a * test_a # * operation은 element-wise operation, 같은 위치의 값들 끼리 계산12array([[ 1. , 4. , 9. ],    [16. , 25. , 36. ]])Dot product:  matrix의 기본 연산, dot() 함수 사용123test_a = np. arange(1,7). reshape(2,3)test_b = np. arange(7,13). reshape(3,2)test_a. dot(test_b)12array([[ 58, 64],    [139, 154]])Broadcasting:  shape이 다른 배열 간 연산을 지원하는 기능 Matrix - Scalar , Vector - Scalar 연산: 12test_matrix = np. array([[1,2,3],[4,5,6]],float)scalar = 3 덧셈1test_matrix + scalar # Matrix-Scalar 덧셈12array([[4. , 5. , 6. ],    [7. , 8. , 9. ]]) 뺄셈1test_matrix - scalar # Matrix-Scalar 뺄셈12array([[-2. , -1. , 0. ],    [ 1. , 2. , 3. ]]) 곱셈1test_matrix * scalar # Matrix-Scalar 곱셈12array([[ 3. , 6. , 9. ],    [12. , 15. , 18. ]]) 나눗셈1test_matrix / 5 # Matrix-Scalar 나눗셈12array([[0. 2, 0. 4, 0. 6],    [0. 8, 1. , 1. 2]]) 몫1test_matrix // 0. 2 # Matrix-Scalar 몫12array([[ 4. , 9. , 14. ],    [19. , 24. , 29. ]]) 제곱1test_matrix ** 2 # Matrix-Scalar 제곱12array([[ 1. , 4. , 9. ],    [16. , 25. , 36. ]])Vector - Matrix 연산:  123test_matrix = np. arange(1,13). reshape(4,3)test_vector = np. arange(10,40,10)test_matrix + test_vector1234array([[11, 22, 33],    [14, 25, 36],    [17, 28, 39],    [20, 31, 42]])Comparison (비교연산): All &amp; Any:  Array의 데이터 전부(and) 또는 일부(or)가 조건에 만족하는지 여부 반환12a = np. arange(10)a1array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])1np. any(a&gt;5), np. any(a&lt;0) # any는 하나라도 조건에 만족하면 True1(True, False)1np. all(a&gt;5), np. all(a&lt;10) # all은 모두가 조건에 만족해야 True1(False, True)Array Comparison Operation:  numpy는 배열의 크기가 동일 할 때 element간 비교의 결과를 Boolean type으로 반환123test_a = np. array([1,3,0], float)test_b = np. array([5,2,1], float)test_a &gt; test_b1array([False, True, False])1test_a == test_b1array([False, False, False])1(test_a &gt; test_b). any() # any는 하나라도 true면 true1TrueAnd, Not, Or operation:  logical_and()12a = np. array([1,3,0], float)np. logical_and(a&gt;0, a&lt;3) # and 조건 condition1array([ True, False, False]) logical_not()12b = np. array([True,False,True], bool)np. logical_not(b)1array([False, True, False]) logical_or()12c = np. array([False,True,False],bool)np. logical_or(b,c)1array([ True, True, True])np. where():  where(condition, True, False)12a = np. array([1,3,0], float)np. where(a&gt;0,3,2) # True면 3 반환, False면 2 반환1array([3, 3, 2]) 조건을 만족하는 Index 반환123a = np. arange(10)print(a)np. where(a&gt;5)12[0 1 2 3 4 5 6 7 8 9](array([6, 7, 8, 9], dtype=int64),)np. isnan():  NaN일 경우 True 반환12a = np. array([1, np. NaN, np. Inf], float)np. isnan(a) # not a number, NaN일 경우 True 반환1array([False, True, False])np. isfinite():  finite number일 경우 True 반환1np. isfinite(a) # finite number일 경우 True 반환1array([ True, False, False])argmax &amp; argmin:  array내 최대값 또는 최소값의 index 반환12a = np. array([1,2,4,5,8,78,23,3])np. argmax(a), np. argmin(a)1(5, 0) axis 기반의 반환12a = np. array([[1,2,4,7],[9,88,6,45],[9,76,3,4]])print(a)123[[ 1 2 4 7] [ 9 88 6 45] [ 9 76 3 4]]1np. argmax(a,axis=1) # 각 row의 최대값의 index 반환1array([3, 1, 1], dtype=int64)1np. argmin(a,axis=0) # 각 col의 최소값의 index 반환1array([0, 0, 2, 2], dtype=int64)참고:  Numpy 한번에 제대로 배우기 이수안 컴퓨터 연구소 부스트 코스 AI numpy - 최성철"
    }, {
    "id": 40,
    "url": "http://localhost:4000/numpy_array_transform/",
    "title": "Numpy - 4(Array Operations 2)",
    "body": "2023/02/05 - 배열 변환: 배열 전치(Transpose) 및 축 변경: T 메소드:  2차원 배열 T 메소드123a2 = np. array([[1,2,3],        [4,5,6],        [7,8,9]])123456789a3 = np. array([[[1,2,3],        [4,5,6],        [7,8,9]],        [[1,2,3],        [4,5,6],        [7,8,9]],        [[1,2,3],        [4,5,6],        [7,8,9]]])12print(a2)print(a2. T) # 배열의 전치(Transpose)1234567[[1 2 3] [4 5 6] [7 8 9]] [[1 4 7] [2 5 8] [3 6 9]] 3차원 배열 T 메소드12print(a3)print(a3. T)1234567891011121314151617181920212223[[[1 2 3] [4 5 6] [7 8 9]] [[1 2 3] [4 5 6] [7 8 9]] [[1 2 3] [4 5 6] [7 8 9]]] [[[1 1 1] [4 4 4] [7 7 7]] [[2 2 2] [5 5 5] [8 8 8]] [[3 3 3] [6 6 6] [9 9 9]]]swapaxes():  배열의 축 교환12345678910a4 = np. array([[[ 0, 1, 2, 3],        [ 4, 5, 6, 7],        [ 8, 9, 10, 11]],        [[12, 13, 14, 15],         [16, 17, 18, 19],         [20, 21, 22, 23]]])print(a4)print(a4. shape) # 현재 정의된 축의 길이 axis0=2, axis1=3, axis2=4# 3행 4열의 배열이 2층 쌓인 3차원 배열123456789[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] (2, 3, 4)123a4_swap = a4. swapaxes(0,1) # axis0과 axis1 교환print(a4_swap) print(a4_swap. shape) # 나오는 shape는 (3,2,4)12345678910[[[ 0 1 2 3] [12 13 14 15]] [[ 4 5 6 7] [16 17 18 19]] [[ 8 9 10 11] [20 21 22 23]]] (3, 2, 4)transpose(): 123print(a4)print(a4. shape)# 기존 shape인 (2,3,4)는 axis 0,1,2에 매칭되어 있음123456789[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] (2, 3, 4)123a4_transpose = a4. transpose(2,1,0) # 새로입력한 (2,1,0)으로 축 순서 변경print(a4_transpose) print(a4_transpose. shape) # shape은 (4,3,2)로 변경됨1234567891011121314151617[[[ 0 12] [ 4 16] [ 8 20]] [[ 1 13] [ 5 17] [ 9 21]] [[ 2 14] [ 6 18] [10 22]] [[ 3 15] [ 7 19] [11 23]]] (4, 3, 2)배열 재구조화: reshape():  배열의 형상을 변경1234print(a2)print(a2. shape)print(a2. reshape(1,9)) # (3,3)을 (1,9)로print(a2. reshape(9)) 123456789[[1 2 3] [4 5 6] [7 8 9]] (3, 3)[[1 2 3 4 5 6 7 8 9]][1 2 3 4 5 6 7 8 9]123a1 = np. array([1,2,3,4,5,6,7,8,9])print(a1. shape)print(a1. reshape(3,3))12345(9,)[[1 2 3] [4 5 6] [7 8 9]]newaxis:  새로운 축 추가123print(a1)print(a1[np. newaxis, :5])print(a1[:5, np. newaxis])123456789[1 2 3 4 5 6 7 8 9][[1 2 3 4 5]][[1] [2] [3] [4] [5]]배열 크기 변경: resize():  배열의 크기와 모양 변경 가능, 원본 배열 직접적으로 변경함 배열 모양만 변경하는 경우1234n2 = np. random. randint(0,10,(2,5))print(n2)n2. resize((5,2))print(n2)12345678[[6 6 1 7 9] [5 2 9 2 0]] [[6 6] [1 7] [9 5] [2 9] [2 0]] 배열 크기가 증가하는 경우 남은 공간은 0으로 채워짐12n2. resize((5,5)) # 남은 공간은 0으로 채움print(n2)12345[[6 6 1 7 9] [5 2 9 2 0] [0 0 0 0 0] [0 0 0 0 0] [0 0 0 0 0]] 배열 크기가 감소하는 경우 포함되지 않은 영역은 삭제됨12n2. resize((3,3)) # 포함되지 않은 값은 삭제됨print(n2)123[[6 6 1] [7 9 0] [0 0 0]]배열 추가: append():  배열 끝에 값 추가 axis 지정이 없으면 1차원 배열 형태로 변형되어 결합1234a2 = np. arange(1,10). reshape(3,3)print(a2)b2 = np. arange(10,19). reshape(3,3)print(b2)1234567[[1 2 3] [4 5 6] [7 8 9]] [[10 11 12] [13 14 15] [16 17 18]]12c2 = np. append(a2,b2) # 2차원 배열끼리 append해도 1차원 배열 형태로 결합됨print(c2) 1[ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18] axis = 0 shape[0]을 제외한 나머지 shape는 같아야 함12c2 = np. append(a2,b2,axis=0) # axis를 통해서 append되는 방향 결정print(c2)123456[[ 1 2 3] [ 4 5 6] [ 7 8 9] [10 11 12] [13 14 15] [16 17 18]] axis = 1 shape[1]을 제외한 나머지 shape는 같아야 함12c2 = np. append(a2,b2,axis=1)print(c2)123[[ 1 2 3 10 11 12] [ 4 5 6 13 14 15] [ 7 8 9 16 17 18]]배열 연결: concatenate():  튜플이나 배열의 리스트를 인수로 사용해 배열 연결1234a1 = np. array([1,3,5])b1 = np. array([2,4,6])c1 = np. concatenate([a1,b1])print(c1)1[1 3 5 2 4 6]123d1 = np. array([7,8,9])e1 = np. concatenate([a1,b1,d1])print(e1)1[1 3 5 2 4 6 7 8 9] axis 설정 안하는 경우123a2 = np. array([[1,2,3],        [4,5,6]])print(np. concatenate([a2,a2])) # axis 설정안하면 아래로 붙음1234[[1 2 3] [4 5 6] [1 2 3] [4 5 6]] axis=11print(np. concatenate([a2,a2], axis=1)) # axis=1이면 오른쪽옆으로 붙음12[[1 2 3 1 2 3] [4 5 6 4 5 6]]vstack():  수직 스택(vertical stack), 1차원으로 연결1print(np. vstack([a2,a2])) # 아래로 붙음(수직)1234[[1 2 3] [4 5 6] [1 2 3] [4 5 6]]hstack():  수평 스택(horizontal stack), 2차원으로 연결1print(np. hstack([a2,a2])) # 옆으로 붙음(수평)12[[1 2 3 1 2 3] [4 5 6 4 5 6]]dstack():  depth stack, 3차원으로 연결1print(np. dstack([a2,a2])) # depth로 붙음, 3차원으로 바뀜1234567[[[1 1] [2 2] [3 3]] [[4 4] [5 5] [6 6]]]stack():  새로운 차원을 추가해서 연결됨1print(np. stack([a2,a2])) # 새로운 차원을 추가해서 연결이 됨12345[[[1 2 3] [4 5 6]] [[1 2 3] [4 5 6]]]배열 분할: split():  배열 분할1234a1 = np. arange(0,10)print(a1)b1,c1 = np. split(a1,[5]) # 5번째 요소를 기준으로 분할print(b1,c1)12[0 1 2 3 4 5 6 7 8 9][0 1 2 3 4] [5 6 7 8 9]12b1,c1,d1,e1,f1 = np. split(a1,[2,4,6,8]) # 2,4,6,8 번째 요소 기준으로 분할print(b1,c1,d1,e1,f1)1[0 1] [2 3] [4 5] [6 7] [8 9]vsplit():  수직 분할, 1차원으로 분할12345a2 = np. arange(1,10). reshape(3,3)print(a2)b2,c2 = np. vsplit(a2,[2]) # 2번째 row를 기준으로 수직으로 분할print(b2) print(c2)12345678[[1 2 3] [4 5 6] [7 8 9]] [[1 2 3] [4 5 6]] [[7 8 9]]hsplit():  수평 분할, 2차원으로 분할12345a2 = np. arange(1,10). reshape(3,3)print(a2)b2,c2 = np. hsplit(a2,[2]) # 2번째 col을 기준으로 수직으로 분할print(b2) print(c2)1234567891011[[1 2 3] [4 5 6] [7 8 9]] [[1 2] [4 5] [7 8]] [[3] [6] [9]]dsplit():  깊이 분할, 3차원으로 분할12345a3 = np. arange(1,28). reshape(3,3,3)print(a3)b3,c3 = np. dsplit(a3,[2]) # depth 기준으로 분할print(b3) print(c3)1234567891011121314151617181920212223242526272829303132333435[[[ 1 2 3] [ 4 5 6] [ 7 8 9]] [[10 11 12] [13 14 15] [16 17 18]] [[19 20 21] [22 23 24] [25 26 27]]] [[[ 1 2] [ 4 5] [ 7 8]] [[10 11] [13 14] [16 17]] [[19 20] [22 23] [25 26]]] [[[ 3] [ 6] [ 9]] [[12] [15] [18]] [[21] [24] [27]]]참고:  Numpy 한번에 제대로 배우기 이수안 컴퓨터 연구소"
    }, {
    "id": 41,
    "url": "http://localhost:4000/numpy_array1/",
    "title": "Numpy - 3(Array Operations 1)",
    "body": "2023/02/03 - 배열 값 삽입, 수정, 삭제, 복사:  원본 배열이 변경되지 않는 이유는 속도적인 면에서 이점이 있기 때문 배열 값 삽입: insert():  배열의 특정 위치에 값 삽입 axis를 지정하지 않으면 1차원 배열로 변환 추가(삽입)할 방향을 axis로 지정   원본 배열 변경없이 새로운 배열 반환   1차원 배열에서의 삽입1a1 = np. array([1,2,3,4,5])12345print(a1)b1 = np. insert(a1,0,10) # a1의 0번째 인덱스에 10을 삽입, a1의 값은 변경되지 않음print(b1)c1 = np. insert(a1,2,10)print(c1)123[1 2 3 4 5][10 1 2 3 4 5][ 1 2 10 3 4 5] 2차원 배열에서의 삽입123a2 = np. array([[1,2,3],       [4,5,6],       [7,8,9]])12345print(a2)b2 = np. insert(a2,1,10,axis=0)print(b2)c2 = np. insert(a2,1,10,axis=1)print(c2)123456789101112[[1 2 3] [4 5 6] [7 8 9]] [[ 1 2 3] [10 10 10] [ 4 5 6] [ 7 8 9]] [[ 1 10 2 3] [ 4 10 5 6] [ 7 10 8 9]]배열 값 수정:    배열의 인덱싱으로 접근해서 값 수정     1차원 배열 값 수정  123456789101112print(a1)a1[0] = 11a1[1] = 22a1[2] = 33print(a1)a1[:2] = 9 # 0~1 까지 9로 수정print(a1)i = np. array([1,3,4])a1[i] = 0 # 1,3,4 인덱스의 값을 0으로 수정print(a1)a1[i] += 3 # 1,3,4 인덱스의 값에 +3을 한것으로 수정print(a1)12345[ 9 0 33 0 0] # a1[11 22 33 0 0][ 9 9 33 0 0][ 9 0 33 0 0][ 9 3 33 3 3] 2차원 배열 값 수정1print(a2)123[[1 2 3] [4 5 6] [7 8 9]]12345678a2[0,0] = 11a2[1,1] = 22a2[2,2] = 33print(a2)a2[0] = 1 # 0번 row에 해당하는 것 전부 1로 수정print(a2)a2[1:,2] = 9print(a2)1234567891011[[11 1 1] [ 4 22 6] [ 7 8 33]] [[ 1 1 1] [ 4 22 6] [ 7 8 33]] [[ 1 1 1] [ 4 22 9] [ 7 8 9]]배열 값 삭제: delete():  배열의 특정 위치에 값 삭제 axis를 지정하지 않으면 1차원 배열로 변환 삭제할 방향을 axis로 지정   원본 배열 변경없이 새로운 배열 변환   1차원 배열 값 삭제1234print(a1)b1 = np. delete(a1,1) # a1의 1번째 인덱스를 삭제한 배열을 b1에 할당print(b1)print(a1) # 원본 배열은 변경되지 않음123[ 9 3 33 3 3][ 9 33 3 3][ 9 3 33 3 3] 2차원 배열 값 삭제12345print(a2)b2 = np. delete(a2,1,axis=0) # 1번 row삭제, axis는 0번print(b2)b2 = np. delete(a2,1,axis=1) # 1번 col삭제, axis는 1번, axis를 통해서 삭제하는 방향 정한다!print(b2)12345678[[ 1 1 1] [ 4 22 9] [ 7 8 9]][[1 1 1] [7 8 9]][[1 1] [4 9] [7 9]]배열 복사: 배열의 슬라이스와 원본의 관계:    리스트 자료형과 달리 배열의 슬라이스는 복사본이 아님     슬라이싱 되는 결과는 복사본이 아니고 원본을 공유, 메모리를 공유한다고 보면 됨  1234567print(a2)print(a2[:2,:2])a2_sub = a2[:2,:2]print(a2_sub)a2_sub[:,1] = 0 # 원본 배열도 바뀜print(a2_sub)print(a2)12345678910111213141516[[ 1 1 1] [ 4 22 9] [ 7 8 9]] [[ 1 1] [ 4 22]] [[ 1 1] [ 4 22]] [[1 0] [4 0]] [[1 0 1] [4 0 9] [7 8 9]]copy():  배열이나 하위 배열 내의 값을 명시적으로 복사123456print(a2)a2_sub_copy = a2[:2,:2]. copy()print(a2_sub_copy)a2_sub_copy[:,1] = 1print(a2_sub_copy)print(a2) # 복사본을 통해서 값을 바꿨기 때문에 원본은 바뀌지 않음12345678910111213[[1 0 1] [4 0 9] [7 8 9]] [[1 0] [4 0]] [[1 1] [4 1]] [[1 0 1] [4 0 9] [7 8 9]]참고:  Numpy 한번에 제대로 배우기 이수안 컴퓨터 연구소"
    }, {
    "id": 42,
    "url": "http://localhost:4000/numpycheck-2/",
    "title": "Numpy - 2(배열 조회)",
    "body": "2023/02/02 - 배열 조회: 배열 속성 정보: 123456789def array_info(array):  print(array)  print( ndim: , array. ndim) # 배열의 차원  print( shape: , array. shape) # 배열의 모양  print( dtype: , array. dtype) # 배열의 데이터 타입  print( size: , array. size) # item(element)의 개수  print( itemsize: , array. itemsize) # 각 item(element)의 byte크기  print( nbytes: , array. nbytes) # 전체 배열의 byte 크기  print( strides: , array. strides) # 한 item(element)를 넘어가는데 필요한 byte크기 1차원 배열12test_array1=np. array([1,3,5,6,7], dtype=int)array_info(test_array1)12345678[1 3 5 6 7]ndim: 1shape: (5,)dtype: int32size: 5itemsize: 4nbytes: 20strides: (4,) 2차원 배열123test_array2=np. array([[1,2,3,4],[5,6,7,8],[9,10,11,12]], dtype=int)array_info(test_array2)# 2차원 배열에서 strides (다음 차원으로 넘어가는데 필요한 stride, 한 item을 넘어가는데 필요한 stride)12345678910[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]ndim: 2shape: (3, 4)dtype: int32size: 12itemsize: 4nbytes: 48strides: (16, 4) 3차원 배열12345test_array3=np. array([[[1,2,3,4],[5,6,7,8],[9,10,11,12]],           [[1,2,3,4],[5,6,7,8],[9,10,11,12]],           [[1,2,3,4],[5,6,7,8],[9,10,11,12]],           [[1,2,3,4],[5,6,7,8],[9,10,11,12]]], dtype=int)array_info(test_array3)12345678910111213141516171819202122[[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]]ndim: 3shape: (4, 3, 4)dtype: int32size: 48itemsize: 4nbytes: 192strides: (48, 16, 4)인덱싱(Indexing):  1차원 배열12345print(test_array1)print(test_array1[0])print(test_array1[2])print(test_array1[-1]) # 마지막 요소print(test_array1[-2]) # 마지막에서 두번째 요소12345[1 3 5 6 7]1576 2차원 배열12345print(test_array2)print(test_array2[0,0])print(test_array2[0,2])print(test_array2[1,1])print(test_array2[2,-1]) # 2번 row(3번째 row)에서 마지막 요소1234567[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]13612 3차원 배열1234print(test_array3)print(test_array3[0,0,0])print(test_array3[1,1,1])print(test_array3[2,-1,-1]) # 2번 axis의 마지막 row의 마지막 요소123456789101112131415161718[[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]]1612슬라이싱(Slicing):  array[start:stop:step]   디폴트값 : start=0, stop=ndim, step=1   1차원 배열 슬라이싱1234567print(test_array1)print(test_array1[0:2])print(test_array1[0:])print(test_array1[:2])print(test_array1[::2])print(test_array1[::-1])print(test_array1[0:5:2])1234567[1 3 5 6 7][1 3][1 3 5 6 7][1 3][1 5 7][7 6 5 3 1][1 5 7] 2차원 배열 슬라이싱123456print(test_array2)print(test_array2[1])print(test_array2[1, :])print(test_array2[:2, :2])print(test_array2[1:, ::-1])print(test_array2[::-1, ::-1])1234567891011121314151617[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [5 6 7 8][5 6 7 8][[1 2] [5 6]] [[ 8 7 6 5] [12 11 10 9]] [[12 11 10 9] [ 8 7 6 5] [ 4 3 2 1]]불리언 인덱싱(Boolean Indexing):  배열 각 요소의 선택 여부를 불리언(True, False)로 지정   True 값인 인덱스의 값만 조회   1차원 배열 불리언 인덱싱123print(test_array1)bi = [True,False,True,True,False]print(test_array1[bi])12[1 3 5 6 7][1 5 6] 2차원 배열 불리언 인덱싱1234print(test_array2)bi = np. random. randint(0,2,(3,4), dtype=bool)print(bi)print(test_array2[bi])123456789[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [[ True True False True] [ True False True True] [False True True False]] [ 1 2 4 5 7 8 10 11]팬시 인덱싱(Fancy Indexing):  1차원 배열 팬시 인덱싱123456print(test_array1)fi = [0,2,3]print(test_array1[fi]) # 주어진 인덱스에 따라서 출력fi2 = np. array([[0,1],         [2,0]]) # 주어진 인덱스의 배열이 2차원 형태면 뽑는 값도 2차원 형태 print(test_array1[fi2])1234[1 3 5 6 7][1 5 6][[1 3] [5 1]] 2차원 배열 팬시 인덱싱123456789print(test_array2)row=np. array([0,2])col=np. array([1,2])print(test_array2[row,col])print(test_array2[row,:])print(test_array2[:,col])print(test_array2[row,1])print(test_array2[1,col])print(test_array2[row,:1]) # fancy indexing과 슬라이싱까지12345678910111213141516171819[[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] [ 2 11][[ 1 2 3 4] [ 9 10 11 12]] [[ 2 3] [ 6 7] [10 11]] [ 2 10][6 7][[1] [9]]참고:  Numpy 한번에 제대로 배우기 이수안 컴퓨터 연구소"
    }, {
    "id": 43,
    "url": "http://localhost:4000/numpyintro-1/",
    "title": "Numpy - 1(Introduction)",
    "body": "2023/02/01 - Numpy란?:  Numerical Python의 약자 과학 계산용 패키지 N차원 배열 객체, 범용적 데이터 처리 등에 사용 가능한 다차원 컨테이너 보로드캐스팅 기능 파이썬의 list 자료형과 비슷하지만, 더 빠르고 효율적임 반복문 없이 데이터 배열에 대한 처리를 지원 C, C++, 포트란 등의 언어와 통합 가능Numpy의 사용법: Array Dimension:  Array Shape: Array Rank: Vector: Matrix: 3rd order tensor: 배열 생성: 리스트로 배열 만들기:  1차원 배열1234567a1 = np. array([7,2,9,10])print(a1)print(type(a1))print(a1. shape) # 배열의 모양print(a1[0],a1[1],a1[2],a1[3]) # 리스트 처럼 인덱스로 접근 가능a1[0]=8 # 인덱스로 접근해서 수정 가능print(a1)12345[ 7 2 9 10]&lt;class 'numpy. ndarray'&gt;(4,)7 2 9 10[ 8 2 9 10] 2차원 배열1234a2 = np. array([[1,2,3],[4,5,6],[7,8,9]])print(a2)print(a2. shape)print(a2[0,0],a2[1,1],a2[2,2])12345[[1 2 3] [4 5 6] [7 8 9]](3, 3)1 5 9 3차원 배열12345a3 = np. array([[[1,2,3],[4,5,6],[7,8,9]],       [[1,2,3],[4,5,6],[7,8,9]],       [[1,2,3],[4,5,6],[7,8,9]]])print(a3)print(a3. shape)123456789101112[[[1 2 3] [4 5 6] [7 8 9]] [[1 2 3] [4 5 6] [7 8 9]] [[1 2 3] [4 5 6] [7 8 9]]](3, 3, 3)배열 생성 및 초기화: zeros():  모든 요소를 0으로 초기화1np. zeros(10)1array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])ones():  모든 요소를 1로 초기화1np. ones(8)1array([1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ])1np. ones((3,3)) # shape를 명시해서 2차원 형태로도 만들 수 있음123array([[1. , 1. , 1. ],    [1. , 1. , 1. ],    [1. , 1. , 1. ]])full():  모든 요소를 지정한 값으로 초기화1np. full((3,3), 1. 25)123array([[1. 25, 1. 25, 1. 25],    [1. 25, 1. 25, 1. 25],    [1. 25, 1. 25, 1. 25]])eye():  단위행렬(identity matrix) 생성1np. eye(4) # 단위 행렬은 정사각행렬임(크기 n만 명시해도 됨)1234array([[1. , 0. , 0. , 0. ],    [0. , 1. , 0. , 0. ],    [0. , 0. , 1. , 0. ],    [0. , 0. , 0. , 1. ]]) eye()에서 k의 값을 정의하면 어느 대각선에 1을 넣을건지 정할 수 있음1np. eye(4,k=1,dtype=int)1234array([[0, 1, 0, 0],    [0, 0, 1, 0],    [0, 0, 0, 1],    [0, 0, 0, 0]])1np. eye(4,k=-1,dtype=int)1234array([[0, 0, 0, 0],    [1, 0, 0, 0],    [0, 1, 0, 0],    [0, 0, 1, 0]])identity():  2차원 nxn 정방단위행렬 ndarray 객체 반환1np. identity(3)123array([[1. , 0. , 0. ],    [0. , 1. , 0. ],    [0. , 0. , 1. ]])diag():  대각 행렬의 값을 추출함123a = np. arange(9). reshape(3,3)print(a)np. diag(a)12345[[0 1 2] [3 4 5] [6 7 8]] array([0, 4, 8]) k로 대각선 정할 수 있음1np. diag(a,k=1)1array([1, 5])tri():  삼각행렬 생성1np. tri(3)123array([[1. , 0. , 0. ],    [1. , 1. , 0. ],    [1. , 1. , 1. ]])empty():  초기화되지 않은 배열 생성1np. empty(8) # 초기화를 하지 않기 때문에 빠름, 대신 기존 메모리에 존재하던 값이 나옴1array([1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ])_like():    지정된 배열과 shape가 같은 행렬 생성          np. zeros_like()     123test_array1 = [1,2,3,4,5]print(test_array1)np. zeros_like(test_array1) # test_array1과 shape가 동일한 zeros 생성        123[1, 2, 3, 4, 5]  array([0, 0, 0, 0, 0])              np. ones_like()     123test_array2 = [[1,2,3,4],[5,6,7,8]]print(test_array2)np. ones_like(test_array2)        1234[[1, 2, 3, 4], [5, 6, 7, 8]]  array([[1, 1, 1, 1],    [1, 1, 1, 1]])          생성한 값으로 배열 생성: arange():  정수 범위로 배열 생성1np. arange(0,30,2) # 0부터 30전까지 step은 2로1array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28])linspace():  범위 내에서 균들 간격의 배열 생성1np. linspace(0,10,5) # 0부터 10까지 균등하게 5개의 간격으로 나눔1array([ 0. , 2. 5, 5. , 7. 5, 10. ])1np. linspace(0,1,5) # 0부터 1까지 균등하게 5개의 간격으로 나눔1array([0.  , 0. 25, 0. 5 , 0. 75, 1.  ])logspace():  범위 내에서 균등간격으로 로그 스케일로 배열 생성1np. logspace(0. 1,1,20)1234array([ 1. 25892541, 1. 40400425, 1. 565802 , 1. 74624535, 1. 94748304,    2. 1719114 , 2. 42220294, 2. 70133812, 3. 0126409 , 3. 35981829,    3. 74700446, 4. 17881006, 4. 66037703, 5. 19743987, 5. 79639395,    6. 46437163, 7. 2093272 , 8. 04013161, 8. 9666781 , 10.     ])랜덤값으로 배열 생성: random. random():  랜덤한 수의 배열 생성1np. random. random((3,3))123array([[0. 38789798, 0. 42888851, 0. 03252684],    [0. 23912518, 0. 92076065, 0. 70782295],    [0. 54324963, 0. 67355909, 0. 20873986]])random. randint():  일정 구간의 랜덤 정수의 배열 생성1np. random. randint(0,10,(4,4)) # 0부터 10까지의 숫자범위에서 4by4에서 랜덤 정수의 배열1234array([[9, 2, 5, 7],    [0, 7, 3, 1],    [7, 5, 7, 9],    [4, 2, 0, 1]])random. normal():  정규분포(normal distribution)를 고려한 랜덤한 수의 배열 생성1np. random. normal(0,1,(3,3)) # 평균 0, 표준편차 1, 샘플사이즈 (3,3)123array([[ 0. 24864604, -1. 46494981, -1. 08069346],    [ 1. 94495246, -1. 58289044, 0. 81391231],    [ 0. 83243239, 1. 70062464, -0. 86074097]])random. rand():  균등분포(uniform distribution)를 고려한 랜덤한 수의 배열 생성1np. random. rand(4,4)1234array([[0. 0972925 , 0. 18570236, 0. 17934436, 0. 85738151],    [0. 06103397, 0. 63004401, 0. 45300681, 0. 19424704],    [0. 57064646, 0. 45949181, 0. 29839345, 0. 91645405],    [0. 36998513, 0. 99162566, 0. 72668558, 0. 13741028]])random. randn():  표준정규분포(standard normal distribution)를 고려한 랜덤한 수의 배열 생성1np. random. randn(3,3)123array([[-0. 52672415, 0. 56609104, 1. 09265444],    [-0. 63231348, -0. 26050481, -1. 25757581],    [ 1. 75737519, -1. 06730305, -0. 15958397]])numpy의 표준 데이터 타입:  bool_ : 바이트로 저장된 boolean, True 또는 False값을 가짐 int_ : 기본 정수(integer) 타입 intc : c언어에서 사용되는 int와 동일(int32 or int64) intp : 인덱싱에 사용되는 정수(c언어에서의 ssize_t 와 동일) int8 : 바이트(Byte) (-128 ~ 127) int16 : 정수 (-32768 ~ 32767) int32 : 정수 ($ -2^{31} $ ~ $ 2^{31}-1 $) int64 : 정수 ($ -2^{63} $ ~ $ 2^{63}-1 $) uint : 부호 없는 정수 (0 ~ 255) float16 : 반정밀 부동 소수점(half precision float), 부호 비트, 5비트 지수, 10비트 가수 float32 : 단정밀 부동 소수점(single precision float), 부호 비트, 8비트 지수, 23비트 가수 float64 : 배정밀 부동 소수점(double precision float), 부호 비트, 11비트 지수, 52비트 가수 float_ : float64 complex64 : 복소수(complex number), 두 개의 32비트 부동 소수점으로 표현 complex128 : 복소수, 두 개의 64비트 부동 소수점으로 표현 dtype 을 통해서 데이터 타입 명시 1np. zeros(15, dtype=int)1array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])1np. ones((3,3), dtype=bool) # 1은 True123array([[ True, True, True],    [ True, True, True],    [ True, True, True]])1np. full((4,4),1. 0,dtype=float)1234array([[1. , 1. , 1. , 1. ],    [1. , 1. , 1. , 1. ],    [1. , 1. , 1. , 1. ],    [1. , 1. , 1. , 1. ]]) 데이터 타입을 더 구체적으로 명시하기 위해서 np. datatype 형태로 123d1 = np. array([[1,2,3],[4. 5,5. 5,6. 5]], dtype = np. int8)print(d1)print(d1. dtype)1234[[1 2 3] [4 5 6]] int8참고:  Numpy 한번에 제대로 배우기 이수안 컴퓨터 연구소"
    }, {
    "id": 44,
    "url": "http://localhost:4000/python_8/",
    "title": "Python - 8(Exception, Logging)",
    "body": "2022/09/08 - Exception:  예외는 예상이 가능한 예외와 불가능한 예외가 있다  예상 가능한 예외:   사전에 인지할 수 있는 예외 사용자의 잘못된 입력 또는 파일 호출 시 파일이 없는 경우 등 개발자가 반드시 명시적으로 정의 해야함예상 불가능한 예외:  인터프리터 과정에서 발생하는 예외, 개발자 실수 리스트의 범위를 넘어가는 값 호출, 정수를 0으로 나누기 등 수행 불가시 인터프리터가 자동 호출예외 처리:  예외가 발생할 경우 후속 조치가 필요     없는 파일 호출 - 파일이 없다는 것을 알림   게임 이상 종료 - 게임 정보를 저장함   Exception handling: try ~ except: 1234try:   # 예외 발생 가능한 코드except &lt;Exception Type&gt;:  # 예외 발생시 대응하는 코드try ~ except example:  0으로 숫자를 나눌 때 예외처리 하기12345for i in range(5):  try:    print(10/i)  except ZeroDivisionError:    print( do not divide by 0 )12345do not divide by 010. 05. 03. 33333333333333352. 5 보통 맨 끝에는 Exception을 달아줌, 모든 예외를 나타내기에는 좋은 코드는 아님1234567for i in range(5):  try:    print(10/i)  except ZeroDivisionError:    print( do not divide by 0 )  except Exception as e:    print(e)exception의 종류:  built-in exception : 기본적으로 제공하는 예외 IndexError : list의 index범위를 넘어가는 경우 NameError : 존재하지 않는 변수를 호출 할 때 ZeroDivisionError : 0으로 숫자를 나눌 때 ValueError : 변환할 수 없는 문자/숫자를 변환할 때 FileNotFoundError : 존재하지 않는 파일을 호출할 때예외 정보 표시하기: 123456for i in range(5):  try:    print(10/i)  except ZeroDivisionError as e:    print(e)    print( do not divide by 0 )123456division by zerodo not divide by 010. 05. 03. 33333333333333352. 5try ~ except ~ else: 123456try:  # 예외 발생 가능 코드except &lt;Exception Type&gt;:  # 예외 발생시 동작하는 코드else:  # 예외가 발생하지 않을 때 동작하는 코드try ~ except ~ else example: 1234567for i in range(5):  try:    result = 10/i  except ZeroDivisionError:    print( do not divide by 0 )  else:    print(result)12345do not divide by 010. 05. 03. 33333333333333352. 5try ~ except ~ finally: 123456try:  # 예외 발생 가능 코드except &lt;Exception Type&gt;:  # 예외 발생시 동작하는 코드finally:  # 예외 발생 여부와 상관없이 실행try ~ except ~ finally example: 12345678try:  for i in range(1,5):    result = 10/i    print(result)except ZeroDivisionError:  print( do not divide by 0 )finally:  print( 종료되었습니다 )1234510. 05. 03. 33333333333333352. 5종료되었습니다raise:  필요에 따라 강제로 예외 발생1raise &lt;Exception Type&gt;(예외정보)raise example: 123456while True:  value=input( 변환할 정수를 입력해주세요 :  )  for digit in value:    if digit not in  0123456789 :      raise ValueError( 숫자값을 입력하지 않았습니다 )  print( 정수값으로 변환된 숫자 :  , int(value))12345678910변환할 정수를 입력해주세요 : a---------------------------------------------------------------------------ValueError                Traceback (most recent call last)~\AppData\Local\Temp\ipykernel_10184\4244621495. py in &lt;module&gt;   3   for digit in value:   4     if digit not in  0123456789 :----&gt; 5       raise ValueError( 숫자값을 입력하지 않았습니다 )   6   print( 정수값으로 변환된 숫자 :  , int(value))ValueError: 숫자값을 입력하지 않았습니다assert:  특정 조건에 만족하지 않을 경우 예외 발생1assert 예외조건assert example: 12345def get_binary_number(decimal_number):  assert isinstance(decimal_number, int)  return bin(decimal_number)print(get_binary_number(10))10b10101print(get_binary_number( sadf ))12345678910111213---------------------------------------------------------------------------AssertionError              Traceback (most recent call last)~\AppData\Local\Temp\ipykernel_10184\488716815. py in &lt;module&gt;----&gt; 1 print(get_binary_number( sadf ))~\AppData\Local\Temp\ipykernel_10184\3506293914. py in get_binary_number(decimal_number)   1 def get_binary_number(decimal_number):----&gt; 2   assert isinstance(decimal_number, int)   3   return bin(decimal_number)   4    5 print(get_binary_number(10))AssertionError: python file i/o:  파이썬은 파일 처리를 위해 open 키워드 사용12f=open( &lt;파일이름&gt; , 접근모드 )f. close()파일열기 모드:  r , 읽기모드 - 파일을 읽기만 할 때 사용 w, 쓰기모드 - 파일에 내용을 쓸 때 사용 a, 추가모드 - 파일의 마지막에 새로운 내용을 추가 시킬 때 사용File read:  read() - txt 파일 안에 있는 내용을 문자열로 반환12345# 대상파일이 같은 폴더에 있을 경우f = open( practice_text. txt ,  r )contents=f. read()print(contents)f. close()1234567this is sentence 1this is sentence 2this is sentence 3this is sentence 4this is sentence 5this is sentence 6bye bye~ with 구문과 함께 사용하는 경우 indentation에서 벗어나면 close1234# with 구문과 함께 사용하기with open( practice_text. txt , r ) as my_file:  contents=my_file. read()  print(type(contents), contents)1234567&lt;class 'str'&gt; this is sentence 1this is sentence 2this is sentence 3this is sentence 4this is sentence 5this is sentence 6bye bye~ 한 줄씩 읽어서 list type으로 반환12345# 한 줄씩 읽어서 list type으로 반환with open( practice_text. txt , r ) as my_file:  content_list=my_file. readlines() # 파일 전체를 list로 반환  print(type(content_list))  print(content_list)12&lt;class 'list'&gt;['this is sentence 1\n', 'this is sentence 2\n', 'this is sentence 3\n', 'this is sentence 4\n', 'this is sentence 5\n', 'this is sentence 6\n', 'bye bye~'] 실행 시 마다 한 줄씩 읽어 오기 그 때 마다 메모리에 올림123456789# 실행 시 마다 한 줄 씩 읽어 오기with open( practice_text. txt , r ) as my_file:  i=0  while True:    line=my_file. readline() # 한 줄 씩, readlines()와 다름    if not line:      break    print(str(i)+ === +line. replace( \n ,  )) # 한 줄씩 값 출력    i+=112345670===this is sentence 11===this is sentence 22===this is sentence 33===this is sentence 44===this is sentence 55===this is sentence 66===bye bye~File write:  인코딩은 utf812345f=open( count_log. txt , w ,encoding= utf8 )for i in range(1,11):  data=f {i} 번째 줄입니다. \n   f. write(data)f. close()count_log. txt 123456789101 번째 줄입니다. 2 번째 줄입니다. 3 번째 줄입니다. 4 번째 줄입니다. 5 번째 줄입니다. 6 번째 줄입니다. 7 번째 줄입니다. 8 번째 줄입니다. 9 번째 줄입니다. 10 번째 줄입니다.  추가 모드 “a”12345f=open( count_log. txt , a ,encoding= utf8 )for i in range(1,4):  data=f {i} 번째 추가된 줄입니다. \n   f. write(data)f. close()123456789101112131 번째 줄입니다. 2 번째 줄입니다. 3 번째 줄입니다. 4 번째 줄입니다. 5 번째 줄입니다. 6 번째 줄입니다. 7 번째 줄입니다. 8 번째 줄입니다. 9 번째 줄입니다. 10 번째 줄입니다. 1 번째 추가된 줄입니다. 2 번째 추가된 줄입니다. 3 번째 추가된 줄입니다. 파이썬 directory:  os 모듈을 사용하여 디렉토리 다루기12import osos. mkdir( log ) # 현재주소에서 log라는 폴더 생성 디렉토리가 있는지 확인하기123if not os. path. isdir( log2 ): # log2라는 디렉토리가 없으면 메세지 출력후 디렉토리 생성  print( log2 is made )  os. mksir( log2 )1log2 is made 디렉토리가 존재하면 사용하는 예외처리1234try:  os. mkdir( log )except FileExistsError as e:  print( already created )1already created os. path. exists()1os. path. exists( log ) # 존재하면 True 반환1True  shutil. copy() - 파일 복사123456import shutilsource =  count_log. txt dest = os. path. join( log , empty_log. txt ) # join 사용 권장shutil. copy(source,dest) # dest에서 log 폴더에 empty_log. txt를 만들고 source를 empty_log에1'log\\empty_log. txt' pathlib 모듈을 사용해서 path를 객체로 다룸12345import pathlibcwd = pathlib. Path. cwd()print(cwd)print(cwd. parent)print(cwd. parent. parent)123C:\Users\KIMSEUNGKI\python_practiceC:\Users\KIMSEUNGKIC:\Users1print(list(cwd. parents))1[WindowsPath('C:/Users/KIMSEUNGKI'), WindowsPath('C:/Users'), WindowsPath('C:/')]Log 파일 생성하기:  디렉토리의 유무, 파일의 유무 확인 후12345678910111213141516import osif not os. path. isdir( log ):  os. mkdir( log )if not os. path. exists( log\count_log. txt ):  f=open( log\count_log. txt , w ,encoding= utf8 )  f. write( 기록이 시작됩니다\n )  f. closef=open( log\count_log. txt , a ,encoding= utf8 )import random, datetimefor i in range(1,5):  stamp=str(datetime. datetime. now())  value=random. random()*1000000  log_line=stamp+ \t +str(value)+ 값이 생성되었습니다 + \n   f. write(log_line)f. close() log\count_log. txt12345기록이 시작됩니다2023-03-05 16:22:16. 344776	172857. 09505431302값이 생성되었습니다2023-03-05 16:22:16. 344776	833191. 7616896472값이 생성되었습니다2023-03-05 16:22:16. 344776	430916. 5337239962값이 생성되었습니다2023-03-05 16:22:16. 344776	635487. 9829742208값이 생성되었습니다Pickle:  파이썬 객체를 영속화(persistence)하는 built-in 객체 데이터, object 등 실행중 정보를 저장하고 불러와서 사용한다 저장해야하는 정보, 계산 결과(모델) 등 활용이 많음1234567891011import picklef=open( list. pickle , wb )test=[1,2,3,4,5]pickle. dump(test,f)f. close()f=open( list. pickle , rb )test_pickle=pickle. load(f)print(test_pickle)f. close()1[1, 2, 3, 4, 5]Logging Handling: Logging - 로그 남기기:  프로그램이 실행되는 동안 일어나는 정보를 기록으로 남기기 유저의 접근, 프로그램의 예외, 특정 함수의 사용 등 console 화면에 출력, 파일에 남기기, DB에 남기기 등 기록된 로그를 분석하여 의미있는 결과를 도출 할 수 있음 실행시점에서 남겨야하는 기록, 개발시점에서 남겨야하는 기록Print vs Logging:  기록을 print로 남기는 것도 가능은 함 그러나 console 창에만 남기는 기록은 분석시 사용 불가 때로는 레벨별(개발, 운영)로 기록을 남길 필요도 있음 모듈별로 별도의 logging을 남길 필요도 있음 이런 기능을 체계적으로 지원하는 모듈이 필요함Logging module:  파이썬의 기본 Log 관리 모듈1234567import logginglogging. debug( 틀렸습니다 )logging. info( 확인해주세요 )logging. warning( 주의해주세요 ) # 파이썬의 기본 logging level은 warning부터 시작logging. error( 에러입니다 )logging. critical( 심각한 문제 )123WARNING:root:주의해주세요ERROR:root:에러입니다CRITICAL:root:심각한 문제Logging Level:  프로그램 진행 상황에 따라 다른 Level의 Log를 출력함 개발 시점, 운영 시점 마다 다른 Log가 남을 수 있도록 지원함 Debug &gt; Info &gt; Warning &gt; Error &gt; Critical Log 관리시 가장 기본이 되는 설정 정보 12345678910111213import loggingif __name__== __main__ :    # logger = logging. getLogger( main )  logging. basicConfig(level=logging. DEBUG) # logging level을 DEBUG로 셋팅  # logger. setLevel(logging. WARNING)  logger. debug( 틀렸습니다 )  logger. info( 확인해주세요 )  logger. warning( 주의해주세요 )  logger. error( 에러입니다 )  logger. critical( 심각한 문제 )12345DEBUG:root:틀렸습니다INFO:root:확인해주세요WARNING:root:주의해주세요ERROR:root:에러입니다CRITICAL:root:심각한 문제12345678910111213141516import loggingif __name__== __main__ :    logger = logging. getLogger( main )  logging. basicConfig(level=logging. DEBUG) # logging level을 DEBUG로 셋팅  logger. setLevel(logging. WARNING) # WARNING level 부터 log 출력    steam_handler = logging. FileHandler( my. log , mode= a ,encoding= utf8 ) # my. log 라는 파일에 로그내용 추가  logger. addHandler(steam_handler)  logger. debug( 틀렸습니다 )  logger. info( 확인해주세요 )  logger. warning( 주의해주세요 )  logger. error( 에러입니다 )  logger. critical( 심각한 문제 )123WARNING:main:주의해주세요ERROR:main:에러입니다CRITICAL:main:심각한 문제Logging을 위한 사전 셋팅:  실제 프로그램을 실행할 땐 여러가지 설정이 필요 데이터 파일의 위치, 파일 저장 장소, Operation Type 같은 정보를 설정 해줄 방법이 필요Configparser:  프로그램의 실행 설정을 file에 저장함 section, key, value 값의 형태로 설정된 설정 파일을 사용   설정파일을 Dict type으로 호출후 사용   example. cfg123456789[SectionOne]Status: SingleName: DerekValue: YesAge: 30Single: True[SectionTwo]FavoriteColor = Green12345678910111213import configparserconfig = configparser. ConfigParser()config. sections()config. read( example. cfg )config. sections()for key in config[ SectionOne ]:  value = config[ SectionOne ][key]  print(key, value)  config[ SectionOne ][ status ]123456status Singlename Derekvalue Yesage 30single True'Single'Argparser:  console 창에서 프로그램 실행시 Setting 정보를 저장함 거의 모든 console 기반 파이썬 프로그램 기본으로 제공 특수 모듈도 많이 존재하지만, 일반적으로 argparser 사용   Command Line Option 이라고 부름   콘솔에서 특정 argument에 따라 특정 옵션을 주는 것을 볼 수 있음 arg_sum. py: 123456789101112131415161718192021import argparseparser = argparse. ArgumentParser(description= sum two integers )parser. add_argument(   -a ,  --a_value ,  dest= a , help= A integer , type=int,  required=True)parser. add_argument(    -b ,  --b_value , # 짧은 이름, 긴 이름  dest= b , help= B integer , type=int, # 표시명, 설명, argument type  required=True)args = parser. parse_args()print(args)print(args. a)print(args. b)print(args. a+args. b) argparser example:  사용자가 미리 설정을 만들어서 실험이 가능함1234567891011121314def main():parser = argparse. ArgumentParser(description='PyTorch MNIST Example')parser. add_argument('--batch-size', type=int, default=64, metavar='N', help='input batch size for training (default: 64)')parser. add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')parser. add_argument('--epochs', type=int, default=10, metavar='N', help='number of epochs to train (default: 10)')parser. add_argument('--lr', type=fl0at, default=0. 01, metavar='LR', help='learning rate (default: 0. 01 )')parser. add_argument('--momentum', type=fl0at, default=0. 5, metavar='M', help='SGD momentum (default: O. 5)')parser. add_argument('--no-cuda', action='store_true', default=False, help='disables CUDA training')parser. add_argument(‘--seed', type=int, default=1, metavar='S', help='random seed (default: 1)')parser. add_argument('--save-model', action='store_true', default=False, help='For Saving the current Model')args = parser. parse_args()if _name_ == '_main_':main()Logging 적용하기: Logging formatter:  Log의 결과값의 format을 지정해줄 수 있음123import loggingformatter = logging. Formatter( %(asctime)s %(levelname)s %(process)d %(message)s )12345672018-01-18 22:47:04,385 ERROR 4410 ERROR occurred2018-01-18 22:47:22,458 ERROR 4439 ERROR occurred2018-01-18 22:47:22,458 INFO 4439 HERE WE ARE2018-01-18 22:47:24,680 ERROR 4443 ERROR occurred2018-01-18 22:47:24,681 INFO 4443 HERE WE ARE2018-01-18 22:47:24,970 ERROR 4445 ERROR occurred2018-01-18 22:47:24,970 INFO 4445 HERE WE ARELog config file: logging. conf: 12345678910111213141516171819202122[loggers]keys=root[handlers]keys=consoleHandler[formatters]keys=simpleFormatter[logger_root]level=DEBUGhandlers=consoleHandler[handler_consoleHandler]class=StreamHandlerlevel=DEBUGformatter=simpleFormatterargs=(sys. stdout,)[formatter_simpleFormatter]format=%(asctime)s - %(levelname)s - %(process)d - %(message)sdatefmt=%m/%d/%Y %I:%M:%S %p12logging. config. fileConfig( logging. conf )logger=logging. getLogger()참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 45,
    "url": "http://localhost:4000/python_7/",
    "title": "Python - 7(Python Package)",
    "body": "2022/09/07 - Module 만들기:  파이썬의 모듈은 module == py 파일을 의미 같은 폴더에 module에 해당하는 py 파일과 사용하는 py 파일을 저장한 후, import 문을 사용해서 module 호출  module example 1:   fah_converter. py 같은 폴더에 만들기12def convert_c_to_f(celcius_value):  return celcius_value *9. 0/5+32 module_ex. py 에서 import1234567import fah_converterprint( enter a celcius value :  )celcius = float(input())fahrenheit = fah_converter. convert_c_to_f(celcius)print( thats ,fahrenheit, degress farenheit )123enter a celcius value : 20thats 68. 0 degress farenheitnamespace:  모듈을 호출할 때 범위 정하는 방법 모듈 안에는 보통 여러가지 클래스나 함수가 존재 할 수 있는데, 호출시 모든 내용이 메모리 위로 올라오는 것을 방지하는 것이 좋음 필요한 내용만 골라서 호출하기 위해 namespace 활용 from과 import 키워드를 사용함namespace example 1: 12345# 모듈에서 특정 함수 또는 클래스만 호출하기from fah_converter import convert_c_to_f # convert_c_to_f만 호출# 모듈에서 모든 함수 또는 클래스 호출하기from fah_converter import * # 전체 호출Package: 패키지란?:  하나의 대형 프로젝트를 만드는 코드의 묶음 다양한 모듈들의 합, 폴더로 연결됨 _ _ init _ _ , _ _ main_ _ 등 키워드 파일명이 사용됨 다양한 오픈 소스들이 모두 패키지로 관리됨Package 만들기: 기능들을 세부적으로 나눠서 폴더로 만든다: 각 폴더별로 필요한 모듈을 구현한다: 123# echo. pydef echo_play(echo_number):  print(f echo {echo_number} number start )1차 test - python shell: 폴더별로 _ _ init _ _ . py 구성하기:  현재 폴더가 패키지임을 알리는 초기화 스크립트 없을 경우 패키지로 간주하지 않음(3. 3 부터는 없어도 ㄱㅊ) 함위 폴더와 py 파일(모듈)을 모두 포함함 import와 _ _ all _ _ 키워드 사용123456# __init__. py in game directory__all__ = [ image , sound , stage ] # 사용하는 폴더들 전부 명시from . import imagefrom . import soundfrom . import stage123456# __init__. py in stage directory__all__ = [ main , sub ]from . import mainfrom . import sub 나머지 폴더들 전부 동일하게 처리game directory에 _ _ main. py _ _ 파일 만들기:  game directory에 생성12345678910from stagemain import game_startfrom stagesub import set_stage_levelfrom imagecharacter import show_characterfrom sound. bgm import bgm_playif _name_ == '__main__':	game_start()	set_stage_level(5)	bgm_play(10)	show_character()참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 46,
    "url": "http://localhost:4000/python_6/",
    "title": "Python - 6(OOP)",
    "body": "2022/09/07 - Python Object Oriented Programming: Python naming rule:  변수명, class명, 함수명을 짓는 방식이 존재 snake_case : 뛰어쓰기 부분에 “_” 추가, 파이썬 함수/변수명에 사용 CamelCase : 뛰어쓰기 부분에 대문자, 파아썬 Class명에 사용Attribute 추가하기: 12345class SoccerPlayer(): # class명은 SoccerPlayer, ()안에는 상속받을 클래스 넣기  def __init__(self,name,position,back_number): # __init__은 객체 초기화 예약함수    self. name=name    self. position=position    self. back_number=back_number__ 의 의미:  특수한 예약 함수나 변수 그리고 함수명 변경(맹글링)으로 사용1234567class SoccerPlayer():  # __init__생략  def __str__(self):    return f Hello, my name is {self. name}. I play as a {self. position} instance1 = SoccerPlayer( Park , MF ,10)print(instance1)1Hello, my name is Park. I play as a MFMethod 구현하기:  method 추가는 기존 함수와 같으나, 반드시 self를 추가해야만 class 함수로 인정됨12345class SoccerPlayer():   # __init__생략  def change_back_number(self, new_number):    print(f 선수의 등번호를 변경합니다: from {self. back_number} to {new_number} )    self. back_number = new_number12park = SoccerPlayer( Park , MF ,10) # 인스턴스 생성, 초기값 입력print(park. back_number)1101park. change_back_number(5)1선수의 등번호를 변경합니다: from 10 to 5상속(Inheritance): 상속이란?:  부모 클래스로 부터 속성과 메소드를 물려받은 자식 클래스를 생성하는 것12345678910class Person(): # 부모 클래스  def __init__(self,name,age):    self. name=name    self. age=ageclass Korean(Person): # 부모 클래스인 Person을 상속 받음  passfirst_korean=Korean( kimseungki ,28)print(first_korean. name)1kimseungkiexample 1 &amp; super(): 12345678910111213141516171819202122class Person(): # 부모 클래스  def __init__(self,name,age,gender):    self. name=name    self. age=age    self. gender=gender      def about_me(self): # 부모 클래스의 메소드    print( 제 이름은 , self. name, 나이는 ,str(self. age), 입니다 )class Employee(Person): # 부모 클래스인 Person으로 부터 상속  def __init__(self,name,age,gender,salary,hire_date):    super(). __init__(name,age,gender) # super로 부모객체 사용    # Employee의 속성값(attribute) 추가    self. salary=salary    self. hire_date=hire_date      def do_work(self): # Employee의 새로운 메소드    print( 일을 하고 있습니다 )      def about_me(self): # 부모 클래스의 함수 재정의    super(). about_me() # 부모 클래스의 함수 사용    print( 제 급여는 ,self. salary, 제 입사일은 ,self. hire_date)12person_instance = Person( kimseungki , 28 , gender )person_instance. about_me()1제 이름은 kimseungki 나이는 28 입니다123employee_instance = Employee( ksk ,  100 ,  male ,  10000$ ,  2222-12-31 )employee_instance. do_work()employee_instance. about_me()123일을 하고 있습니다제 이름은 ksk 나이는 100 입니다제 급여는 10000$ 제 입사일은 2222-12-31가시성(visibilty): 가시성이란?:  객체의 정보를 볼 수 있는 레벨을 조절하는 것 누구나 객체 안의 변수를 보고 접근 할 필요가 없음     객체를 사용하는 사용자가 임의로 정보 수정 할 수 없게   필요 없는 정보에는 접근 할 필요가 없음   소스의 보호를 위해   Encapsulation:  캡슐화, 은닉화 class를 설계할 때, 클래스 간 간섭/정보고유의 최소화 인터페이스만 알아서 써야함example 1:  product 객체를 inventory 객체에 추가 inventory에는 오직 product 객체만 들어감 inventory에 product가 몇 개인지 확인 필요 inventory에 product items는 직접 접근이 불가능하게1234567891011121314151617# -----------------------------모두가 접근이 가능한 경우-----------------------------class Product():  passclass Inventory():  def __init__(self):    self. items=[]     def add_new_item(self,product):    if type(product)==Product: # type이 Product면, items에 product 추가      self. items. append(product)      print( new item added )    else:      raise ValueError( invalid item )    def get_number_of_items(self):    return len(self. items)12345# private 변수 없이 모두 접근 가능한 경우my_inventory=Inventory()my_inventory. add_new_item(Product())my_inventory. add_new_item(Product())my_inventory123new item addednew item added&lt;__main__. Inventory at 0x246526807f0&gt;1my_inventory. items # items에 접근해보면 마음대로 들어다 볼 수 있음1[&lt;__main__. Product at 0x24652680e20&gt;, &lt;__main__. Product at 0x24652680610&gt;]12my_inventory. items. append( abc ) # 다른 사용자가 임의로 items에 추가 가능my_inventory. items 123[&lt;__main__. Product at 0x24652680e20&gt;, &lt;__main__. Product at 0x24652680610&gt;, 'abc']1234567891011121314151617# -----------------------------private 변수 선언으로 접근 막는 경우-----------------------------class Product():  passclass Inventory():  def __init__(self):    self. __items=[] # private 변수로 선언해서 타객체에서 접근 불가능    def add_new_item(self,product):    if type(product)==Product: # type이 Product면, items에 product 추가      self. __items. append(product)      print( new item added )    else:      raise ValueError( invalid item )    def get_number_of_items(self):    return len(self. __items)1234my_inventory_2=Inventory()my_inventory_2. add_new_item(Product())my_inventory_2. add_new_item(Product())my_inventory_2123new item addednew item added&lt;__main__. Inventory at 0x24650c925b0&gt;1my_inventory_2. __items # 접근이 불가123456---------------------------------------------------------------------------AttributeError              Traceback (most recent call last)~\AppData\Local\Temp\ipykernel_16516\3260174018. py in &lt;module&gt;----&gt; 1 my_inventory_2. __itemsAttributeError: 'Inventory' object has no attribute '__items'example 2:  product 객체를 inventory 객체에 추가 inventory에는 오직 product 객체만 들어감 inventory에 product가 몇 개인지 확인 필요 inventory에 product items 접근을 허용123456789101112131415161718192021class Product():  passclass Inventory():  def __init__(self):    self. __items=[]    @property # property decorator 숨겨진 변수를 반환하게 해줌  # 보통은 그대로 리턴하지 않고 copy한 걸 리턴해줌  def items(self):    return self. __items    def add_new_item(self,product):    if type(product)==Product: # type이 Product면, items에 product 추가      self. __items. append(product)      print( new item added )    else:      raise ValueError( invalid item )    def get_number_of_items(self):    return len(self. __items)1234my_inventory=Inventory()my_inventory. add_new_item(Product())my_inventory. add_new_item(Product())print(my_inventory. get_number_of_items())123new item addednew item added2123items=my_inventory. items # property decorator로 함수를 변수처럼 호출items. append(Product())print(my_inventory. get_number_of_items())131my_inventory. items # items로는 접근 가능123[&lt;__main__. Product at 0x276fff8b4f0&gt;, &lt;__main__. Product at 0x276fff71640&gt;, &lt;__main__. Product at 0x2768006d4c0&gt;]First-Class Objects:  일등 함수 또는 일급 객체 변수나 데이터 구조에 할당이 가능한 객체 파라미터로 전달이 가능, 리턴 값으로 사용가능 파이썬의 함수는 일급함수12345def square(x):  return x*xf=squaref(5) # 함수를 변수 처럼 사용125 함수를 파라미터 처럼 사용12345def cube(x):  return x*x*xdef formula(method, argument_list):  return [method(value) for value in argument_list]12ex_list = [1,2,3,4,5]formula(cube,ex_list)1[1, 8, 27, 64, 125]Inner function:  함수 내에 또 다른 함수가 존재123456def print_msg(msg):  def printer():    print(msg)  printer()print_msg( hello python! )1hello python! closures : inner function을 return 값으로 반환1234567def print_msg_closure(msg):  def printer():    print(msg)  return printeranother=print_msg_closure( hello python? )another()1hello python?closure example 1: 12345678910111213def tag_func(tag,text):  text=text  tag=tag    def inner_func():    return  &lt;{0}&gt;{1}&lt;{0}&gt; . format(tag,text)    return inner_funch1_func = tag_func( title , this is a python class )p_func = tag_func( p ,  data academy )h1_func()1'&lt;title&gt;this is a python class&lt;title&gt;'decorator function:  복잡한 클로져 함수를 간단하게decorator example 1: 123456789101112def star(func):  def inner(*args,**kwargs):    print( * *30)    func(*args, **kwargs)    print( * *30)  return inner@star # star 아래의 printer라는 함수는 star(func)로 전달됨def printer(msg):  print(msg)  printer( hello )123******************************hello******************************123456789101112def star(func):  def inner(*args,**kwargs):    print(args[1]*30) # mark 넘겨받음    func(*args, **kwargs)    print(args[1]*30)  return inner@stardef printer(msg, mark):  print(msg)  printer( hello ,  X )123XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXhelloXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXdecorator example 2 - 다단계 형태로도 만들기 가능: 12345@star@percent # msg를 percent로 넘겨주고 그 결과를 star에 넘겨주는 방식def printer(msg, mark):  print(msg)printer( hello! )decorator example 3: 12345678910111213def generate_power(exponent):  def wrapper(f):    def inner(*args): # f는 raise_two를 받은거임      result=f(*args)      return exponent**result    return inner  return wrapper@generate_power(2) # 2는 generate_power의 exponent에 들어감def raise_two(n): # raise_two는 wrapper로 전달  return n**2print(raise_two(7))1562949953421312참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 47,
    "url": "http://localhost:4000/python_5/",
    "title": "Python - 5(Pythonic Coding)",
    "body": "2022/09/06 - Pythonic Coding: 파이썬 스타일의 코드란?:  파이썬 스타일의 코딩 기법은 파이썬 특유의 문법을 활용하여 효율적으로 코드를 표현함 고급 코드를 작성 할 수록 더 필요해짐pythonic code를 사용하는 이유:  많은 개발자들이 python 스타일로 코딩함 단순 for loop append보다 list가 더 빠름 코드가 짧아짐Split &amp; Join: split:  string type의 값을 특정값을 기준으로 나눠서 list로 변환12items = 'Zero one two three'. split() # 빈칸 기준으로 문자열 나누기print(items)1['Zero', 'one', 'two', 'three']123example = 'python,java,javascript' # , 을 기준으로 문자열 나누기example_split = example. split( , )print(example_split)1['python', 'java', 'javascript']123a,b,c = example_split # 리스트의 각 값을 a,b,c 변수로 unpackingprint(a)print(b,c)12pythonjava javascriptjoin:  string으로 구성된 list를 합쳐서 하나의 string으로 반환123colors = [ red , blue , green , yellow ]result =    . join(colors) # 공백으로 연결print(result)1red blue green yellow12result =  , . join(colors) # , 으로 연결print(result)1red,blue,green,yellowList Comprehension:  기존 리스트를 사용하여 간단히 다른 리스트를 만드는 기법 포함되는 리스트라는 의미로 사용됨 파이썬에서 가장 많이 사용되는 기법 중 하나 일반적으로 for+append 보다 속도가 빠름example 1:  general style123456# general styleresult = []for i in range(10):  result. append(i)  print(result)1[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] list comprehension123# list comprehensionresult = [i for i in range(10)]print(result)1[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]12result = [i for i in range(10) if i%2==1] # i가 홀수인 경우 리스트로print(result)1[1, 3, 5, 7, 9]example 2: 1234word1 =  Hello word2 =  World result = [i+j for i in word1 for j in word2]print(result)1['HW', 'Ho', 'Hr', 'Hl', 'Hd', 'eW', 'eo', 'er', 'el', 'ed', 'lW', 'lo', 'lr', 'll', 'ld', 'lW', 'lo', 'lr', 'll', 'ld', 'oW', 'oo', 'or', 'ol', 'od']example 3: 1234case1 = [ a , b , c ]case2 = [ d , e , a ]result = [i+j for i in case1 for j in case2]print(result)1['ad', 'ae', 'aa', 'bd', 'be', 'ba', 'cd', 'ce', 'ca']12result = [i+j for i in case1 for j in case2 if not (i==j)] # i와 j가 같다면 리스트에 추가 안함, a와 a가 같아서 aa 제외print(result)1['ad', 'ae', 'bd', 'be', 'ba', 'cd', 'ce', 'ca']example 4: 12words =  The quick brown fox jumps over the lazy dog . split()print(words)1['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']12stuff = [[w. upper(), w. lower(), len(w)] for w in words]print(stuff)1[['THE', 'the', 3], ['QUICK', 'quick', 5], ['BROWN', 'brown', 5], ['FOX', 'fox', 3], ['JUMPS', 'jumps', 5], ['OVER', 'over', 4], ['THE', 'the', 3], ['LAZY', 'lazy', 4], ['DOG', 'dog', 3]]two-dimension list comprehension: 12345# 2dcase1 = [ a , b , c ]case2 = [ d , e , a ]result = [[i+j for i in case1]for j in case2]print(result)1[['ad', 'bd', 'cd'], ['ae', 'be', 'ce'], ['aa', 'ba', 'ca']]Enumerate &amp; Zip: enumerate:  리스트의 요소를 추출할 때 번호를 붙여서 추출123for i,v in enumerate([ tic , tac , toe ]):  # list의 index와 value unpacking  print(i,v)1230 tic1 tac2 toe12mylist = ['a','b','c','d']print(list(enumerate(mylist))) # index와 value를 unpacking해서 다시 list로 저장1[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd')]12sentence =  Artificial Intelligence, is intelligence demonstrated by machines.  print({i:value for i,value in enumerate(sentence. split())}) # sentence를 list로 split해서 list를 unpacking해서 dict로 저장1{0: 'Artificial', 1: 'Intelligence,', 2: 'is', 3: 'intelligence', 4: 'demonstrated', 5: 'by', 6: 'machines. '}zip:  두 개의 iter object의 값을 병렬적으로 추출함12345alist = ['a1','a2','a3']blist = ['b1','b2','b3']for a,b in zip(alist,blist): # 병렬적으로 값을 추출  print(a,b)123a1 b1a2 b2a3 b312result = [sum(x) for x in zip((1,2,3), (10,20,30), (100,200,300))] # 각 tuple의 같은 index에 대한 합을 list로 반환print(result)1[111, 222, 333]enumerate &amp; zip 동시 사용: 12345alist = ['a1','a2','a3']blist = ['b1','b2','b3']for i, (a,b) in enumerate(zip(alist,blist)):  print(i,a,b)1230 a1 b11 a2 b22 a3 b3Lambda &amp; Map &amp; Reduce: lambda:  함수 이름 없이 함수처럼 쓸 수 있는 익명함수 수학의 람다 대수에서 유래example 1:  general function1234def f(x,y):  return x+yprint(f(1,4))15 lambda function12f = lambda x,y:x+y # variables:functionprint(f(1,4))15example 2:  python 3부터는 권장하지는 않으나 여전히 많이 쓰임12f = lambda x:x**2print(f(3))191print((lambda x:x+1)(5))16map:  두 개 이상의 list에도 적용 가능 if filter도 사용 가능example 1: 123ex = [1,2,3,4,5]f =lambda x,y:x+yprint(list(map(f,ex,ex))) # f라는 함수에 ex,ex를 x,y로 전달하고 list로 변환1[2, 4, 6, 8, 10]12345678910result = list(  map(  lambda x:x**2 if x%2==0  else x,  ex))# ex의 element x가 짝수면 제곱하고, 아니면 그대로 x로print(result)1[1, 4, 3, 16, 5]example 2:  python 3는 iteration을 생성, list를 붙여줘야 list 사용가능 실행시점의 값을 생성, 메모리 효율적123ex = [1,2,3,4,5]print(list(map(lambda x:x+x, ex)))print(map(lambda x:x+x, ex))12[2, 4, 6, 8, 10]&lt;map object at 0x000001DA53C69340&gt;1234f = lambda x:x**2print(map(f,ex))for i in map(f,ex):  print(i, end=   )12&lt;map object at 0x000001DA53C69040&gt;1 4 9 16 25 123result = map(f,ex)print(next(result)) # next는 iterable object의 다음 요소 반환print(next(result))1214list comprehension vs map,lambda: 12ex = [1,2,3,4,5]list(map(lambda x:x**2 if x%2==0 else x, ex)) # map, lambda 사용한 경우1[1, 4, 3, 16, 5]1[value**2 if value%2==0 else value for value in ex] # list comprehension 사용한 경우1[1, 4, 3, 16, 5]reduce:  대용량 데이터 다룰 때 사용하는 경우가 있음 map function과 달리 list에 똑같은 함수를 적용해서 통합 1 -&gt; x, 2 -&gt; y 1+2=3 -&gt; x, 3 -&gt; y 순차적으로 12from functools import reduceprint(reduce(lambda x,y:x+y, [1,2,3,4,5]))115요약:  lambda, map, reduce는 간단한 코드로 다양한 기능 제공 코드의 직관성이 떨어져서 lambda와 reduce는 python 3에서 사용을 권장하지 않음 legacy library나 다양한 머신러닝 코드에서 여전히 많이 사용Generator: 제너레이터란?:  iterable object를 특수한 형태로 사용해주는 함수 element가 사용되는 시점에 값을 메모리에 반환   yield를 사용해 한번에 하나의 element만 반환   일반 리스트 사용1234567def general_list(value):  result = []  for i in range(value):    result. append(i)  return resultprint(general_list(4))1[0, 1, 2, 3] 제네레이터 사용12345678def generator_list(value):  result = []  for i in range(value):    yield i # 평소에는 메모리 위에 올려놓지 않고 주소만 가짐, 호출하는 순간 생성print(generator_list(4))for value in generator_list(10):  print(value, end=   )12&lt;generator object generator_list at 0x000001DA5678B660&gt;0 1 2 3 4 5 6 7 8 9 generator comprehension:  list comprehension과 유사한 형태로 generator형태의 list 생성 generator expression이라는 이름으로도 부름 [] 대신 () 를 사용하여 표현1234gen_ex = (n*n for n in range(50))print(type(gen_ex))print(gen_ex)print(list(gen_ex))123&lt;class 'generator'&gt;&lt;generator object &lt;genexpr&gt; at 0x000001DA53D88970&gt;[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401]제네레이터를 사용하는 이유:  일반적인 iterator는 generator에 비해 훨씬 큰 메모리 용량 사용1234567from sys import getsizeofgen_ex = (n*n for n in range(50))print(getsizeof(gen_ex))print(getsizeof(list(gen_ex)))list_ex = [n*n for n in range(50)]print(getsizeof(list_ex))123112472472제네레이터를 사용하는 경우:  list type의 데이터를 반환해주는 함수는 제너레이터로 만들기 큰 데이터를 처리할 때는 generator expression을 고려한다 파일 데이터를 처리할 때도 제너레이터를 쓴다참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 48,
    "url": "http://localhost:4000/python_4/",
    "title": "Python - 4(Call-by-)",
    "body": "2022/09/06 - 함수 호출 방식: Call by Value:  값에 의한 호출 함수에 인자를 넘길 때 값만 넘김 함수 내에 인자 값 병경 시, 호출자에게 영향을 주지 않음1a = [1,2,3,4,5]1234567def swap_value (x, y):  temp=x  x=y  y=tempswap_value(a[1],a[2])print(a)1[1, 2, 3, 4, 5]Call by Reference:  참조에 의한 호출 함수에 인자를 넘길 때 메모리 주소를 넘김 함수 내에 인자 값 변경 시, 호출자의 값도 변경됨1234567def swap_offset(offset_x,offset_y): # 리스트 a 자체를 만지기 때문에 값의 할당이 일어남  temp=a[offset_x]  a[offset_x]=a[offset_y]  a[offset_y]=tempswap_offset(1,2) # 1,2번 인덱스의 값을 서로 swapprint(a)1[1, 3, 2, 4, 5]Call by Object Reference:  객체 참조에 의한 호출 파이썬은 객체의 주소가 함수로 전달되는 방식 전달된 객체를 참조하여 변경시 호출자에게 영향을 줌 새로운 객체를 만들 경우 호출자에게 영향을 주지 않음예시: 123456789def spam(eggs):  eggs. append(1) # 기존 객체의 주소값에 1 추가  eggs=[2,3] # 새로운 객체 생성(연결 끊어짐)  eggs. append(2)  print(eggs)  ham=[0]spam(ham)print(ham) # [0,1]12[2, 3, 2][0, 1]12345678def swap_reference(list_parameter,offset_x,offset_y): # 리스트 자체를 넘겨줘서 값이 할당됨  # temp_list=list_parameter[:] # 값을 복사해서 사용하는 것이 좋음  temp=list_parameter[offset_x]  list_parameter[offset_x]=list_parameter[offset_y]  list_parameter[offset_y]=tempswap_reference(a,3,4) # 3,4번 인덱스의 값 서로 swapprint(a)1[1, 3, 2, 5, 4]Function type hints:  사용자에게 인터페이스를 명확히 알려줄 수 있음 함수의 문서화시 parameter에 대한 정보를 명확히 알 수 있음 코드의 발생 가능한 오류를 사전에 확인 가능 전체적인 안정성 확보사용방법: 12def do_function(var_name: var_type) -&gt; return_type:  pass예시: 12def type_hint_example(name:str) -&gt; str:  return f Hello, {name} 1print(type_hint_example( BOB ))1Hello, BOBDocstring:  파이썬 함수에 대한 상세스펙을 사전에 작성 세 개의 따옴표로 docstring 영력 표시(함수명 아래)1234567891011121314def some_function(argument1):     Summary or Description of the Function  Parameters:  argument1 (int): Description of arg1  Returns:  int:Returning value       return argument1print(some_function. __doc__)1234567Summary or Description of the Function  Parameters:  argument1 (int): Description of arg1  Returns:  int:Returning valuedocstring 쉽게 생성하기:  vscode의 extension을 사용해서 쉽게 추가 가능 ctrl shift p + docstring함수 작성: 가이드라인:  함수는 가능하면 짧게 작성 함수 이름에 함수의 역할, 의도를 명확히 들어낼 것 하나의 함수에는 유사한 역할을 하는 코드만 포함 인자로 받은 값 자체를 바꾸진 말 것(임시 변수 선언)함수를 만드는 상황:  공통적으로 사용되는 코드는 함수로 변환 복잡한 수식은 식별 가능한 이름의 함수로 변환 복잡한 조건은 식별 가능한 이름의 함수로 변환Coding Convention: 파이썬의 코딩 컨벤션:  보토은 팀마다, 프로젝트마다 다름 중요한 건 일관성 읽기 좋은 코드가 좋은 코드구체적으로:  한 줄은 최대 79자 까지 권장 들여쓰기는 4space 권장 불필요한 공백 피하기 = 연산자는 1칸 이상 안 뛰움 불필요한 주석은 삭제 코드의 마지막에는 항상 한 줄 추가 소문자 l, 대문자 O, 대문자 I 금지 함수명은 소문자로 구성, 필요하면 밑줄로 나눔flake8 모듈:  파이썬 코딩 컨벤션은 flake8로 체크1conda install -c anaconda flake8 사용법1flake8 &lt;파일명&gt;1flake8 flake8_test. pyblack 모듈:    black 모듈을 활용하여 pep8 like 수준을 준수     사용법  1black &lt;파일명&gt;1black black_test. pyCommit hook을 이용:  https://pre-commit. com/참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation https://pre-commit. com/"
    }, {
    "id": 49,
    "url": "http://localhost:4000/python_3/",
    "title": "Python - 3(String Methods)",
    "body": "2022/09/06 - 문자열 함수:  문자열 s를 정의  1s =  abbcdeffggg  # 문자열 s    len():   문자열의 문자 개수 반환1len(s)111upper():  대문자로 변환1s. upper()1'ABBCDEFFGGG'lower():  소문자로 변환1s. lower()1'abbcdeffggg'count():  문자열에 특정 문자열이 들어간 횟수 반환1s. count( b ) # 문자열 s에  b 가 들어있는 개수121s. count( fg ) # 문자열 s에  fg 가 들어있는 개수11find(), rfind():  문자열에 특정 문자열이 들어간 위치(오프셋) 반환1s. find( b )111s. rfind( e )15startswith():  특정 문자열로 시작하는 문자열 여부 반환1s. startswith( ab )1True1s. startswith( c )1Falseendswith():  특정 문자열로 끝나는 문자열 여부 반환1s. endswith( g )1Truestrip():  좌우 공백 제거12space_s =    there are 3 spaces on both sides   space_s. strip()1'there are 3 spaces on both sides'split():  특정값을 기준으로 나눠서 리스트로 반환1space_s. split()1['there', 'are', '3', 'spaces', 'on', 'both', 'sides']1space_s. split( s ) # s를 기준으로 나눠서 리스트로 반환1['  there are 3 ', 'pace', ' on both ', 'ide', '  ']isdigit():  문자열이 숫자인지 여부 반환1space_s. isdigit()1Falseislower():  문자열이 소문자인지 여부 반환1space_s. islower()1Truejoin():  매개변수로 들어온 리스트를 구분자 기준으로 합쳐서 하나의 문자열로 반환1list2 = [ He , llo ,   , Worl , d ]1['He', 'llo', ' ', 'Worl', 'd']1  . join(list2) # 공백 기준으로 서로 합치기1'Hello World'1 + . join(list2) # + 기준으로 서로 합치기1'He+llo+ +Worl+d'다양한 문자열 표현: 두 줄 이상 문자열 표현:  큰따옴표 또는 작은 따옴표 세 번 연속 사용123a =    Hello!I'm happyto see you.    1a # a에 들어가는 값은?1 Hello!\nI'm happy\nto see you.  1print(a)123Hello!I'm happyto see you. raw string:  특수문자 기호인 \escape 글자를 무시하고 그대로 출력12notraw_string =  이것은 raw string이 아닙니다. \n줄 바꿈이 됨 print(notraw_string)12이것은 raw string이 아닙니다. 줄 바꿈이 됨12raw_string = r 이것은 raw string입니다. \n 개행 문자가 출력됨 print(raw_string)1이것은 raw string입니다. \n 개행 문자가 출력됨참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 50,
    "url": "http://localhost:4000/python_2/",
    "title": "Python - 2(Formatting, __main__)",
    "body": "2022/09/05 - Formatting:  형식(format)에 맞춰서 출력을 하기 위한 여러가지 방법들  % string:   “%datatype” % (variable) 형태로 format 표현1print( Art: %5d, Price per unit: %8. 2f  % (453, 59. 058))1Art:  453, Price per unit:  59. 061print('%s %s' % ('one', 'two'))1one two str. format(): 1print( product: {0}, Price per unit: {1:. 3f} . format( apple , 5. 243))1product: apple, Price per unit: 5. 243f-string: 123456name =  cookie price = 500. 23print(f product: {name}, price per unit: {price} )number1 = 3. 1415926535print(f {number1:. 2f} )12product: cookie, price per unit: 500. 233. 14Debugging: 디버깅이란?:  코드의 오류를 발견하여 수정하는 과정 원인과 해결책을 알아야함 문법적 에러를 찾기 위한 에러 메세지 분석 논리적 에러를 찾기 위한 테스트문법적 오류:  들여쓰기(indentation error) 오탈자 대소문자 구분 안 함 에러 발생시 인터프리터가 알려주기 때문에 에러 메세지를 분석하자 논리적 에러:  뜻 대로 실행이 안되는 코드 중간 중간 프린터문을 찍어서 값 확인Namespace: Example 1: trapezium. py: 1234567891011121314# if __name__ ==  __main__ : 의 의미def addition(x, y):  return x+y# 사용 안하는 경우 import 하면 main 실행됨def main():   print('if __name__ ==  __main__ : 사용 안함')   print(addition(10,100))# main()if __name__ ==  __main__ :  main() 있는 경우  없는 경우 참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation"
    }, {
    "id": 51,
    "url": "http://localhost:4000/python_1/",
    "title": "Python - 1(Introduction)",
    "body": "2022/09/05 - Python’s Features:  Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding  플랫폼 독립적인 인터프리터 언어     코드를 실행하기전 컴파일할 필요가 없음    os에 적절한 인터프리터만 깔려 있으면 돌아감 객체 지향의 동적 타이핑 언어 가비지 콜렉터(garbage collector) 존재Compiler vs Interpreter: Compiler:  컴파일러(Compiler)는 특정 프로그래밍 언어로 쓰여 있는 문서를 다른 프로그래밍 언어로 옮기는 언어 번역 프로그램을 말한다. 컴파일러는 고급 프로그래밍 언어를 실행 프로그램으로 만들기 위해 저급 프로그래밍 언어(Assembly, Object code, Machine code. . )로 바꾸는 데 사용된다.  소스코드를 기계어로 먼저 번역 플랫폼에 최적화되어 프로그램을 실행 실행속도가 빠름 한번의 많은 기억장소 필요 C, JAVA, C++, C#Interpreter:  인터프리터(Interpreter)는 프로그래밍 언어의 소스 코드를 바로 실행하는 컴퓨터 프로그램 또는 환경을 말한다. 원시 코드를 기계어로 번역하는 컴파일러와 대비된다. 인터프리터는 다음의 과정 가운데 적어도 한 가지 기능을 가진 프로그램이다.    소스 코드를 직접 실행한다.   소스 코드를 효율적인 다른 중간 코드로 변환하고, 변환한 것을 바로 실행한다  인터프리터 시스템의 일부인 컴파일러가 만든, 미리 컴파일된 저장 코드의 실행을 호출한다.   별도의 번역과정 없이 소스코드를 실행시점에 해석하여 컴퓨가 처리 간단히 작성 메모리 적게 필요 실행속도가 느림 PYTHON, SCALARAssembler:  Assembler is a program for converting instructions written in low-level assembly code into relocatable machine code and generating along information for the loader. 프로그램 동작 과정:  사람이 알 수 있는 high-level 언어를 기계만 알 수 있는 low-level 언어로 변환 파이썬은 컴파일러 언어인 C로 작성됨 실행 시 assembler와 같은 기계어 변환 거침 출처 - https://automateinfra. com/2021/11/09/python-internal-working-easiest-way/ OOP Language(객체 지향 언어):  Object Oriented Programming 실행 순서가 아닌 단위 모듈(객체) 중심으로 프로그램 작성 하나의 객체는 목적을 달성하기 위한 행동(method)과 속성(attribute)을 가짐Dynamic Typing Language(동적 타이핑 언어):  Python is a dynamically typed language. It doesn’t know about the type of the variable until the code is run. So declaration is of no use. What it does is, It stores that value at some memory location and then binds that variable name to that memory container. And makes the contents of the container accessible through that variable name. So the data type does not matter. As it will get to know the type of the value at run-time.  프로그램이 실행하는 시점에 사용해야할 테이터에 대한 타입 결정Garbage Collector:  Garbage collection is to release memory when the object is no longer in use. This system destroys the unused object and reuses its memory slot for new objects. You can imagine this as a recycling system in computers.  Python has an automated garbage collection. It has an algorithm to deallocate objects which are no longer needed. Python has two ways to delete the unused objects from the memory.  Reference counting Generational garbage collection 참고:  boostcourse - 머신러닝을 위한 파이썬 Python documentation https://www. geeksforgeeks. org/introduction-of-assembler/ https://www. geeksforgeeks. org/why-python-is-called-dynamically-typed/ https://towardsdatascience. com/memory-management-and-garbage-collection-in-python-c1cb51d1612c"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});