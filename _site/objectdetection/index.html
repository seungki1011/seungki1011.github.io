<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">

<title>Object Detection Overview | Seungki1011's Dev Blog</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Object Detection Overview | Seungki1011’s Dev Blog</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Object Detection Overview" />
<meta name="author" content="seungki" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Object Detection Classification + Localization 하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인 아키텍쳐의 경우 CNN 네트워크에 class score를 정해주는 FC layer와 box coordinate을 정해주는 FC layer로 구성된다. 이 경우 계산하는 loss는 두 개이고, 학습하는 data는 이미지의 클래스 레이블과 bounding box가 정해진 ground truth를 가진 형태이다. 학습을 할 때 두 개의 loss에 대한 가중합을 학습한다. 이 때 각 loss에 대한 가중치를 hyper parameter 형태로 조절해줘야하고, 이런 multi-loss에 대한 hyper parameter를 결정하는 작업은 까다롭다. two losses for classification and localization - 출처 : CS231n What is Object Detection? 앞서 말한 classification + localization과는 다르게 객체가 여러개가 존재해서, 각 객체에 대한 bounding box와 class를 정해줘야 하는 task이다. Object detection은 결국 하나의 이미지내에서 multi object에 대한 classification and localization을 한다고 보면 된다(학습을 위해 사용하는 방법은 다름). 출처 - YOLOv3: An Incremental Improvement Ideas Used for Object Detection Sliding Window Object detection을 위해 초기에 시도 되었던 방법이다. Window를 이미지 내에서 sliding(이동) 시키면서 모든 window의 경우에 대해 classification을 진행하는 방식이다. Semantic segmentation의 sliding window와 거의 동일하다고 볼 수 있다. 이런 brute force 방식의 접근은 결국 너무 높은 computational cost를 요구하기 때문에, 특히 높은 용량의 데이터를 다루는 computer vision에서 지양해야한다. sliding window Region Proposal(RoI - Regions of Interest) 객체가 있을 법한 후보군들의 region 찾아서 그 region들에 대해 CNN 네트워크의 입력으로 주는 방식이다. 아주 간단하게 설명하자면, region proposal을 뽑아서 CNN 네트워크의 입력으로 줘서 object detection을 수행한다고 보면 된다. 이 경우 모든 경우의 수 말고 몇몇(~2k)개의 후보군만 확인하면 되는 방식이라고 brute force보다 효율적이다. Region proposal 방식은 R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)의 논문에 처음 소개 되었다. R-CNN R-CNN은 selective search라는 전통적인 알고리즘 기법(학습 x)을 통해 RoI를 만들어낸다. 뽑힌 RoI의 사이즈는 다양하기 때문에, classification을 위한 CNN 네트워크의 입력으로 주기 위해서는 image의 사이즈를 FC layer에 들어갈 수 있게 전부 동일하게 고정된 사이즈로 변경해줘야 한다. 이 때 변경된 이미지는 warped image region이라고 한다. Warped images 각각은 CNN 네트워크에 통과 시키고 classification을 위해서 SVM(2014년임을 고려하자)을 사용한다. 또한, RoI를 보정하기 위한 regression 과정도 거친다. 보정의 경우 RoI와 bounding box가 일치하지 않는 경우를 보정하기 위한 offset 값을 4개 예측 해준다고 보면 된다. R-CNN architecture - 출처 : CS231n ​ R-CNN 방식은 그럼에도 불구하고 selective search를 통한 CNN 연산을 2000번 넘게 연산으로 인해 computational cost가 높고, selective search도 cpu를 통한 연산이기 때문에 상대적으로 속도가 느리다. 이를 해결하기 위해 Fast R-CNN이 등장 했다. Fast R-CNN Fast R-CNN의 경우 앞서 말한 RCNN과 동일하게 selective search라는 region proposal method를 사용하지만, RCNN과 다른점은 selective search로 구한 RoI 각각을 CNN 네트워크를 통과시키는 것이 아니라 input image 하나에 대해 CNN에 통과시키고, RoI들은 축소된 형태로 여러가지 사이즈로 feature map에 나타난다. 이렇게 한다면, RCNN과 다르게 입력 이미지에 대해서만 CNN을 연산하고, 각각의 RoI에 대해서 classification과 regression을 진행하기 때문에 더 효율적이다. 조금 더 세부하게 과정을 살피자면, feature map의 RoI들은 사이즈가 제각각 이기 때문에 RoI pooling이라는 과정을 통해서 FC layer의 입력에 넣을 수 있도록 고정된 크기의 vector로 변환해야한다. 이 RoI feature vector는 softmax 연산을 통해 classification을 하고, bounding box regression으로 bounding box를 위한 보정값을 예측한다. Fast R-CNN architecture - 출처 : Fast R-CNN, CS231n Fast R-CNN에서도 한계점은 있다. 학습 시키지 않는 전통적인 알고리즘 기반의 selective search가 bottleneck의 원인 되었고, 이를 해결하기 위해서 Faster R-CNN이라는 방법이 등장했다. Faster R-CNN 앞의 Fast R-CNN의 한계는 결국 region proposal 단계의 bottleneck 때문이다. Faster R-CNN은 이를 해결하기 위해 네트워크가 region proposal을 학습 할 수 있는 형태로 바꾼다. 간단히 말하자면 RPN(Region Proposal Network)를 RoI pooling과 함께 GPU단에서 해결 할 수 있도록 설계되었다. Faster R-CNN architecture - 출처 : CS231n 지금까지 설명한 R-CNN 계열의 네트워크들은 region based method를 사용하는 2 stage detector 방식이다. 이 다음으로 1 stage detector와 2 stage detector의 차이 그리고 대표적인 1 stage detector 방식인 YOLO를 살펴보자. R-CNN comparison - 출처 : Recent Advances in Deep Learning for Object Detection 1-stage detector vs 2-stage detector 1 stage vs 2 stage - 출처 : A Survey of Deep Learning-Based Object Detection 2 stage detector localization과 classification을 순차적으로 해결 속도가 느림 RCNN family가 대표적인 2 stage detector 1 stage detector localization과 classification을 동시에 해결 feature extraction과 object detection이 전체 이미지에 대해 이루어지는 간단한 디자인 속도가 빠름(real time detection이 가능) 낮은 background error YOLO가 대표적인 1 stage detector Metrics For Object Detection Precision and Recall precision and recall - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial accuracy - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial Precision(정확도) - 올바르게 탐지한 물체의 수(TP) / 모델이 탐지한 물체의 수(TP+FP) Recall(재현율) - 올바르게 탐지한 물체의 수(TP) / 실제 정답 물체의 수(TP+FN) 정확도와 재현율에 따른 Trade Off 모든 영역에 대하여 전무 물체가 존재한다고 판단을 하는 경우, 재현율은 높아지지만 정확도가 떨어진다. 확실할 때만(confident 한 경우만) 물체가 존재한다고 판단을 하는 경우, 정확도는 높아지지만, 재현율이 떨어진다. Mean Average Precision (mAP) 출처 - End-to-end training of object class detectors for mean average precision 출처 - End-to-end training of object class detectors for mean average precision Precision과 recall은 보통 반비례 관계를 가진다. mAP를 계산하기 위해서는 우선 각 클래스의 average precision(AP)를 계산한다. AP는 각 precision-recall 그래프의 넓이로 계산 할 수 있다. 그 다음 모든 AP들의 평균을 계산하면 mAP를 구할 수 있다. Intersection over Union (IoU) True Positive(TP)와 False Positive(FP)를 결정하는 기준으로 IoU를 사용한다. IoU를 간략히 설명하자면 두 바운딩 박스가 겹치는 비율로 생각 할 수 있다. 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ mAP@0.5는 ground truth와 prediction의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미 NMS 계산의 경우, 같은 클래스끼리 IoU가 50% 이상일 때 낮은 confidence의 bounding box를 제거한다 Non Maximum Suppression (NMS) 의미만 해석하자면 제일 큰 것을 제외하고 나머지는 suppress 하자는 의미이다. Object detection에서 하나의 instance에는 하나의 bounding box가 적용되어야 한다. 그렇게 때문에 하나의 물체에 대해 여러개의 bounding box가 겹쳐져 있을 경우 하나로 합치는 방법이 필요하다. NMS는 보통 IoU가 특정 threshold를 넘어가는 중복되는 box를 제거하는 방식이다. 출처 - https://wikidocs.net/163295 YOLO (You Look Only Once) YOLO timeline - 출처 : https://www.researchgate.net/figure/Timeline-of-You-Only-Look-Once-YOLO-variants_fig1_369379818 YOLO v1 : 하나의 이미지의 bbox와 classification을 동시에 예측하는 1 stage detector의 등장 YOLO v3 : multi scale feature map의 사용 YOLO v5 : small, medium, large 크기 별로 모델 구성 Unified Detection in YOLO detection in YOLO - 출처 : You Only Look Once YOLO에서 object detection을 regression problem으로 전환해서 풀고 있다. 또한 간단한 구조의 end to end 네트워크를 사용하고 있다. YOLO에서 unified detection의 과정을 설명하자면, 입력 이미지를 SxS 그리드 영역으로 나누고 각 그리드 영역마다 B개의 bounding box와 confidence score를 계산하고 C개의 클래스에 대해서 해당 클래스일 확률을 계산한다. (parameter used in YOLO, S=7, B=2, C=20) Confidence score는 그리드에서 (object가 존재할 확률) x (ground truth와 IoU)로 계산한다. 수식으로 표현하면 다음과 같다. \(confidence = Pr(object) \times IoU^{truth}_{pred}\) C개의 클래스에 대한 해당 클래스일 확률은 다음과 같이 계산한다. \(class probability = Pr(Class_{i}|Object)\) 간단하게 요약을 하자면, input 이미지가 들어오면 이미지를 7x7(49개)의 그리드로 나누고, 각 그리드 마다 2개의 bounding box와 confidence score를 계산하고, 각 그리드(49개의 그리드)마다 어떤 클래스인지 해당 클래스일 확률을 계산한다. Network Design of YOLO YOLO&#39;s architecture - 출처 : You Only Look Once googlenet의 변형 사용 24 conv layers for feature extraction 2 fully connected layers for prediction output tensor of YOLO - 출처 : https://wikidocs.net/187967 output은 7x7x30으로 나온다 bbox 1, 2에 대한 (x좌표, y좌표, 너비, 높이, confidence score) + 20개 클래스에 대한 확률 Inference 각 bounding box의 confidence score를 20개의 클래스 확률과 곱해서 각 bounding box가 각 클래스에 대해 어떤 클래스에 해당하는지 확률을 계산한다. YOLO test time - 출처 : You Only Look Once 49개의 그리드당 2개의 bounding box를 계산하기 때문에 총 98개의 box에 대한 class specific confidence score를 계산한다. Advantages and Disadvantages of YOLO 장점 속도가 빨라서 real time detection에 활용 가능하다 물체의 일반화된 특징을 학습하기 때문에 새로운 도메인의 이미지에 대한 좋은 성능을 보인다 단점 그리드보다 작은 크기의 물체 검출이 불가능하다 신경망을 통과할 때 마지막 feature만 사용하기 때문에 정확도가 하락한다 Further Studying Object Detection의 milestone을 확인해보기 YOLO vs SSD 비교해보기 최근 SOTA Object Detection 모델들의 트렌드 살펴보기 참고 You Only Look Once: Unified, Real-Time Object Detection End-to-end training of object class detectors for mean average precision Rich feature hierarchies for accurate object detection and semantic segmentation boostcourse : 재활용 쓰레기를 활용한 딥러닝 - Detection CS231n" />
<meta property="og:description" content="Object Detection Classification + Localization 하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인 아키텍쳐의 경우 CNN 네트워크에 class score를 정해주는 FC layer와 box coordinate을 정해주는 FC layer로 구성된다. 이 경우 계산하는 loss는 두 개이고, 학습하는 data는 이미지의 클래스 레이블과 bounding box가 정해진 ground truth를 가진 형태이다. 학습을 할 때 두 개의 loss에 대한 가중합을 학습한다. 이 때 각 loss에 대한 가중치를 hyper parameter 형태로 조절해줘야하고, 이런 multi-loss에 대한 hyper parameter를 결정하는 작업은 까다롭다. two losses for classification and localization - 출처 : CS231n What is Object Detection? 앞서 말한 classification + localization과는 다르게 객체가 여러개가 존재해서, 각 객체에 대한 bounding box와 class를 정해줘야 하는 task이다. Object detection은 결국 하나의 이미지내에서 multi object에 대한 classification and localization을 한다고 보면 된다(학습을 위해 사용하는 방법은 다름). 출처 - YOLOv3: An Incremental Improvement Ideas Used for Object Detection Sliding Window Object detection을 위해 초기에 시도 되었던 방법이다. Window를 이미지 내에서 sliding(이동) 시키면서 모든 window의 경우에 대해 classification을 진행하는 방식이다. Semantic segmentation의 sliding window와 거의 동일하다고 볼 수 있다. 이런 brute force 방식의 접근은 결국 너무 높은 computational cost를 요구하기 때문에, 특히 높은 용량의 데이터를 다루는 computer vision에서 지양해야한다. sliding window Region Proposal(RoI - Regions of Interest) 객체가 있을 법한 후보군들의 region 찾아서 그 region들에 대해 CNN 네트워크의 입력으로 주는 방식이다. 아주 간단하게 설명하자면, region proposal을 뽑아서 CNN 네트워크의 입력으로 줘서 object detection을 수행한다고 보면 된다. 이 경우 모든 경우의 수 말고 몇몇(~2k)개의 후보군만 확인하면 되는 방식이라고 brute force보다 효율적이다. Region proposal 방식은 R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)의 논문에 처음 소개 되었다. R-CNN R-CNN은 selective search라는 전통적인 알고리즘 기법(학습 x)을 통해 RoI를 만들어낸다. 뽑힌 RoI의 사이즈는 다양하기 때문에, classification을 위한 CNN 네트워크의 입력으로 주기 위해서는 image의 사이즈를 FC layer에 들어갈 수 있게 전부 동일하게 고정된 사이즈로 변경해줘야 한다. 이 때 변경된 이미지는 warped image region이라고 한다. Warped images 각각은 CNN 네트워크에 통과 시키고 classification을 위해서 SVM(2014년임을 고려하자)을 사용한다. 또한, RoI를 보정하기 위한 regression 과정도 거친다. 보정의 경우 RoI와 bounding box가 일치하지 않는 경우를 보정하기 위한 offset 값을 4개 예측 해준다고 보면 된다. R-CNN architecture - 출처 : CS231n ​ R-CNN 방식은 그럼에도 불구하고 selective search를 통한 CNN 연산을 2000번 넘게 연산으로 인해 computational cost가 높고, selective search도 cpu를 통한 연산이기 때문에 상대적으로 속도가 느리다. 이를 해결하기 위해 Fast R-CNN이 등장 했다. Fast R-CNN Fast R-CNN의 경우 앞서 말한 RCNN과 동일하게 selective search라는 region proposal method를 사용하지만, RCNN과 다른점은 selective search로 구한 RoI 각각을 CNN 네트워크를 통과시키는 것이 아니라 input image 하나에 대해 CNN에 통과시키고, RoI들은 축소된 형태로 여러가지 사이즈로 feature map에 나타난다. 이렇게 한다면, RCNN과 다르게 입력 이미지에 대해서만 CNN을 연산하고, 각각의 RoI에 대해서 classification과 regression을 진행하기 때문에 더 효율적이다. 조금 더 세부하게 과정을 살피자면, feature map의 RoI들은 사이즈가 제각각 이기 때문에 RoI pooling이라는 과정을 통해서 FC layer의 입력에 넣을 수 있도록 고정된 크기의 vector로 변환해야한다. 이 RoI feature vector는 softmax 연산을 통해 classification을 하고, bounding box regression으로 bounding box를 위한 보정값을 예측한다. Fast R-CNN architecture - 출처 : Fast R-CNN, CS231n Fast R-CNN에서도 한계점은 있다. 학습 시키지 않는 전통적인 알고리즘 기반의 selective search가 bottleneck의 원인 되었고, 이를 해결하기 위해서 Faster R-CNN이라는 방법이 등장했다. Faster R-CNN 앞의 Fast R-CNN의 한계는 결국 region proposal 단계의 bottleneck 때문이다. Faster R-CNN은 이를 해결하기 위해 네트워크가 region proposal을 학습 할 수 있는 형태로 바꾼다. 간단히 말하자면 RPN(Region Proposal Network)를 RoI pooling과 함께 GPU단에서 해결 할 수 있도록 설계되었다. Faster R-CNN architecture - 출처 : CS231n 지금까지 설명한 R-CNN 계열의 네트워크들은 region based method를 사용하는 2 stage detector 방식이다. 이 다음으로 1 stage detector와 2 stage detector의 차이 그리고 대표적인 1 stage detector 방식인 YOLO를 살펴보자. R-CNN comparison - 출처 : Recent Advances in Deep Learning for Object Detection 1-stage detector vs 2-stage detector 1 stage vs 2 stage - 출처 : A Survey of Deep Learning-Based Object Detection 2 stage detector localization과 classification을 순차적으로 해결 속도가 느림 RCNN family가 대표적인 2 stage detector 1 stage detector localization과 classification을 동시에 해결 feature extraction과 object detection이 전체 이미지에 대해 이루어지는 간단한 디자인 속도가 빠름(real time detection이 가능) 낮은 background error YOLO가 대표적인 1 stage detector Metrics For Object Detection Precision and Recall precision and recall - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial accuracy - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial Precision(정확도) - 올바르게 탐지한 물체의 수(TP) / 모델이 탐지한 물체의 수(TP+FP) Recall(재현율) - 올바르게 탐지한 물체의 수(TP) / 실제 정답 물체의 수(TP+FN) 정확도와 재현율에 따른 Trade Off 모든 영역에 대하여 전무 물체가 존재한다고 판단을 하는 경우, 재현율은 높아지지만 정확도가 떨어진다. 확실할 때만(confident 한 경우만) 물체가 존재한다고 판단을 하는 경우, 정확도는 높아지지만, 재현율이 떨어진다. Mean Average Precision (mAP) 출처 - End-to-end training of object class detectors for mean average precision 출처 - End-to-end training of object class detectors for mean average precision Precision과 recall은 보통 반비례 관계를 가진다. mAP를 계산하기 위해서는 우선 각 클래스의 average precision(AP)를 계산한다. AP는 각 precision-recall 그래프의 넓이로 계산 할 수 있다. 그 다음 모든 AP들의 평균을 계산하면 mAP를 구할 수 있다. Intersection over Union (IoU) True Positive(TP)와 False Positive(FP)를 결정하는 기준으로 IoU를 사용한다. IoU를 간략히 설명하자면 두 바운딩 박스가 겹치는 비율로 생각 할 수 있다. 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ mAP@0.5는 ground truth와 prediction의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미 NMS 계산의 경우, 같은 클래스끼리 IoU가 50% 이상일 때 낮은 confidence의 bounding box를 제거한다 Non Maximum Suppression (NMS) 의미만 해석하자면 제일 큰 것을 제외하고 나머지는 suppress 하자는 의미이다. Object detection에서 하나의 instance에는 하나의 bounding box가 적용되어야 한다. 그렇게 때문에 하나의 물체에 대해 여러개의 bounding box가 겹쳐져 있을 경우 하나로 합치는 방법이 필요하다. NMS는 보통 IoU가 특정 threshold를 넘어가는 중복되는 box를 제거하는 방식이다. 출처 - https://wikidocs.net/163295 YOLO (You Look Only Once) YOLO timeline - 출처 : https://www.researchgate.net/figure/Timeline-of-You-Only-Look-Once-YOLO-variants_fig1_369379818 YOLO v1 : 하나의 이미지의 bbox와 classification을 동시에 예측하는 1 stage detector의 등장 YOLO v3 : multi scale feature map의 사용 YOLO v5 : small, medium, large 크기 별로 모델 구성 Unified Detection in YOLO detection in YOLO - 출처 : You Only Look Once YOLO에서 object detection을 regression problem으로 전환해서 풀고 있다. 또한 간단한 구조의 end to end 네트워크를 사용하고 있다. YOLO에서 unified detection의 과정을 설명하자면, 입력 이미지를 SxS 그리드 영역으로 나누고 각 그리드 영역마다 B개의 bounding box와 confidence score를 계산하고 C개의 클래스에 대해서 해당 클래스일 확률을 계산한다. (parameter used in YOLO, S=7, B=2, C=20) Confidence score는 그리드에서 (object가 존재할 확률) x (ground truth와 IoU)로 계산한다. 수식으로 표현하면 다음과 같다. \(confidence = Pr(object) \times IoU^{truth}_{pred}\) C개의 클래스에 대한 해당 클래스일 확률은 다음과 같이 계산한다. \(class probability = Pr(Class_{i}|Object)\) 간단하게 요약을 하자면, input 이미지가 들어오면 이미지를 7x7(49개)의 그리드로 나누고, 각 그리드 마다 2개의 bounding box와 confidence score를 계산하고, 각 그리드(49개의 그리드)마다 어떤 클래스인지 해당 클래스일 확률을 계산한다. Network Design of YOLO YOLO&#39;s architecture - 출처 : You Only Look Once googlenet의 변형 사용 24 conv layers for feature extraction 2 fully connected layers for prediction output tensor of YOLO - 출처 : https://wikidocs.net/187967 output은 7x7x30으로 나온다 bbox 1, 2에 대한 (x좌표, y좌표, 너비, 높이, confidence score) + 20개 클래스에 대한 확률 Inference 각 bounding box의 confidence score를 20개의 클래스 확률과 곱해서 각 bounding box가 각 클래스에 대해 어떤 클래스에 해당하는지 확률을 계산한다. YOLO test time - 출처 : You Only Look Once 49개의 그리드당 2개의 bounding box를 계산하기 때문에 총 98개의 box에 대한 class specific confidence score를 계산한다. Advantages and Disadvantages of YOLO 장점 속도가 빨라서 real time detection에 활용 가능하다 물체의 일반화된 특징을 학습하기 때문에 새로운 도메인의 이미지에 대한 좋은 성능을 보인다 단점 그리드보다 작은 크기의 물체 검출이 불가능하다 신경망을 통과할 때 마지막 feature만 사용하기 때문에 정확도가 하락한다 Further Studying Object Detection의 milestone을 확인해보기 YOLO vs SSD 비교해보기 최근 SOTA Object Detection 모델들의 트렌드 살펴보기 참고 You Only Look Once: Unified, Real-Time Object Detection End-to-end training of object class detectors for mean average precision Rich feature hierarchies for accurate object detection and semantic segmentation boostcourse : 재활용 쓰레기를 활용한 딥러닝 - Detection CS231n" />
<link rel="canonical" href="http://localhost:4000/objectdetection/" />
<meta property="og:url" content="http://localhost:4000/objectdetection/" />
<meta property="og:site_name" content="Seungki1011’s Dev Blog" />
<meta property="og:image" content="http://localhost:4000/post_images/objectdetection.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-29T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/post_images/objectdetection.jpeg" />
<meta property="twitter:title" content="Object Detection Overview" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"seungki"},"dateModified":"2023-04-29T00:00:00+09:00","datePublished":"2023-04-29T00:00:00+09:00","description":"Object Detection Classification + Localization 하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인 아키텍쳐의 경우 CNN 네트워크에 class score를 정해주는 FC layer와 box coordinate을 정해주는 FC layer로 구성된다. 이 경우 계산하는 loss는 두 개이고, 학습하는 data는 이미지의 클래스 레이블과 bounding box가 정해진 ground truth를 가진 형태이다. 학습을 할 때 두 개의 loss에 대한 가중합을 학습한다. 이 때 각 loss에 대한 가중치를 hyper parameter 형태로 조절해줘야하고, 이런 multi-loss에 대한 hyper parameter를 결정하는 작업은 까다롭다. two losses for classification and localization - 출처 : CS231n What is Object Detection? 앞서 말한 classification + localization과는 다르게 객체가 여러개가 존재해서, 각 객체에 대한 bounding box와 class를 정해줘야 하는 task이다. Object detection은 결국 하나의 이미지내에서 multi object에 대한 classification and localization을 한다고 보면 된다(학습을 위해 사용하는 방법은 다름). 출처 - YOLOv3: An Incremental Improvement Ideas Used for Object Detection Sliding Window Object detection을 위해 초기에 시도 되었던 방법이다. Window를 이미지 내에서 sliding(이동) 시키면서 모든 window의 경우에 대해 classification을 진행하는 방식이다. Semantic segmentation의 sliding window와 거의 동일하다고 볼 수 있다. 이런 brute force 방식의 접근은 결국 너무 높은 computational cost를 요구하기 때문에, 특히 높은 용량의 데이터를 다루는 computer vision에서 지양해야한다. sliding window Region Proposal(RoI - Regions of Interest) 객체가 있을 법한 후보군들의 region 찾아서 그 region들에 대해 CNN 네트워크의 입력으로 주는 방식이다. 아주 간단하게 설명하자면, region proposal을 뽑아서 CNN 네트워크의 입력으로 줘서 object detection을 수행한다고 보면 된다. 이 경우 모든 경우의 수 말고 몇몇(~2k)개의 후보군만 확인하면 되는 방식이라고 brute force보다 효율적이다. Region proposal 방식은 R-CNN(Rich feature hierarchies for accurate object detection and semantic segmentation)의 논문에 처음 소개 되었다. R-CNN R-CNN은 selective search라는 전통적인 알고리즘 기법(학습 x)을 통해 RoI를 만들어낸다. 뽑힌 RoI의 사이즈는 다양하기 때문에, classification을 위한 CNN 네트워크의 입력으로 주기 위해서는 image의 사이즈를 FC layer에 들어갈 수 있게 전부 동일하게 고정된 사이즈로 변경해줘야 한다. 이 때 변경된 이미지는 warped image region이라고 한다. Warped images 각각은 CNN 네트워크에 통과 시키고 classification을 위해서 SVM(2014년임을 고려하자)을 사용한다. 또한, RoI를 보정하기 위한 regression 과정도 거친다. 보정의 경우 RoI와 bounding box가 일치하지 않는 경우를 보정하기 위한 offset 값을 4개 예측 해준다고 보면 된다. R-CNN architecture - 출처 : CS231n ​ R-CNN 방식은 그럼에도 불구하고 selective search를 통한 CNN 연산을 2000번 넘게 연산으로 인해 computational cost가 높고, selective search도 cpu를 통한 연산이기 때문에 상대적으로 속도가 느리다. 이를 해결하기 위해 Fast R-CNN이 등장 했다. Fast R-CNN Fast R-CNN의 경우 앞서 말한 RCNN과 동일하게 selective search라는 region proposal method를 사용하지만, RCNN과 다른점은 selective search로 구한 RoI 각각을 CNN 네트워크를 통과시키는 것이 아니라 input image 하나에 대해 CNN에 통과시키고, RoI들은 축소된 형태로 여러가지 사이즈로 feature map에 나타난다. 이렇게 한다면, RCNN과 다르게 입력 이미지에 대해서만 CNN을 연산하고, 각각의 RoI에 대해서 classification과 regression을 진행하기 때문에 더 효율적이다. 조금 더 세부하게 과정을 살피자면, feature map의 RoI들은 사이즈가 제각각 이기 때문에 RoI pooling이라는 과정을 통해서 FC layer의 입력에 넣을 수 있도록 고정된 크기의 vector로 변환해야한다. 이 RoI feature vector는 softmax 연산을 통해 classification을 하고, bounding box regression으로 bounding box를 위한 보정값을 예측한다. Fast R-CNN architecture - 출처 : Fast R-CNN, CS231n Fast R-CNN에서도 한계점은 있다. 학습 시키지 않는 전통적인 알고리즘 기반의 selective search가 bottleneck의 원인 되었고, 이를 해결하기 위해서 Faster R-CNN이라는 방법이 등장했다. Faster R-CNN 앞의 Fast R-CNN의 한계는 결국 region proposal 단계의 bottleneck 때문이다. Faster R-CNN은 이를 해결하기 위해 네트워크가 region proposal을 학습 할 수 있는 형태로 바꾼다. 간단히 말하자면 RPN(Region Proposal Network)를 RoI pooling과 함께 GPU단에서 해결 할 수 있도록 설계되었다. Faster R-CNN architecture - 출처 : CS231n 지금까지 설명한 R-CNN 계열의 네트워크들은 region based method를 사용하는 2 stage detector 방식이다. 이 다음으로 1 stage detector와 2 stage detector의 차이 그리고 대표적인 1 stage detector 방식인 YOLO를 살펴보자. R-CNN comparison - 출처 : Recent Advances in Deep Learning for Object Detection 1-stage detector vs 2-stage detector 1 stage vs 2 stage - 출처 : A Survey of Deep Learning-Based Object Detection 2 stage detector localization과 classification을 순차적으로 해결 속도가 느림 RCNN family가 대표적인 2 stage detector 1 stage detector localization과 classification을 동시에 해결 feature extraction과 object detection이 전체 이미지에 대해 이루어지는 간단한 디자인 속도가 빠름(real time detection이 가능) 낮은 background error YOLO가 대표적인 1 stage detector Metrics For Object Detection Precision and Recall precision and recall - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial accuracy - 출처 : https://www.datacamp.com/tutorial/precision-recall-curve-tutorial Precision(정확도) - 올바르게 탐지한 물체의 수(TP) / 모델이 탐지한 물체의 수(TP+FP) Recall(재현율) - 올바르게 탐지한 물체의 수(TP) / 실제 정답 물체의 수(TP+FN) 정확도와 재현율에 따른 Trade Off 모든 영역에 대하여 전무 물체가 존재한다고 판단을 하는 경우, 재현율은 높아지지만 정확도가 떨어진다. 확실할 때만(confident 한 경우만) 물체가 존재한다고 판단을 하는 경우, 정확도는 높아지지만, 재현율이 떨어진다. Mean Average Precision (mAP) 출처 - End-to-end training of object class detectors for mean average precision 출처 - End-to-end training of object class detectors for mean average precision Precision과 recall은 보통 반비례 관계를 가진다. mAP를 계산하기 위해서는 우선 각 클래스의 average precision(AP)를 계산한다. AP는 각 precision-recall 그래프의 넓이로 계산 할 수 있다. 그 다음 모든 AP들의 평균을 계산하면 mAP를 구할 수 있다. Intersection over Union (IoU) True Positive(TP)와 False Positive(FP)를 결정하는 기준으로 IoU를 사용한다. IoU를 간략히 설명하자면 두 바운딩 박스가 겹치는 비율로 생각 할 수 있다. 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ 출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ mAP@0.5는 ground truth와 prediction의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미 NMS 계산의 경우, 같은 클래스끼리 IoU가 50% 이상일 때 낮은 confidence의 bounding box를 제거한다 Non Maximum Suppression (NMS) 의미만 해석하자면 제일 큰 것을 제외하고 나머지는 suppress 하자는 의미이다. Object detection에서 하나의 instance에는 하나의 bounding box가 적용되어야 한다. 그렇게 때문에 하나의 물체에 대해 여러개의 bounding box가 겹쳐져 있을 경우 하나로 합치는 방법이 필요하다. NMS는 보통 IoU가 특정 threshold를 넘어가는 중복되는 box를 제거하는 방식이다. 출처 - https://wikidocs.net/163295 YOLO (You Look Only Once) YOLO timeline - 출처 : https://www.researchgate.net/figure/Timeline-of-You-Only-Look-Once-YOLO-variants_fig1_369379818 YOLO v1 : 하나의 이미지의 bbox와 classification을 동시에 예측하는 1 stage detector의 등장 YOLO v3 : multi scale feature map의 사용 YOLO v5 : small, medium, large 크기 별로 모델 구성 Unified Detection in YOLO detection in YOLO - 출처 : You Only Look Once YOLO에서 object detection을 regression problem으로 전환해서 풀고 있다. 또한 간단한 구조의 end to end 네트워크를 사용하고 있다. YOLO에서 unified detection의 과정을 설명하자면, 입력 이미지를 SxS 그리드 영역으로 나누고 각 그리드 영역마다 B개의 bounding box와 confidence score를 계산하고 C개의 클래스에 대해서 해당 클래스일 확률을 계산한다. (parameter used in YOLO, S=7, B=2, C=20) Confidence score는 그리드에서 (object가 존재할 확률) x (ground truth와 IoU)로 계산한다. 수식으로 표현하면 다음과 같다. \\(confidence = Pr(object) \\times IoU^{truth}_{pred}\\) C개의 클래스에 대한 해당 클래스일 확률은 다음과 같이 계산한다. \\(class probability = Pr(Class_{i}|Object)\\) 간단하게 요약을 하자면, input 이미지가 들어오면 이미지를 7x7(49개)의 그리드로 나누고, 각 그리드 마다 2개의 bounding box와 confidence score를 계산하고, 각 그리드(49개의 그리드)마다 어떤 클래스인지 해당 클래스일 확률을 계산한다. Network Design of YOLO YOLO&#39;s architecture - 출처 : You Only Look Once googlenet의 변형 사용 24 conv layers for feature extraction 2 fully connected layers for prediction output tensor of YOLO - 출처 : https://wikidocs.net/187967 output은 7x7x30으로 나온다 bbox 1, 2에 대한 (x좌표, y좌표, 너비, 높이, confidence score) + 20개 클래스에 대한 확률 Inference 각 bounding box의 confidence score를 20개의 클래스 확률과 곱해서 각 bounding box가 각 클래스에 대해 어떤 클래스에 해당하는지 확률을 계산한다. YOLO test time - 출처 : You Only Look Once 49개의 그리드당 2개의 bounding box를 계산하기 때문에 총 98개의 box에 대한 class specific confidence score를 계산한다. Advantages and Disadvantages of YOLO 장점 속도가 빨라서 real time detection에 활용 가능하다 물체의 일반화된 특징을 학습하기 때문에 새로운 도메인의 이미지에 대한 좋은 성능을 보인다 단점 그리드보다 작은 크기의 물체 검출이 불가능하다 신경망을 통과할 때 마지막 feature만 사용하기 때문에 정확도가 하락한다 Further Studying Object Detection의 milestone을 확인해보기 YOLO vs SSD 비교해보기 최근 SOTA Object Detection 모델들의 트렌드 살펴보기 참고 You Only Look Once: Unified, Real-Time Object Detection End-to-end training of object class detectors for mean average precision Rich feature hierarchies for accurate object detection and semantic segmentation boostcourse : 재활용 쓰레기를 활용한 딥러닝 - Detection CS231n","headline":"Object Detection Overview","image":"http://localhost:4000/post_images/objectdetection.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/objectdetection/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"seungki"},"url":"http://localhost:4000/objectdetection/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<link href="/assets/css/custom.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="Seungki1011's Dev Blog">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="/about">About Me</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">Seungki1011's Dev Blog</h1>
    <p class="lead">
        머신러닝 엔지니어의 공부 블로그 🙂
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Object Detection Overview&url=http://localhost:4000/objectdetection/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/objectdetection/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/objectdetection/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="/assets/images/avatar.png" alt="Seungki">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="">Seungki</a><a target="_blank" href="" class="btn follow">Follow</a>
                        <span class="author-description">Trying to find a career for ML engineering in Seoul & Pangyo.</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Object Detection Overview</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid" src="/post_images/objectdetection.jpeg" alt="Object Detection Overview">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#object-detection">Object Detection</a>
    <ul>
      <li><a href="#classification--localization">Classification + Localization</a></li>
      <li><a href="#what-is-object-detection">What is Object Detection?</a></li>
      <li><a href="#ideas-used-for-object-detection">Ideas Used for Object Detection</a>
        <ul>
          <li><a href="#sliding-window">Sliding Window</a></li>
          <li><a href="#region-proposalroi---regions-of-interest">Region Proposal(RoI - Regions of Interest)</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#r-cnn">R-CNN</a></li>
  <li><a href="#fast-r-cnn">Fast R-CNN</a></li>
  <li><a href="#faster-r-cnn">Faster R-CNN</a></li>
  <li><a href="#1-stage-detector-vs-2-stage-detector">1-stage detector vs 2-stage detector</a>
    <ul>
      <li><a href="#2-stage-detector">2 stage detector</a></li>
      <li><a href="#1-stage-detector">1 stage detector</a></li>
    </ul>
  </li>
  <li><a href="#metrics-for-object-detection">Metrics For Object Detection</a>
    <ul>
      <li><a href="#precision-and-recall">Precision and Recall</a></li>
      <li><a href="#정확도와-재현율에-따른-trade-off">정확도와 재현율에 따른 Trade Off</a></li>
      <li><a href="#mean-average-precision-map">Mean Average Precision (mAP)</a></li>
      <li><a href="#intersection-over-union-iou">Intersection over Union (IoU)</a></li>
      <li><a href="#non-maximum-suppression-nms">Non Maximum Suppression (NMS)</a></li>
    </ul>
  </li>
  <li><a href="#yolo-you-look-only-once">YOLO (You Look Only Once)</a>
    <ul>
      <li><a href="#unified-detection-in-yolo">Unified Detection in YOLO</a></li>
      <li><a href="#network-design-of-yolo">Network Design of YOLO</a></li>
      <li><a href="#inference">Inference</a></li>
      <li><a href="#advantages-and-disadvantages-of-yolo">Advantages and Disadvantages of YOLO</a>
        <ul>
          <li><a href="#장점">장점</a></li>
          <li><a href="#단점">단점</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#further-studying">Further Studying</a></li>
  <li><a href="#참고">참고</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <hr />
<h2 id="object-detection">Object Detection</h2>
<h3 id="classification--localization">Classification + Localization</h3>
<p>하나의 객체에 대해서 클래스를 판별 하고, 그 객체의 위치를 알려주는 bounding box를 찾아주는 작업을 classification and localization task라고 한다. 기본적인 아키텍쳐의 경우 CNN 네트워크에 class score를 정해주는 FC layer와 box coordinate을 정해주는 FC layer로 구성된다.</p>

<p>이 경우 계산하는 loss는 두 개이고, 학습하는 data는 이미지의 클래스 레이블과 bounding box가 정해진 ground truth를 가진 형태이다. 학습을 할 때 두 개의 loss에 대한 가중합을 학습한다. 이 때 각 loss에 대한 가중치를 hyper parameter 형태로 조절해줘야하고, 이런 multi-loss에 대한 hyper parameter를 결정하는 작업은 까다롭다.</p>

<p><img src="../post_images/objectdetection/localization and classification.PNG" alt="localization and classification" style="zoom:67%;" class="center-image" /></p>

<p align="center">two losses for classification and localization - 출처 : CS231n </p>

<h3 id="what-is-object-detection">What is Object Detection?</h3>

<p>앞서 말한 classification + localization과는 다르게 객체가 여러개가 존재해서, 각 객체에 대한 bounding box와 class를 정해줘야 하는 task이다. Object detection은 결국 하나의 이미지내에서 multi object에 대한 classification and localization을 한다고 보면 된다(학습을 위해 사용하는 방법은 다름).</p>

<p><img src="../post_images/objectdetection/object detection 1.PNG" alt="object detection 1" style="zoom:67%;" class="center-image" /></p>

<p align="center">출처 - YOLOv3: An Incremental Improvement </p>

<h3 id="ideas-used-for-object-detection">Ideas Used for Object Detection</h3>

<h4 id="sliding-window">Sliding Window</h4>

<p>Object detection을 위해 초기에 시도 되었던 방법이다. Window를 이미지 내에서 sliding(이동) 시키면서 모든 window의 경우에 대해 classification을 진행하는 방식이다. Semantic segmentation의 sliding window와 거의 동일하다고 볼 수 있다. 이런 brute force 방식의 접근은 결국 너무 높은 computational cost를 요구하기 때문에, 특히 높은 용량의 데이터를 다루는 computer vision에서 지양해야한다.</p>

<p><img src="../post_images/objectdetection/sliding_window_example.gif" alt="sliding_window_example" style="zoom:100%;" class="center-image" /></p>

<p align="center">sliding window</p>

<h4 id="region-proposalroi---regions-of-interest">Region Proposal(RoI - Regions of Interest)</h4>

<p>객체가 있을 법한 후보군들의 region 찾아서 그 region들에 대해 CNN 네트워크의 입력으로 주는 방식이다. 아주 간단하게 설명하자면, region proposal을 뽑아서 CNN 네트워크의 입력으로 줘서 object detection을 수행한다고 보면 된다. 이 경우 모든 경우의 수 말고 몇몇(~2k)개의 후보군만 확인하면 되는 방식이라고 brute force보다 효율적이다. Region proposal 방식은 R-CNN(<a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a>)의 논문에 처음 소개 되었다.</p>

<hr />

<h2 id="r-cnn">R-CNN</h2>

<p>R-CNN은 selective search라는 전통적인 알고리즘 기법(학습 x)을 통해 RoI를 만들어낸다. 뽑힌 RoI의 사이즈는 다양하기 때문에, classification을 위한 CNN 네트워크의 입력으로 주기 위해서는 image의 사이즈를 FC layer에 들어갈 수 있게 전부 동일하게 고정된 사이즈로 변경해줘야 한다. 이 때 변경된 이미지는 warped image region이라고 한다. Warped images 각각은 CNN 네트워크에 통과 시키고 classification을 위해서 SVM(2014년임을 고려하자)을 사용한다. 또한, RoI를 보정하기 위한 regression 과정도 거친다. 보정의 경우 RoI와 bounding box가 일치하지 않는 경우를 보정하기 위한 offset 값을 4개 예측 해준다고 보면 된다.</p>

<p><img src="../post_images/objectdetection/RCNN1.PNG" alt="RCNN1" style="zoom:67%;" class="center-image" /></p>

<p align="center">R-CNN architecture - 출처 : CS231n </p>

<p>​</p>

<p>R-CNN 방식은 그럼에도 불구하고 selective search를 통한 CNN 연산을 2000번 넘게 연산으로 인해 computational cost가 높고, selective search도 cpu를 통한 연산이기 때문에 상대적으로 속도가 느리다. 이를 해결하기 위해 Fast R-CNN이 등장 했다.</p>

<hr />

<h2 id="fast-r-cnn">Fast R-CNN</h2>

<p>Fast R-CNN의 경우 앞서 말한 RCNN과 동일하게 selective search라는 region proposal method를 사용하지만, RCNN과 다른점은 selective search로 구한 RoI 각각을 CNN 네트워크를 통과시키는 것이 아니라 input image 하나에 대해 CNN에 통과시키고, RoI들은 축소된 형태로 여러가지 사이즈로 feature map에 나타난다. 이렇게 한다면, RCNN과 다르게 입력 이미지에 대해서만 CNN을 연산하고, 각각의 RoI에 대해서 classification과 regression을 진행하기 때문에 더 효율적이다. 조금 더 세부하게 과정을 살피자면, feature map의 RoI들은 사이즈가 제각각 이기 때문에 RoI pooling이라는 과정을 통해서 FC layer의 입력에 넣을 수 있도록 고정된 크기의 vector로 변환해야한다. 이 RoI feature vector는 softmax 연산을 통해 classification을 하고, bounding box regression으로 bounding box를 위한 보정값을 예측한다.</p>

<p><img src="../post_images/objectdetection/fRCNN architecture 1.png" alt="fRCNN architecture 1" style="zoom:67%;" class="center-image" /></p>

<p align="center">Fast R-CNN architecture - 출처 : Fast R-CNN, CS231n </p>

<p>Fast R-CNN에서도 한계점은 있다. 학습 시키지 않는 전통적인 알고리즘 기반의 selective search가 bottleneck의 원인 되었고, 이를 해결하기 위해서 Faster R-CNN이라는 방법이 등장했다.</p>

<hr />

<h2 id="faster-r-cnn">Faster R-CNN</h2>

<p>앞의 Fast R-CNN의 한계는 결국 region proposal 단계의 bottleneck 때문이다. Faster R-CNN은 이를 해결하기 위해 네트워크가 region proposal을 학습 할 수 있는 형태로 바꾼다. 간단히 말하자면 RPN(Region Proposal Network)를 RoI pooling과 함께 GPU단에서 해결 할 수 있도록 설계되었다.</p>

<p><img src="../post_images/objectdetection/Faster RCNN architecture 2.PNG" alt="Faster RCNN architecture 2" style="zoom:67%;" class="center-image" /></p>

<p align="center">Faster R-CNN architecture - 출처 : CS231n </p>

<p>지금까지 설명한 R-CNN 계열의 네트워크들은 region based method를 사용하는 2 stage detector 방식이다. 이 다음으로 1 stage detector와 2 stage detector의 차이 그리고 대표적인 1 stage detector 방식인 YOLO를 살펴보자.</p>

<p><img src="../post_images/objectdetection/rcnn계열비교.PNG" alt="rcnn계열비교" style="zoom:100%;" class="center-image" /></p>

<p align="center">R-CNN comparison - 출처 : Recent Advances in Deep Learning for Object Detection </p>

<hr />

<h2 id="1-stage-detector-vs-2-stage-detector">1-stage detector vs 2-stage detector</h2>

<p><img src="../post_images/objectdetection/1stage vs 2 stage.png" alt="1stage vs 2 stage" style="zoom:67%;" class="center-image" /></p>

<p align="center">1 stage vs 2 stage - 출처 : A Survey of Deep Learning-Based Object Detection </p>

<h3 id="2-stage-detector">2 stage detector</h3>

<ul>
  <li>localization과 classification을 순차적으로 해결</li>
  <li>속도가 느림</li>
  <li>RCNN family가 대표적인 2 stage detector</li>
</ul>

<h3 id="1-stage-detector">1 stage detector</h3>

<ul>
  <li>localization과 classification을 동시에 해결</li>
  <li>feature extraction과 object detection이 전체 이미지에 대해 이루어지는 간단한 디자인</li>
  <li>속도가 빠름(real time detection이 가능)</li>
  <li>낮은 background error</li>
  <li>YOLO가 대표적인 1 stage detector</li>
</ul>

<hr />

<h2 id="metrics-for-object-detection">Metrics For Object Detection</h2>

<h3 id="precision-and-recall">Precision and Recall</h3>

<p><img src="../post_images/objectdetection/Precision_recall_Representation_1052507280.png" alt="Precision_recall_Representation_1052507280" style="zoom:67%;" class="center-image" /></p>

<p align="center">precision and recall - 출처 : 
https://www.datacamp.com/tutorial/precision-recall-curve-tutorial </p>

<p><img src="../post_images/objectdetection/Precision_Recall_Accuracy_f1a9096d20.png" alt="Precision_Recall_Accuracy_f1a9096d20" style="zoom:80%;" class="center-image" /></p>

<p align="center">accuracy - 출처 : 
https://www.datacamp.com/tutorial/precision-recall-curve-tutorial </p>

<ul>
  <li>
    <p>Precision(정확도) - 올바르게 탐지한 물체의 수(TP) / 모델이 탐지한 물체의 수(TP+FP)</p>
  </li>
  <li>
    <p>Recall(재현율) - 올바르게 탐지한 물체의 수(TP) / 실제 정답 물체의 수(TP+FN)</p>
  </li>
</ul>

<h3 id="정확도와-재현율에-따른-trade-off">정확도와 재현율에 따른 Trade Off</h3>

<p>모든 영역에 대하여 전무 물체가 존재한다고 판단을 하는 경우, 재현율은 높아지지만 정확도가 떨어진다.</p>

<p>확실할 때만(confident 한 경우만) 물체가 존재한다고 판단을 하는 경우, 정확도는 높아지지만, 재현율이 떨어진다.</p>

<h3 id="mean-average-precision-map">Mean Average Precision (mAP)</h3>

<p><img src="../post_images/objectdetection/mAP.PNG" alt="mAP" style="zoom:100%;" class="center-image" /></p>

<p align="center">출처 - 
End-to-end training of object class detectors for mean average precision </p>

<p><img src="../post_images/objectdetection/average precision.PNG" alt="average precision" style="zoom:100%;" class="center-image" /></p>

<p align="center">출처 - 
End-to-end training of object class detectors for mean average precision </p>

<ul>
  <li>Precision과 recall은 보통 반비례 관계를 가진다.</li>
  <li>mAP를 계산하기 위해서는 우선 각 클래스의 average precision(AP)를 계산한다. AP는 각 precision-recall 그래프의 넓이로 계산 할 수 있다. 그 다음 모든 AP들의 평균을 계산하면 mAP를 구할 수 있다.</li>
</ul>

<h3 id="intersection-over-union-iou">Intersection over Union (IoU)</h3>

<p>True Positive(TP)와 False Positive(FP)를 결정하는 기준으로  IoU를 사용한다. IoU를 간략히 설명하자면 두 바운딩 박스가 겹치는 비율로 생각 할 수 있다.</p>

<p><img src="../post_images/objectdetection/iou_stop_sign.png" alt="iou_stop_sign" style="zoom:67%;" class="center-image" /></p>

<p align="center">출처 - pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ </p>

<p><img src="../post_images/objectdetection/iou_equation.png" alt="iou_equation" style="zoom:100%;" class="center-image" /></p>

<p align="center">출처 - 
pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ </p>

<ul>
  <li>mAP@0.5는 ground truth와 prediction의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미</li>
  <li>NMS 계산의 경우, 같은 클래스끼리 IoU가 50% 이상일 때 낮은 confidence의 bounding box를 제거한다</li>
</ul>

<h3 id="non-maximum-suppression-nms">Non Maximum Suppression (NMS)</h3>

<p>의미만 해석하자면 제일 큰 것을 제외하고 나머지는 suppress 하자는 의미이다. Object detection에서 하나의 instance에는 하나의 bounding box가 적용되어야 한다. 그렇게 때문에 하나의 물체에 대해 여러개의 bounding box가 겹쳐져 있을 경우 하나로 합치는 방법이 필요하다. NMS는 보통 IoU가 특정 threshold를 넘어가는 중복되는 box를 제거하는 방식이다.</p>

<p><img src="../post_images/objectdetection/NMS_4.png" alt="NMS_4" style="zoom:100%;" class="center-image" /></p>

<p align="center">출처 - 
https://wikidocs.net/163295 </p>

<hr />

<h2 id="yolo-you-look-only-once">YOLO (You Look Only Once)</h2>

<p><img src="../post_images/objectdetection/Timeline-of-You-Only-Look-Once-YOLO-variants.png" alt="Timeline-of-You-Only-Look-Once-YOLO-variants" style="zoom:100%;" class="center-image" /></p>

<p align="center">YOLO timeline - 출처 : https://www.researchgate.net/figure/Timeline-of-You-Only-Look-Once-YOLO-variants_fig1_369379818 </p>

<ul>
  <li>YOLO v1 : 하나의 이미지의 bbox와 classification을 동시에 예측하는 1 stage detector의 등장</li>
  <li>YOLO v3 : multi scale feature map의 사용</li>
  <li>YOLO v5 : small, medium, large 크기 별로 모델 구성</li>
</ul>

<h3 id="unified-detection-in-yolo">Unified Detection in YOLO</h3>

<p><img src="../post_images/objectdetection/yolo unified detection.PNG" alt="yolo unified detection" style="zoom:67%;" class="center-image" /></p>

<p align="center">detection in YOLO  - 출처 : You Only Look Once</p>

<p>YOLO에서 object detection을 regression problem으로 전환해서 풀고 있다. 또한 간단한 구조의 end to end 네트워크를 사용하고 있다. YOLO에서 unified detection의 과정을 설명하자면, 입력 이미지를 SxS 그리드 영역으로 나누고 각 그리드 영역마다 B개의 bounding box와 confidence score를 계산하고 C개의 클래스에 대해서 해당 클래스일 확률을 계산한다. (parameter used in YOLO, S=7, B=2, C=20)</p>

<p>Confidence score는 그리드에서 (object가 존재할 확률) x (ground truth와 IoU)로 계산한다. 수식으로 표현하면 다음과 같다.
\(confidence = Pr(object) \times IoU^{truth}_{pred}\)
C개의 클래스에 대한 해당 클래스일 확률은 다음과 같이 계산한다.
\(class probability = Pr(Class_{i}|Object)\)
간단하게 요약을 하자면, input 이미지가 들어오면 이미지를 7x7(49개)의 그리드로 나누고, 각 그리드 마다 2개의 bounding box와 confidence score를 계산하고, 각 그리드(49개의 그리드)마다 어떤 클래스인지 해당 클래스일  확률을 계산한다.</p>

<h3 id="network-design-of-yolo">Network Design of YOLO</h3>

<p><img src="../post_images/objectdetection/yolo design.PNG" alt="yolo design" style="zoom:100%;" class="center-image" /></p>

<p align="center">YOLO's architecture  - 출처 : You Only Look Once</p>

<ul>
  <li>
    <p>googlenet의 변형 사용</p>
  </li>
  <li>
    <p>24 conv layers for feature extraction</p>
  </li>
  <li>
    <p>2 fully connected layers for prediction</p>
  </li>
</ul>

<p><img src="../post_images/objectdetection/yolo output.PNG" alt="yolo output" style="zoom:100%;" class="center-image" /></p>

<p align="center">output tensor of YOLO  - 출처 : https://wikidocs.net/187967</p>

<ul>
  <li>output은 7x7x30으로 나온다</li>
  <li>bbox 1, 2에 대한 (x좌표, y좌표, 너비, 높이, confidence score) + 20개 클래스에 대한 확률</li>
</ul>

<h3 id="inference">Inference</h3>

<p>각 bounding box의 confidence score를 20개의 클래스 확률과 곱해서 각 bounding box가 각 클래스에 대해 어떤 클래스에 해당하는지 확률을 계산한다.</p>

<p><img src="../post_images/objectdetection/yolo tt.PNG" alt="yolo tt" style="zoom:100%;" class="center-image" /></p>

<p align="center">YOLO test time  - 출처 : You Only Look Once</p>

<p>49개의 그리드당 2개의 bounding box를 계산하기 때문에 총 98개의 box에 대한 class specific confidence score를 계산한다.</p>

<h3 id="advantages-and-disadvantages-of-yolo">Advantages and Disadvantages of YOLO</h3>

<h4 id="장점">장점</h4>

<ul>
  <li>속도가 빨라서 real time detection에 활용 가능하다</li>
  <li>물체의 일반화된 특징을 학습하기 때문에 새로운 도메인의 이미지에 대한 좋은 성능을 보인다</li>
</ul>

<h4 id="단점">단점</h4>

<ul>
  <li>그리드보다 작은 크기의 물체 검출이 불가능하다</li>
  <li>신경망을 통과할 때 마지막 feature만 사용하기 때문에 정확도가 하락한다</li>
</ul>

<hr />

<h2 id="further-studying">Further Studying</h2>

<ol>
  <li>Object Detection의 milestone을 확인해보기</li>
  <li>YOLO vs SSD 비교해보기</li>
  <li>최근 SOTA Object Detection 모델들의 트렌드 살펴보기</li>
</ol>

<p><br /></p>

<h2 id="참고">참고</h2>

<hr />

<ol>
  <li><a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></li>
  <li><a href="https://arxiv.org/abs/1607.03476">End-to-end training of object class detectors for mean average precision</a></li>
  <li><a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
  <li><a href="https://www.boostcourse.org/ai341">boostcourse : 재활용 쓰레기를 활용한 딥러닝 - Detection</a></li>
  <li><a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk">CS231n</a></li>
</ol>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2023-04-29">29 Apr 2023</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#CV">CV</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#Object-Detection">Object Detection</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="//Docker-Basic-1/"> &laquo; Docker Basic - 1</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="//Debugging/">Debugging &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'roadtoml'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span class="center-vertically"> <a href="https://github.com/seungki1011" target="_blank"><img src="../assets/images/github-mark.png" alt="Seungki1011's Dev Blog"><b class="bold-with-margin">&nbsp; My Github</b></a></span>
        <!-- <form action="" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form> -->
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#Python">Python (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Software-Engineering">Software Engineering (5)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Linux">Linux (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Docker">Docker (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#CV">CV (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Object-Detection">Object Detection (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Debugging">Debugging (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Segmentation">Segmentation (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Cloud">Cloud (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#CI/CD">CI/CD (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Backend">Backend (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#FastAPI">FastAPI (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#DB">DB (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Data">Data (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Airflow">Airflow (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#Mlops">Mlops (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#MLflow">MLflow (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#MLops">MLops (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#BentoML">BentoML (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2023 Seungki1011's Dev Blog 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//roadtoml.disqus.com/count.js"></script>


</body>
</html>
